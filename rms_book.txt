Chapter 1

Introduction

1.1 Hypothesis Testing, Estimation, and Prediction
Statistics comprises among other areas study design, hypothesis testing, estimation, and prediction. This text aims at the last area, by presenting methods
that enable an analyst to develop models that will make accurate predictions
of responses for future observations. Prediction could be considered a superset of hypothesis testing and estimation, so the methods presented here will
also assist the analyst in those areas. It is worth pausing to explain how this
is so.
In traditional hypothesis testing one often chooses a null hypothesis defined as the absence of some e↵ect. For example, in testing whether a variable such as cholesterol is a risk factor for sudden death, one might test the
null hypothesis that an increase in cholesterol does not increase the risk of
death. Hypothesis testing can easily be done within the context of a statistical
model, but a model is not required. When one only wishes to assess whether
an e↵ect is zero, P -values may be computed using permutation or rank (nonparametric) tests while making only minimal assumptions. But there are still
reasons for preferring a model-based approach over techniques that only yield
P -values.
1. Permutation and rank tests do not easily give rise to estimates of magnitudes of e↵ects.
2. These tests cannot be readily extended to incorporate complexities such
as cluster sampling or repeated measurements within subjects.
3. Once the analyst is familiar with a model, that model may be used to
carry out many di↵erent statistical tests; there is no need to learn specific
formulas to handle the special cases. The two-sample t-test is a special case
of the ordinary multiple regression model having as its sole X variable
a dummy variable indicating group membership. The Wilcoxon-MannWhitney test is a special case of the proportional odds ordinal logistic
model.658 The analysis of variance (multiple group) test and the Kruskal–

1

2

1 Introduction

Wallis test can easily be obtained from these two regression models by
using more than one dummy predictor variable.
Even without complexities such as repeated measurements, problems can
arise when many hypotheses are to be tested. Testing too many hypotheses
is related to fitting too many predictors in a regression model. One commonly
hears the statement that “the dataset was too small to allow modeling, so we
just did hypothesis tests.” It is unlikely that the resulting inferences would be
reliable. If the sample size is insufficient for modeling it is often insufficient
for tests or estimation. This is especially true when one desires to publish
an estimate of the e↵ect corresponding to the hypothesis yielding the smallest P -value. Ordinary point estimates are known to be badly biased when
the quantity to be estimated was determined by “data dredging.” This can
be remedied by the same kind of shrinkage used in multivariable modeling
(Section 9.10).
Statistical estimation is usually model-based. For example, one might use a
survival regression model to estimate the relative e↵ect of increasing cholesterol from 200 to 250 mg/dl on the hazard of death. Variables other than
cholesterol may also be in the regression model, to allow estimation of the
e↵ect of increasing cholesterol, holding other risk factors constant. But accurate estimation of the cholesterol e↵ect will depend on how cholesterol as
well as each of the adjustment variables is assumed to relate to the hazard
of death. If linear relationships are incorrectly assumed, estimates will be
inaccurate. Accurate estimation also depends on avoiding overfitting the adjustment variables. If the dataset contains 200 subjects, 30 of whom died, and
if one adjusted for 15 “confounding” variables, the estimates would be “overadjusted” for the e↵ects of the 15 variables, as some of their apparent e↵ects
would actually result from spurious associations with the response variable
(time until death). The overadjustment would reduce the cholesterol e↵ect.
The resulting unreliability of estimates equals the degree to which the overall
model fails to validate on an independent sample.
It is often useful to think of e↵ect estimates as di↵erences between two
predicted values from a model. This way, one can account for nonlinearities
and interactions. For example, if cholesterol is represented nonlinearly in a
logistic regression model, predicted values on the “linear combination of X’s
scale” are predicted log odds of an event. The increase in log odds from raising
cholesterol from 200 to 250 mg/dl is the di↵erence in predicted values, where
cholesterol is set to 250 and then to 200, and all other variables are held
constant. The point estimate of the 250:200 mg/dl odds ratio is the anti-log
of this di↵erence. If cholesterol is represented nonlinearly in the model, it
does not matter how many terms in the model involve cholesterol as long as
the correct predicted values are obtained.
Thus when one develops a reliable multivariable prediction model, hypothesis testing and estimation of e↵ects are byproducts of the fitted model. So
predictive modeling is often desirable even when prediction is not the main
goal.

1.2 Examples of Uses of Predictive Multivariable Modeling

3

1.2 Examples of Uses of Predictive Multivariable
Modeling
There is an endless variety of uses for multivariable models. Predictive models have long been used in business to forecast financial performance and
to model consumer purchasing and loan pay-back behavior. In ecology, regression models are used to predict the probability that a fish species will
disappear from a lake. Survival models have been used to predict product
life (e.g., time to burn-out of an mechanical part, time until saturation of a
disposable diaper). Models are commonly used in discrimination litigation in
an attempt to determine whether race or sex is used as the basis for hiring
or promotion, after taking other personnel characteristics into account.
Multivariable models are used extensively in medicine, epidemiology, biostatistics, health services research, pharmaceutical research, and related
fields. The author has worked primarily in these fields, so most of the examples in this text come from those areas. In medicine, two of the major
areas of application are diagnosis and prognosis. There models are used to
predict the probability that a certain type of patient will be shown to have a
specific disease, or to predict the time course of an already diagnosed disease.
In observational studies in which one desires to compare patient outcomes
between two or more treatments, multivariable modeling is very important
because of the biases caused by nonrandom treatment assignment. Here the
simultaneous e↵ects of several uncontrolled variables must be controlled (held
constant mathematically if using a regression model) so that the e↵ect of the
factor of interest can be more purely estimated. A newer technique for more
aggressively adjusting for nonrandom treatment assignment, the propensity
score,113, 525 provides yet another opportunity for multivariable modeling (see
Section 10.1.4). The propensity score is merely the predicted value from a
multivariable model where the response variable is the exposure or the treatment actually used. The estimated propensity score is then used in a second
step as an adjustment variable in the model for the response of interest.
It is not widely recognized that multivariable modeling is extremely valuable even in well-designed randomized experiments. Such studies are often
designed to make relative comparisons of two or more treatments, using odds
ratios, hazard ratios, and other measures of relative e↵ects. But to be able
to estimate absolute e↵ects one must develop a multivariable model of the
response variable. This model can predict, for example, the probability that a
patient on treatment A with characteristics X will survive five years, or it can
predict the life expectancy for this patient. By making the same prediction
for a patient on treatment B with the same characteristics, one can estimate
the absolute di↵erence in probabilities or life expectancies. This approach
recognizes that low-risk patients must have less absolute benefit of treatment
(lower change in outcome probability) than high-risk patients,344 a fact that
has been ignored in many clinical trials. Another reason for multivariable

4

1 Introduction

modeling in randomized clinical trials is that when the basic response model
is nonlinear (e.g., logistic, Cox, parametric survival models), the unadjusted
estimate of the treatment e↵ect is not correct if there is moderate heterogeneity of subjects, even with perfect balance of baseline characteristics across
the treatment groups.a9, 24, 194 So even when investigators are interested in
simple comparisons of two groups’ responses, multivariable modeling can be
advantageous and sometimes mandatory.
Cost-e↵ectiveness analysis is becoming increasingly used in health care
research, and the “e↵ectiveness” (denominator of the cost-e↵ectiveness ratio) is always a measure of absolute e↵ectiveness. As absolute e↵ectiveness
varies dramatically with the risk profiles of subjects, it must be estimated for
individual subjects using a multivariable model.

1.3 Prediction vs. Classification
For problems ranging from bioinformatics to marketing, many analysts desire
to develop “classifiers” instead of developing predictive models. Consider an
optimum case for classifier development, in which the response variable is
binary, the two levels represent a sharp dichotomy with no gray zone (e.g.,
complete succes vs. total failure with no possibility of a partial success), the
user of the classifier is forced to make one of the two choices, the cost of
misclassification is the same for every future observation, and the ratio of the
cost of a false positive to that of a false negative equals the (often hidden)
ratio implied by the analyst’s classification rule. Even if all of those conditions are met, classification is still inferior to probability modeling for driving
the development of a predictive instrument or for estimation or hypothesis
testing. It is far better to use the full information in the data to develop a
probability model, then develop classification rules on the basis of estimated
probabilities. At the least, this forces the analyst to use a proper accuracy
score215 in finding or weighting data features.
When the dependent variable is ordinal or continuous, classification through
forced up-front dichotomization in an attempt to simplify the problem results
in arbitrariness and major information loss even when the optimum cut point
(the median) is used. Dichtomizing the outcome at a di↵erent point may require a many-fold increase in sample size to make up for the lost information183 . In the area of medical diagnosis, it is often the case that the disease
is really on a continuum, and predicting the severity of disease (rather than
just its presence or absence) will greatly increase power and precision, not to
mention removing a level of arbitrariness from the analysis.
a

For example, unadjusted odds ratios from 2 ⇥ 2 tables are di↵erent from adjusted
odds ratios when there is variation in subjects’ risk factors within each treatment
group, even when the distribution of the risk factors is identical between the two
groups.

1.3 Prediction vs. Classification

5

It is important to note that two-group classification represents an artificial
forced choice. It is not often the case that the user of the classifier needs to
be limited to two possible actions. The best option for many subjects may
be to refuse to make a decision or to obtain more data (e.g., order another
medical diagnostic test). A gray zone can be helpful, and predictions include
gray zones automatically.
Unlike prediction (e.g., of absolute risk), classification implicitly uses utility functions (also called loss or cost functions, e.g., cost of a false positive
classification). Implicit utility functions are highly problematic. First, it is
well known that the utility function depends on variables that are not predictive of outcome and are not collected (e.g., subjects’ preferences) that
are available only at the decision point. Second, the approach assumes every
subject has the same utility functionb . Third, the analyst presumptuously
assumes that the subject’s utility coincides with his own.
Formal decision analysis uses subject-specific utilities and optimum predictions based on all available data61, 73, 179, 206, 215, 634c . It follows that receiver
operating characteristic curve analysis is misleading except for the special
case of mass one-time group decision making with unknowable utilities (e.g.,
launching a vaccination program).
An analyst’s goal should be the development of the most accurate and
reliable predictive model or the best model on which to base estimation or
hypothesis testing. In the vast majority of cases, classification is the task of
the user of the predictive model, at the point in which utilities (costs) and
preferences are known.
b

Simple examples to the contrary are the less weight given to a false negative diagnosis of cancer in the elderly and the aversion of some subjects to surgery or
chemotherapy.
c
To make an optimal decision you need to know all relevant data about an individual (used to estimate the probability of an outcome), and the utility (cost, loss
function) of making each decision. Sensitivity and specificity do not provide this information. For example, if one estimated that the probability of a disease given age,
sex, and symptoms is 0.1 and the “cost”of a false positive equaled the “cost” of a
false negative, one would act as if the person does not have the disease. Given other
utilities, one would make di↵erent decisions. If the utilities are unknown, one gives
the best estimate of the probability of the outcome to the decision maker and let her
incorporate her own unspoken utilities in making an optimum decision for her.
Besides the fact that cuto↵s do not apply to individuals, only to groups, individual decision making does not utilize sensitivity and specificity. For an individual we
can compute Prob(Y = 1|X = x); we don’t care about Prob(Y = 1|X > c), and
an individual having X = x would be quite puzzled if she were given Prob(X >
c|future unknown Y) when she already knows X = x so X is no longer a random
variable.
Even when group decision making is needed, sensitivity and specificity can be
bypassed. For mass marketing, for example, one can rank order individuals by the
estimated probability of buying the product, to create a lift curve. This is then used
to target the k most likely buyers where k is chosen to meet total program cost
constraints.

1

6

1 Introduction

1.4 Planning for Modeling
When undertaking the development of a model to predict a response, one
of the first questions the researcher must ask is “will this model actually be
used?” Many models are never used, for several reasons517 including: (1) it
was not deemed relevant to make predictions in the setting envisioned by
the authors; (2) potential users of the model did not trust the relationships,
weights, or variables used to make the predictions; and (3) the variables
necessary to make the predictions were not routinely available.
Once the researcher convinces herself that a predictive model is worth
developing, there are many study design issues to be addressed.18, 370 Models
are often developed using a “convenience sample,” that is, a dataset that was
not collected with such predictions in mind. The resulting models are often
fraught with difficulties such as the following.
1. The most important predictor or response variables may not have been
collected, tempting the researchers to make do with variables that do not
capture the real underlying processes.
2. The subjects appearing in the dataset are ill-defined, or they are not representative of the population for which inferences are to be drawn; similarly,
the data collection sites may not represent the kind of variation in the
population of sites.
3. Key variables are missing in large numbers of subjects.
4. Data are not missing at random; for example, data may not have been
collected on subjects who dropped out of a study early, or on patients who
were too sick to be interviewed.
5. Operational definitions of some of the key variables were never made.
6. Observer variability studies may not have been done, so that the reliability of measurements is unknown, or there are other kinds of important
measurement errors.
A predictive model will be more accurate, as well as useful, when data collection is planned prospectively. That way one can design data collection
instruments containing the necessary variables, and all terms can be given
standard definitions (for both descriptive and response variables) for use at
all data collection sites. Also, steps can be taken to minimize the amount of
missing data.
In the context of describing and modeling health outcomes, Iezzoni310 has
an excellent discussion of the dimensions of risk that should be captured by
variables included in the model. She lists these general areas that should be
quantified by predictor variables:
1.
2.
3.
4.

age,
sex,
acute clinical stability,
principal diagnosis,

1.4 Planning for Modeling

5.
6.
7.
8.
9.
10.
11.

7

severity of principal diagnosis,
extent and severity of comorbidities,
physical functional status,
psychological, cognitive, and psychosocial functioning,
cultural, ethnic, and socioeconomic attributes and behaviors,
health status and quality of life, and
patient attitudes and preferences for outcomes.
Some baseline covariates to be sure to capture in general include

1.
2.
3.
4.
5.

a baseline measurement of the response variable,
the subject’s most recent status,
the subject’s trajectory as of time zero or past levels of a key variable,
variables explaining much of the variation in the response, and
more subtle predictors whose distributions strongly di↵er between the levels of a key variable of interest in an observational study.
Many things can go wrong in statistical modeling, including the following.

1. The process generating the data is not stable.
2. The model is misspecified with regard to nonlinearities or interactions, or
there are predictors missing.
3. The model is misspecified in terms of the transformation of the response
variable or the model’s distributional assumptions.
4. The model contains discontinuities (e.g., by categorizing continuous predictors or fitting regression shapes with sudden changes) that can be gamed
by users.
5. Correlations among subjects are not specified, or the correlation structure
is misspecified, resulting in inefficient parameter estimates and overconfident inference.
6. The model is overfitted, resulting in predictions that are too extreme or
positive associations that are false.
7. The user of the model relies on predictions obtained by extrapolating to
combinations of predictor values well outside the range of the dataset used
to develop the model.
8. Accurate and discriminating predictions can lead to behavior changes that
make future predictions inaccurate.

1.4.1 Emphasizing Continuous Variables
When designing the data collection it is important to emphasize the use of
continuous variables over categorical ones. Some categorical variables are subjective and hard to standardize, and on the average they do not contain the

8

2

1 Introduction

same amount of statistical information as continuous variables. Above all, it
is unwise to categorize naturally continuous variables during data collection,d
as the original values can then not be recovered, and if another researcher
feels that the (arbitrary) cuto↵ values were incorrect, other cuto↵s cannot
be substituted. Many researchers make the mistake of assuming that categorizing a continuous variable will result in less measurement error. This is a
false assumption, for if a subject is placed in the wrong interval this will be
as much as a 100% error. Thus the magnitude of the error multiplied by the
probability of an error is no better with categorization.

1.5 Choice of the Model
The actual method by which an underlying statistical model should be chosen
by the analyst is not well developed. A. P. Dawid is quoted in Lehmann389
as saying the following.
Where do probability models come from? To judge by the resounding silence
over this question on the part of most statisticians, it seems highly embarrassing. In general, the theoretician is happy to accept that his abstract probability
triple (⌦, A, P ) was found under a gooseberry bush, while the applied statistician’s model “just growed”.

3

In biostatistics, epidemiology, economics, psychology, sociology, and many
other fields it is seldom the case that subject matter knowledge exists that
would allow the analyst to pre-specify a model (e.g., Weibull or log-normal
survival model), a transformation for the response variable, and a structure
for how predictors appear in the model (e.g., transformations, addition of
nonlinear terms, interaction terms). Indeed, some authors question whether
the notion of a model even exists in many cases.97 We are for better or worse
forced to develop models empirically in the majority of cases. Fortunately,
careful and objective validation of the accuracy of model predictions against
observable responses can lend credence to a model, if a good validation is not
merely the result of overfitting (see Section 5.3).
There are a few general guidelines that can help in choosing the basic form
of the statistical model.
1. The model must use the data efficiently. If, for example, one were interested in predicting the probability that a patient with a specific set of
characteristics would live five years from diagnosis, an inefficient model
d

An exception may be sensitive variables such as income level. Subjects may be more
willing to check a box corresponding to a wide interval containing their income. It
is unlikely that a reduction in the probability that a subject will inflate her income
will o↵set the loss of precision due to categorization of income, but there will be a
decrease in the number of refusals. This reduction in missing data can more than
o↵set the lack of precision.

1.5 Choice of the Model

2.

3.

4.

5.

9

would be a binary logistic model. A more efficient method, and one that
would also allow for losses to follow-up before five years, would be a semiparametric (rank based) or parametric survival model. Such a model uses
individual times of events in estimating coefficients, but it can easily be
used to estimate the probability of surviving five years. As another example, if one were interested in predicting patients’ quality of life on a scale
of excellent, very good, good, fair, and poor, a polytomous (multinomial)
categorical response model would not be efficient as it would not make use
of the ordering of responses.
Choose a model that fits overall structures likely to be present in the
data. In modeling survival time in chronic disease one might feel that the
importance of most of the risk factors is constant over time. In that case,
a proportional hazards model such as the Cox or Weibull model would
be a good initial choice. If on the other hand one were studying acutely
ill patients whose risk factors wane in importance as the patients survive
longer, a model such as the log-normal or log-logistic regression model
would be more appropriate.
Choose a model that is robust to problems in the data that are difficult to
check. For example, the Cox proportional hazards model and ordinal logistic models are not a↵ected by monotonic transformations of the response
variable.
Choose a model whose mathematical form is appropriate for the response
being modeled. This often has to do with minimizing the need for interaction terms that are included only to address a basic lack of fit. For
example, many researchers have used ordinary linear regression models
for binary responses, because of their simplicity. But such models allow
predicted probabilities to be outside the interval [0, 1], and strange interactions among the predictor variables are needed to make predictions
remain in the legal range.
Choose a model that is readily extendible. The Cox model, by its use of
stratification, easily allows a few of the predictors, especially if they are
categorical, to violate the assumption of equal regression coefficients over
time (proportional hazards assumption). The continuation ratio ordinal
logistic model can also be generalized easily to allow for varying coefficients
of some of the predictors as one proceeds across categories of the response.

R. A. Fisher as quoted in Lehmann389 had these suggestions about model
building: “(a) We must confine ourselves to those forms which we know how
to handle,” and (b) “More or less elaborate forms will be suitable according
to the volume of the data.” Ameen [97, p. 453] stated that a good model is
“(a) satisfactory in performance relative to the stated objective, (b) logically
sound, (c) representative, (d) questionable and subject to on-line interrogation, (e) able to accommodate external or expert information and (f) able to
convey information.”
It is very typical to use the data to make decisions about the form of
the model as well as about how predictors are represented in the model.

10

4

5

1 Introduction

Then, once a model is developed, the entire modeling process is routinely
forgotten, and statistical quantities such as standard errors, confidence limits,
P -values, and R2 are computed as if the resulting model were entirely prespecified. However, Faraway,182 Draper,160 Chatfield,97 Buckland et al.79 and
others have written about the severe problems that result from treating an
empirically derived model as if it were pre-specified and as if it were the
correct model. As Chatfield states [97, p. 426]:“It is indeed strange that we
often admit model uncertainty by searching for a best model but then ignore
this uncertainty by making inferences and predictions as if certain that the
best fitting model is actually true.”
Stepwise variable selection is one of the most widely used and abused of all
data analysis techniques. Much is said about this technique later (see Section
4.3), but there are many other elements of model development that will need
to be accounted for when making statistical inferences, and unfortunately it
is difficult to derive quantities such as confidence limits that are properly
adjusted for uncertainties such as the data-based choice between a Weibull
and a log-normal regression model.
Ye673 developed a general method for estimating the “generalized degrees
of freedom” (GDF) for any “data mining” or model selection procedure based
on least squares. The GDF is an extremely useful index of the amount of
“data dredging” or overfitting that has been done in a modeling process.
It is also useful for estimating the residual variance with less bias. In one
example, Ye developed a regression tree using recursive partitioning involving
10 candidate predictor variables on 100 observations. The resulting tree had
19 nodes and GDF of 76. The usual way of estimating the residual variance
involves dividing the pooled within-node sum of squares by 100 19, but Ye
showed that dividing by 100 76 instead yielded a much less biased (and
much higher) estimate of 2 . In another example, Ye considered stepwise
variable selection using 20 candidate predictors and 22 observations. When
there is no true association between any of the predictors and the response,
Ye found that GDF = 14.1 for a strategy that selected the best five-variable
model.
Given that the choice of the model has been made (e.g., a log-normal
model), penalized maximum likelihood estimation has major advantages in
the battle between making the model fit adequately and avoiding overfitting
(Sections 9.10 and 13.4.7). Penalization lessens the need for model selection.

1.6 Further Reading
1

Briggs and Zaretzki73 eloquently state the problem with ROC curves and the
areas under them (AUC):
Statistics such as the AUC are not especially relevant to someone who
must make a decision about a particular xc . . . . ROC curves lack or ob-

1.6 Further Reading

11

scure several quantities that are necessary for evaluating the operational
e↵ectiveness of diagnostic tests. . . . ROC curves were first used to check
how radio receivers (like radar receivers) operated over a range of frequencies. . . . This is not how must ROC curves are used now, particularly
in medicine. The receiver of a diagnostic measurement . . . wants to make
a decision based on some xc , and is not especially interested in how well
he would have done had he used some di↵erent cuto↵.
In the discussion to their paper, David Hand states
When integrating to yield the overall AUC measure, it is necessary to
decide what weight to give each value in the integration. The AUC implicitly does this using a weighting derived empirically from the data.
This is nonsensical. The relative importance of misclassifying a case as
a noncase, compared to the reverse, cannot come from the data itself. It
must come externally, from considerations of the severity one attaches to
the di↵erent kinds of misclassifications.
2
3

4

5

More problems caused by dichotomizing continuous variables are discussed in
[13, 17, 44, 81, 181, 287, 371, 516, 591].
See the excellent editorial by Mallows427 for more about model choice. See
Breiman and discussants66 for an interesting debate about the use of data
models vs. algorithms. This material also covers interpretability vs. predictive
accuracy and several other topics.
See [15,79,97,160,182,408] for information about accounting for model selection in
making final inferences. Faraway182 demonstrated that the bootstrap has good
potential in related although somewhat simpler settings, and Buckland et al.79
developed a promising bootstrap weighting method for accounting for model
uncertainty.
Tibshirani and Knight604 developed another approach to estimating the generalized degrees of freedom. Luo et al.423 developed a way to add noise of known
variance to the response variable to tune the stopping rule used for variable
selection. Zou et al.686 showed that the lasso, an approach that simultaneously
selects variables and shrinks coefficients, has a nice property. Since it uses penalization (shrinkage), an unbiased estimate of its e↵ective number of degrees
of freedom is the number of nonzero regression coefficients in the final model.

Chapter 2

General Aspects of Fitting Regression
Models

2.1 Notation for Multivariable Regression Models
The ordinary multiple linear regression model is frequently used and has parameters that are easily interpreted. In this chapter we study a general class of
regression models, those stated in terms of a weighted sum of a set of independent or predictor variables. It is shown that after linearizing the model with
respect to the predictor variables, the parameters in such regression models
are also readily interpreted. Also, all the designs used in ordinary linear regression can be used in this general setting. These designs include analysis
of variance (ANOVA) setups, interaction e↵ects, and nonlinear e↵ects. Besides describing and interpreting general regression models, this chapter also
describes, in general terms, how the three types of assumptions of regression
models can be examined.
First we introduce notation for regression models. Let Y denote the response (dependent) variable, and let X = X1 , X2 , . . . , Xp denote a list or
vector of predictor variables (also called covariables or independent, descriptor, or concomitant variables). These predictor variables are assumed to be
constants for a given individual or subject from the population of interest.
Let = 0 , 1 , . . . , p denote the list of regression coefficients (parameters).
0 is an optional intercept parameter, and 1 , . . . , p are weights or regression
coefficients corresponding to X1 , . . . , Xp . We use matrix or vector notation
to describe a weighted sum of the Xs:
X =

0

+

1 X1

+ ... +

p Xp ,

(2.1)

where there is an implied X0 = 1.
A regression model is stated in terms of a connection between the predictors X and the response Y . Let C(Y |X) denote a property of the distribution
of Y given X (as a function of X). For example, C(Y |X) could be E(Y |X),
the expected value or average of Y given X, or C(Y |X) could be the probability that Y = 1 given X (where Y = 0 or 1).
13

14

2 General Aspects of Fitting Regression Models

2.2 Model Formulations
We define a regression function as a function that describes interesting properties of Y that may vary across individuals in the population. X describes the
list of factors determining these properties. Stated mathematically, a general
regression model is given by
C(Y |X) = g(X).

(2.2)

We restrict our attention to models that, after a certain transformation,
are linear in the unknown parameters, that is, models that involve X only
through a weighted sum of all the Xs. The general linear regression model is
given by
C(Y |X) = g(X ).
(2.3)
For example, the ordinary linear regression model is
C(Y |X) = E(Y |X) = X ,
and given X, Y has a normal distribution with mean X
ance 2 . The binary logistic regression model126, 639 is

(2.4)
and constant vari-

C(Y |X) = Prob{Y = 1|X} = (1 + exp( X ))

1

,

(2.5)

where Y can take on the values 0 and 1. In general the model, when
stated in terms of the property C(Y |X), may not be linear in X ; that
is C(Y |X) = g(X ), where g(u) is nonlinear in u. For example, a regression
model could be E(Y |X) = (X ).5 . The model may be made linear in the
unknown parameters by a transformation in the property C(Y |X):
h(C(Y |X)) = X ,

(2.6)

where h(u) = g 1 (u), the inverse function of g. As an example consider the
binary logistic regression model given by
C(Y |X) = Prob{Y = 1|X} = (1 + exp( X ))
If h(u) = logit(u) = log(u/(1

1

.

(2.7)

u)), the transformed model becomes

h(Prob(Y = 1|X)) = log(exp(X )) = X .

(2.8)

The transformation h(C(Y |X)) is sometimes called a link function. Let
h(C(Y |X)) be denoted by C 0 (Y |X). The general linear regression model then
becomes
C 0 (Y |X) = X .
(2.9)

In other words, the model states that some property C 0 of Y , given X, is
a weighted sum of the Xs (X ). In the ordinary linear regression model,

2.3 Interpreting Model Parameters

15

C 0 (Y |X) = E(Y |X). In the logistic regression case, C 0 (Y |X) is the logit of
the probability that Y = 1, log Prob{Y = 1}/[1 Prob{Y = 1}]. This is the
log of the odds that Y = 1 versus Y = 0.
It is important to note that the general linear regression model has two
major components: C 0 (Y |X) and X . The first part has to do with a property
or transformation of Y . The second, X , is the linear regression or linear
predictor part. The method of least squares can sometimes be used to fit
the model if C 0 (Y |X) = E(Y |X). Other cases must be handled using other
methods such as maximum likelihood estimation or nonlinear least squares.

2.3 Interpreting Model Parameters
In the original model, C(Y |X) specifies the way in which X a↵ects a property
of Y . Except in the ordinary linear regression model, it is difficult to interpret
the individual parameters if the model is stated in terms of C(Y |X). In the
model C 0 (Y |X) = X = 0 + 1 X1 + . . . + p Xp , the regression parameter
0
j is interpreted as the change in the property C of Y per unit change in
the descriptor variable Xj , all other descriptors remaining constanta :
C 0 (Y |X1 , X2 , . . . , Xj , . . . , Xp ).
(2.10)
In the ordinary linear regression model, for example, j is the change in
expected value of Y per unit change in Xj . In the logistic regression model
j is the change in log odds that Y = 1 per unit change in Xj . When a
non-interacting Xj is a dichotomous variable or a continuous one that is
linearly related to C 0 , Xj is represented by a single term in the model and
its contribution is described fully by j .
In all that follows, we drop the 0 from C 0 and assume that C(Y |X) is the
property of Y that is linearly related to the weighted sum of the Xs.
j

= C 0 (Y |X1 , X2 , . . . , Xj + 1, . . . , Xp )

2.3.1 Nominal Predictors
Suppose that we wish to model the e↵ect of two or more treatments and be
able to test for di↵erences between the treatments in some property of Y .
A nominal or polytomous factor such as treatment group having k levels, in
which there is no definite ordering of categories, is fully described by a series of
a

Note that it is not necessary to “hold constant” all other variables to be able to
interpret the e↵ect of one predictor. It is sufficient to hold constant the weighted sum
of all the variables other than Xj . And in many cases it is not physically possible to
hold other variables constant while varying one, e.g., when a model contains X and
X 2 (David Hoaglin, personal communication).

16

2 General Aspects of Fitting Regression Models

k 1 binary indicator variables (sometimes called dummy variables). Suppose
that there are four treatments, J, K, L, and M , and the treatment factor is
denoted by T . The model can be written as
C(Y |T = J) =

0

C(Y |T = K) =
C(Y |T = L) =

C(Y |T = M ) =

0

+

1

0

+

2

0

+

3.

(2.11)

The four treatments are thus completely specified by three regression parameters and one intercept that we are using to denote treatment J, the reference
treatment. This model can be written in the previous notation as
C(Y |T ) = X =

0

+

1 X1

+

2 X2

+

3 X3 ,

(2.12)

where
X1 = 1 if T = K, 0 otherwise
X2 = 1 if T = L, 0 otherwise

(2.13)

X3 = 1 if T = M, 0 otherwise.
For treatment J (T = J), all three Xs are zero and C(Y |T = J) = 0 .
The test for any di↵erences in the property C(Y ) between treatments is
H0 : 1 = 2 = 3 = 0.
This model is an analysis of variance or k-sample-type model. If there are
other descriptor covariables in the model, it becomes an analysis of covariance-type model.

2.3.2 Interactions
Suppose that a model has descriptor variables X1 and X2 and that the e↵ect
of the two Xs cannot be separated; that is the e↵ect of X1 on Y depends on
the level of X2 and vice versa. One simple way to describe this interaction is
to add the constructed variable X3 = X1 X2 to the model:
C(Y |X) =

0

+

1 X1

+

2 X2

+

3 X1 X2 .

(2.14)

It is now difficult to interpret 1 and 2 in isolation. However, we may quantify the e↵ect of a one-unit increase in X1 if X2 is held constant as
C(Y |X1 + 1, X2 )

=
+

C(Y |X1 , X2 )
0

+

3 (X1

1 (X1

+ 1) +

+ 1)X2

2 X2

(2.15)

2.3 Interpreting Model Parameters

17

Table 2.1

Parameter
0
1
2
3

Meaning
C(Y |age = 0, sex = m)
C(Y |age = x + 1, sex = m) C(Y |age = x, sex = m)
C(Y |age = 0, sex = f ) C(Y |age = 0, sex = m)
C(Y |age = x + 1, sex = f ) C(Y |age = x, sex = f )
[C(Y |age = x + 1, sex = m) C(Y |age = x, sex = m)]
[
=

0
1

+

+

1 X1

+

2 X2

+

3 X1 X2 ]

3 X2 .

Likewise, the e↵ect of a one-unit increase in X2 on C if X1 is held constant is
2 + 3 X1 . Interactions can be much more complex than can be modeled with
a product of two terms. If X1 is binary, the interaction may take the form
of a di↵erence in shape (and/or distribution) of X2 versus C(Y ) depending
on whether X1 = 0 or X1 = 1 (e.g., logarithm vs. square root). When both
variables are continuous, the possibilities are much greater (this case is discussed later). Interactions among more than two variables can be exceedingly
complex.

2.3.3 Example: Inference for a Simple Model
Suppose we postulated the model
C(Y |age, sex) =

0

+

1 age

+

2 [sex

= f] +

3 age[sex

= f ],

where [sex = f ] is a 0–1 indicator variable for sex = female; the reference cell
is sex = male corresponding to a zero value of the indicator variable. This is
a model that assumes
1. age is linearly related to C(Y ) for males,
2. age is linearly related to C(Y ) for females, and
3. whatever distribution, variance, and independence assumptions are appropriate for the model being considered.
We are thus assuming that the interaction between age and sex is simple;
that is it only alters the slope of the age e↵ect. The parameters in the model
have interpretations shown in Table 2.1. 3 is the di↵erence in slopes (female
– male).
There are many useful hypotheses that can be tested for this model. First
let’s consider two hypotheses that are seldom appropriate although they are
routinely tested.
1. H0 :

1

= 0: This tests whether age is associated with Y for males.

18

2. H0 :

2 General Aspects of Fitting Regression Models
2

= 0: This tests whether sex is associated with Y for zero-year olds.

Now consider more useful hypotheses. For each hypothesis we should write
what is being tested, translate this to tests in terms of parameters, write the
alternative hypothesis, and describe what the test has maximum power to
detect. The latter component of a hypothesis test needs to be emphasized, as
almost every statistical test is focused on one specific pattern to detect. For
example, a test of association against an alternative hypothesis that a slope
is nonzero will have maximum power when the true association is linear. If
the true regression model is exponential in X, a linear regression test will
have some power to detect “non-flatness” but it will not be as powerful as
the test from a well-specified exponential regression e↵ect. If the true e↵ect
is U-shaped, a test of association based on a linear model will have almost no
power to detect association. If one tests for association against a quadratic
(parabolic) alternative, the test will have some power to detect a logarithmic
shape but it will have very little power to detect a cyclical trend having
multiple “humps.” In a quadratic regression model, a test of linearity against
a quadratic alternative hypothesis will have reasonable power to detect a
quadratic nonlinear e↵ect but very limited power to detect a multiphase
cyclical trend. Therefore in the tests in Table 2.2 keep in mind that power is
maximal when linearity of the age relationship holds for both sexes. In fact
it may be useful to write alternative hypotheses as, for example, “Ha : age is
associated with C(Y ), powered to detect a linear relationship.”
Note that if there is an interaction e↵ect, we know that there is both an
age and a sex e↵ect. However, there can also be age or sex e↵ects when the
lines are parallel. That’s why the tests of total association have 2 d.f.

2.4 Relaxing Linearity Assumption for Continuous
Predictors
2.4.1 Avoiding Categorization
Relationships among variables are seldom linear, except in special cases such
as when one variable is compared with itself measured at a di↵erent time.
It is a common belief among practitioners who do not study bias and efficiency in depth that the presence of non-linearity should be dealt with by
chopping continuous variables into intervals. Nothing could be more disastrous.13, 14, 17, 44, 81, 181, 183, 211, 287, 293, 371, 439, 458, 516, 528, 554, 591, 638
Problems caused by dichotomization include the following.
1. Estimated values will have reduced precision, and associated tests will have reduced power.
2. Categorization assumes that the relationship between the predictor and the response is flat within intervals; this assumption is far less reasonable than a linearity
assumption in most cases.

2.4 Relaxing Linearity Assumption for Continuous Predictors

19

Table 2.2 Most Useful Tests for Linear Age ⇥ Sex Model

Null or Alternative Hypothesis
E↵ect of age is independent of sex or
E↵ect of sex is independent of age or
Age and sex are additive
Age e↵ects are parallel
Age interacts with sex
Age modifies e↵ect of sex
Sex modifies e↵ect of age
Sex and age are non-additive (synergistic)
Age is not associated with Y
Age is associated with Y
Age is associated with Y for either
Females or males
Sex is not associated with Y
Sex is associated with Y
Sex is associated with Y for some
Value of age
Neither age nor sex is associated with Y
Either age or sex is associated with Y

H0 :

3

Ha :

3

H0 :
Ha :

H0 :
Ha :

H0 :
Ha :

1
1

2
2

1
1

Mathematical
Statement
=0

6= 0

= 3=0
6= 0 or 3 6= 0
= 3=0
6= 0 or 3 6= 0
= 2=
6= 0 or

3
2

=0
6= 0 or

3

6= 0

3. To make a continuous predictor be more accurately modeled when categorization
is used, multiple intervals are required. The needed indicator variables will spend
more degrees of freedom than will fitting a smooth relationship, hence power and
precision will su↵er. And because of sample size limitations in the very low and
very high range of the variable, the outer intervals (e.g., outer quintiles) will be
wide, resulting in significant heterogeneity of subjects within those intervals, and
residual confounding.
4. Categorization assumes that there is a discontinuity in response as interval boundaries are crossed. Other than the e↵ect of time (e.g., an instant stock price drop
after bad news), there are very few examples in which such discontinuities have
been shown to exist.
5. Categorization only seems to yield interpretable estimates such as odds ratios. For
example, suppose one computes the odds ratio for stroke for persons with a systolic
blood pressure > 160 mmHg compared with persons with a blood pressure  160
mmHg. The interpretation of the resulting odds ratio will depend on the exact
distribution of blood pressures in the sample (the proportion of subjects > 170,
> 180, etc.). On the other hand, if blood pressure is modeled as a continuous
variable (e.g., using a regression spline, quadratic, or linear e↵ect) one can estimate
the ratio of odds for exact settings of the predictor, e.g., the odds ratio for 200
mmHg compared with 120 mmHg.
6. Categorization does not condition on full information. When, for example, the
risk of stroke is being assessed for a new subject with a known blood pressure
(say 162 mmHg), the subject does not report to her physician ”my blood pressure
exceeds 160” but rather reports 162 mmHg. The risk for this subject will be much
lower than that of a subject with a blood pressure of 200 mmHg.

20

2 General Aspects of Fitting Regression Models

7. If cutpoints are determined in a way that is not blinded to the response variable, calculation of P -values and confidence intervals requires special simulation
techniques; ordinary inferential methods are completely invalid. For example, if
cutpoints are chosen by trial and error in a way that utilizes the response, even
informally, ordinary P -values will be too small and confidence intervals will not
have the claimed coverage probabilities. The correct Monte-Carlo simulations must
take into account both multiplicities and uncertainty in the choice of cutpoints.
For example, if a cutpoint is chosen that minimizes the P -value and the resulting
P -value is 0.05, the true type I error can easily be above 0.5293 .
8. Likewise, categorization that is not blinded to the response variable results in
biased e↵ect estimates17, 554 .
9. ”Optimal” cutpoints do not replicate over studies. Hollander et al.293 state that
“. . . the optimal cutpoint approach has disadvantages. One of these is that in almost every study where this method is applied, another cutpoint will emerge.
This makes comparisons across studies extremely difficult or even impossible. Altman et al. point out this problem for studies of the prognostic relevance of the
S-phase fraction in breast cancer published in the literature. They identified 19
di↵erent cutpoints used in the literature; some of them were solely used because
they emerged as the ’optimal’ cutpoint in a specific data set. In a meta-analysis
on the relationship between cathepsin-D content and disease-free survival in nodenegative breast cancer patients, 12 studies were in included with 12 di↵erent cutpoints . . . Interestingly, neither cathepsin-D nor the S-phase fraction are recommended to be used as prognostic markers in breast cancer in the recent update
of the American Society of Clinical Oncology.” Giannoni et al.211 demonstrated
that many claimed “optimal cutpoints” are just the observed median values in the
sample, which happens to optimize statistical power for detecting a separation in
outcomes and have nothing to do with true outcome thresholds. Disagreements in
cutpoints (which are bound to happen whenever one searches for things that do
not exist) cause severe interpretation problems. One study may provide an odds
ratio for comparing body mass index (BMI) > 30 with BMI  30, another for
comparing BMI > 28 with BMI  28. Neither of these odds ratios has a good
definition and the two estimates are not comparable.
10. Cutpoints are arbitrary and manipulatable; cutpoints can be found that can result
in both positive and negative associations638 .
11. If a confounder is adjusted for by categorization, there will be residual confounding
that can be explained away by inclusion of the continuous form of the predictor
in the model in addition to the categories.

When cutpoints are chosen using Y , categorization represents one of those
few times in statistics where both type I and type II errors are elevated.
A scientific quantity is a quantity which can be defined outside of the
specifics of the current experiment. The kind of high:low estimates that result from categorizing a continuous variable are not scientific quantities; their
interpretation depends on the entire sample distribution of continuous measurements within the chosen intervals.
To summarize problems with categorization it is useful to examine its
e↵ective assumptions. Suppose one assumes there is a single cutpoint c for
predictor X. Assumptions implicit in seeking or using this cutpoint include
(1) the relationship between X and the response Y is discontinuous at X = c
and only X = c; (2) c is correctly found as the cutpoint; (3) X vs. Y is
flat to the left of c; (4) X vs. Y is flat to the right of c; (5) the “optimal”
cutpoint does not depend on the values of other predictors. Failure to have

2.4 Relaxing Linearity Assumption for Continuous Predictors

21

these assumptions satisfied will result in great error in estimating c (because
it doesn’t exist), low predictive accuracy, serious lack of model fit, residual
confounding, and overestimation of e↵ects of remaining variables.
A better approach that maximizes power and that only assumes a smooth
relationship is to use regression splines for predictors that are not known
to predict linearly. Use of flexible parametric approaches such as this allows
standard inference techniques (P -values, confidence limits) to be used, as
will be described below. Before introducing splines, we consider the simplest
approach to allowing for nonlinearity.

2.4.2 Simple Nonlinear Terms
If a continuous predictor is represented, say, as X1 in the model, the model
is assumed to be linear in X1 . Often, however, the property of Y of interest
does not behave linearly in all the predictors. The simplest way to describe
a nonlinear e↵ect of X1 is to include a term for X2 = X12 in the model:
C(Y |X1 ) =

0

+

1 X1

+

2
2 X1 .

(2.16)

If the model is truly linear in X1 , 2 will be zero. This model formulation
allows one to test H0 : model is linear in X1 against Ha : model is quadratic
(parabolic) in X1 by testing H0 : 2 = 0.
Nonlinear e↵ects will frequently not be of a parabolic nature. If a transformation of the predictor is known to induce linearity, that transformation
(e.g., log(X)) may be substituted for the predictor. However, often the transformation is not known. Higher powers of X1 may be included in the model
to approximate many types of relationships, but polynomials have some undesirable properties (e.g., undesirable peaks and valleys, and the fit in one
region of X can be greatly a↵ected by data in other regions426 ) and will not
adequately fit many functional forms.153 For example, polynomials do not
adequately fit logarithmic functions or “threshold” e↵ects.

2.4.3 Splines for Estimating Shape of Regression
Function and Determining Predictor
Transformations
A draftsman’s spline is a flexible strip of metal or rubber used to draw curves.
Spline functions are piecewise polynomials used in curve fitting. That is, they
are polynomials within intervals of X that are connected across di↵erent
intervals of X. Splines have been used, principally in the physical sciences,
to approximate a wide variety of functions. The simplest spline function is a

22

2 General Aspects of Fitting Regression Models

linear spline function, a piecewise linear function. Suppose that the x axis is
divided into intervals with endpoints at a, b, and c, called knots. The linear
spline function is given by
f (X) =

0

+

1X

+

2 (X

a)+ +

3 (X

b)+ +

4 (X

c)+ ,

(2.17)

where
(u)+ = u, u > 0,
0, u  0.

(2.18)

The number of knots can vary depending on the amount of available data for
fitting the function. The linear spline function can be rewritten as
f (X)

=
=
=

0

+

0

+

1X

0

+

1X

+

+

Xa

1 X,
2 (X

a)

a) +

3 (X

=

0+

2 (X
1X +

2 (X

a)

+

3 (X

b) +

4 (X

c)

a<Xb

(2.19)

b) b < X  c
c < X.

f(X)

A linear spline is depicted in Figure 2.1.

0

1

2

3

4

5

X
Fig. 2.1 A linear spline function with knots at a = 1, b = 3, c = 5.

6

2.4 Relaxing Linearity Assumption for Continuous Predictors

23

The general linear regression model can be written assuming only piecewise
linearity in X by incorporating constructed variables X2 , X3 , and X4 :
C(Y |X) = f (X) = X ,
where X =

0

+

1 X1

+

2 X2

+

3 X3

+

4 X4 ,

X1 = X X2 = (X
X3 = (X

b)+ X4 = (X

(2.20)

and
a)+
c)+ .

(2.21)

By modeling a slope increment for X in an interval (a, b] in terms of (X a)+ ,
the function is constrained to join (“meet”) at the knots. Overall linearity in
X can be tested by testing H0 : 2 = 3 = 4 = 0.

2.4.4 Cubic Spline Functions
Although the linear spline is simple and can approximate many common
relationships, it is not smooth and will not fit highly curved functions well.
These problems can be overcome by using piecewise polynomials of order
higher than linear. Cubic polynomials have been found to have nice properties
with good ability to fit sharply curving shapes. Cubic splines can be made to
be smooth at the join points (knots) by forcing the first and second derivatives
of the function to agree at the knots. Such a smooth cubic spline function
with three knots (a, b, c) is given by
f (X) =
+

0

+

4 (X

1X

+

2X

a)3+ +

2

+

5 (X

3
3X
b)3+

+

6 (X

c)3+

(2.22)

=X
with the following constructed variables:
X1 = X X 2 = X 2
X3 = X 3 X4 = (X
X5 = (X

b)3+

X6 = (X

a)3+

(2.23)

c)3+ .

If the cubic spline function has k knots, the function will require estimating k + 3 regression coefficients besides the intercept. See Section 2.4.6 for
information on choosing the number and location of knots.
There are more numerically stable ways to form a design matrix for cubic
spline functions that are based on B-splines instead of the truncated power
basis149, 571 used here. However, B-splines are more complex and do not allow
for extrapolation beyond the outer knots, and the truncated power basis

1

24

2

2 General Aspects of Fitting Regression Models

seldom presents estimation problems (see Section 4.6) when modern methods
such as the Q–R decomposition are used for matrix inversion.

2.4.5 Restricted Cubic Splines

3

Stone and Koo589 have found that cubic spline functions do have a drawback
in that they can be poorly behaved in the tails, that is before the first knot
and after the last knot. They cite advantages of constraining the function
to be linear in the tails. Their restricted cubic spline function (also called
natural splines) has the additional advantage that only k 1 parameters
must be estimated (besides the intercept) as opposed to k + 3 parameters
with the unrestricted cubic spline. The restricted spline function with k knots
t1 , . . . , tk is given by153
f (X) =

0

+

1 X1

+

where X1 = X and for j = 1, . . . , k
Xj+1 = (X
+ (X

tj )3+

2 X2

k 1 Xk 1 ,

(2.24)

2,

(X

tk )3+ (tk 1

+ ... +

tk

3
1 )+ (tk

tj )/(tk

tk

tj )/(tk

tk

1)

1 ).

(2.25)

It can be shown that Xj is linear in X for X
tk . For numerical behavior
and to put all basis functions for X on the same scale, R Hmisc and rms
package functions by default divide the terms in Eq. 2.25 by
⌧ = (tk

t1 ) 2 .

(2.26)

Figure 2.2 displays the ⌧ -scaled spline component variables Xj for j =
2, 3, 4 and k = 5 and one set of knots. The left graph magnifies the lower
portion of the curves.
r e q u i r e ( Hmisc )
x

r c s p l i n e . e v a l ( seq (0 ,1 , .01 ) ,
k n o t s=s e q ( . 0 5 , . 9 5 , l e n g t h =5) , i n c l x=T)
xm
x
xm [ xm > . 0 1 0 6 ]
NA
m a t p l o t ( x [ , 1 ] , xm, t y p e=” l ” , y l i m=c ( 0 , . 0 1 ) ,
x l a b=e x p r e s s i o n (X) , y l a b= ’ ’ , l t y =1)
matplot ( x [ , 1 ] , x ,
t y p e=” l ” ,
x l a b=e x p r e s s i o n (X) , y l a b= ’ ’ , l t y =1)

Figure 2.3 displays some typical shapes of restricted cubic spline functions
with k = 3, 4, 5, and 6. These functions were generated using random .
x
s e q ( 0 , 1 , l e n g t h =300)
f o r ( nk i n 3 : 6 ) {
s e t . s e e d ( nk )

2.4 Relaxing Linearity Assumption for Continuous Predictors

0.010

1.0

0.008

0.8

0.006

0.6

0.004

0.4

0.002

0.2

0.000

0.0
0.0

0.4

X

0.8

0.0

25

0.4

0.8

X

Fig. 2.2 Restricted cubic spline component variables for k = 5 and knots at X =
.05, .275, .5, .725, and .95. Nonlinear basis functions are scaled by au. The left panel
is a y–magnification of the right panel. Fitted functions such as those in Figure 2.3
will be linear combinations of these basis functions as long as knots are at the same
locations used here.

}

knots
s e q ( . 0 5 , . 9 5 , l e n g t h=nk )
xx
r c s p l i n e . e v a l ( x , k n o t s=knots , i n c l x=T)
f o r ( i i n 1 : ( nk
1))
xx [ , i ]
( xx [ , i ]
min ( xx [ , i ] ) ) /
(max( xx [ , i ] )
min ( xx [ , i ] ) )
f o r ( i in 1 : 20) {
beta
2 ⇤ r u n i f ( nk 1 )
1
xbeta
xx %⇤% b e t a + 2 ⇤ r u n i f ( 1 )
1
xbeta
( xbeta
min ( x b e t a ) ) /
(max( x b e t a )
min ( x b e t a ) )
i f ( i == 1 ) {
p l o t ( x , xbeta , t y p e=” l ” , l t y =1 ,
x l a b=e x p r e s s i o n (X) , y l a b= ’ ’ , bty=” l ” )
t i t l e ( sub=p a s t e ( nk , ” k n o t s ” ) , a d j =0 , c e x=. 7 5 )
f o r ( j i n 1 : nk )
arrows ( knots [ j ] , .04 , knots [ j ] ,
.03 ,
a n g l e =20 , l e n g t h=. 0 7 , lwd=1 . 5 )
}
e l s e l i n e s ( x , xbeta , c o l=i )
}

26

2 General Aspects of Fitting Regression Models

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0
0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

X
3 knots
1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0
0.2

0.8

1.0

0.6

0.8

1.0

4 knots

1.0

0.0

0.6

X

0.4

0.6

0.8

1.0

0.0

0.2

0.4

X

X

5 knots

6 knots

Fig. 2.3 Some typical restricted cubic spline functions for k = 3, 4, 5, 6. The y–axis
is X . Arrows indicate knots. These curves were derived by randomly choosing values
of subject to standard deviations of fitted functions being normalized.

Once 0 , . . . ,
in the form

k 1

are estimated, the restricted cubic spline can be restated

f (X) =

0

+

1X

+

2 (X

t1 )3+ +

3 (X

t2 )3+

2.4 Relaxing Linearity Assumption for Continuous Predictors

+... +
by dividing

2, . . . ,

k 1

k

=[

2 (t1

tk ) +

k+1

=[

2 (t1

tk

1)

tk )3+

k+1 (X

(2.27)

by ⌧ (Eq. 2.26) and computing

3 (t2

+

27

tk ) + . . . +

3 (t2

tk

1)

k 1 (tk 2

+ ... +

tk )]/(tk

k 1 (tk 2

tk

tk

1)

(2.28)

1 )]/(tk 1

tk ).

A test of linearity in X can be obtained by testing
H0 :

2

=

3

= ... =

k 1

= 0.

(2.29)
4

The truncated power basis for restricted cubic splines does allow for rational (i.e., linear) extrapolation beyond the outer knots. However, when the
outer knots are in the tails of the data, extrapolation can still be dangerous.
When nonlinear terms in Equation 2.25 are normalized, for example, by
dividing them by the square of the di↵erence in the outer knots to make all
terms have units of X, the ordinary truncated power basis has no numerical
difficulties when modern matrix algebra software is used.

2.4.6 Choosing Number and Position of Knots
We have assumed that the locations of the knots are specified in advance;
that is, the knot locations are not treated as free parameters to be estimated.
If knots were free parameters, the fitted function would have more flexibility
but at the cost of instability of estimates, statistical inference problems, and
inability to use standard regression modeling software for estimating regression parameters.
How then does the analyst pre-assign knot locations? If the regression
relationship were described by prior experience, pre-specification of knot locations would be easy. For example, if a function were known to change
curvature at X = a, a knot could be placed at a. However, in most situations
there is no way to pre-specify knots. Fortunately, Stone587 has found that
the location of knots in a restricted cubic spline model is not very crucial in
most situations; the fit depends much more on the choice of k, the number of
knots. Placing knots at fixed quantiles (percentiles) of a predictor’s marginal
distribution is a good approach in most datasets. This ensures that enough
points are available in each interval, and also guards against letting outliers
overly influence knot placement. Recommended equally spaced quantiles are
shown in Table 2.3.
The principal reason for using less extreme default quantiles for k = 3 and
more extreme ones for k = 7 is that one usually uses k = 3 for small sample
sizes and k = 7 for large samples. When the sample size is less than 100, the

5

28

2 General Aspects of Fitting Regression Models

Table 2.3 Default quantiles for knots

k
Quantiles
3
.10 .5 .90
4
.05 .35 .65 .95
5
.05 .275 .5 .725 .95
6 .05 .23 .41 .59 .77 .95
7 .025 .1833 .3417 .5 .6583 .8167 .975

outer quantiles should be replaced by the fifth smallest and fifth largest data
points, respectively.589 What about the choice of k? The flexibility of possible
fits must be tempered by the sample size available to estimate the unknown
parameters. Stone587 has found that more than 5 knots are seldom required
in a restricted cubic spline model. The principal decision then is between
k = 3, 4, or 5. For many datasets, k = 4 o↵ers an adequate fit of the model
and is a good compromise between flexibility and loss of precision caused
by overfitting a small sample. When the sample size is large (e.g., n
100
with a continuous uncensored response variable), k = 5 is a good choice.
Small samples (< 30, say) may require the use of k = 3. Akaike’s information
criterion (AIC, Section 9.8.1) can be used for a data-based choice of k. The
value of k maximizing the model likelihood ratio 2 2k would be the best
“for the money” using AIC.
The analyst may wish to devote more knots to variables that are thought
to be more important, and risk lack of fit for less important variables. In this
way the total number of estimated parameters can be controlled (Section 4.1).

2.4.7 Nonparametric Regression
One of the most important results of an analysis is the estimation of the
tendency (trend) of how X relates to Y . This trend is useful in its own right
and it may be sufficient for obtaining predicted values in some situations,
but trend estimates can also be used to guide formal regression modeling (by
suggesting predictor variable transformations) and to check model assumptions.
Nonparametric smoothers are excellent tools for determining the shape
of the relationship between a predictor and the response. The standard nonparametric smoothers work when one is interested in assessing one continuous
predictor at a time and when the property of the response that should be linearly related to the predictor is a standard measure of central tendency. For
example, when C(Y ) is E(Y ) or Pr[Y = 1], standard smoothers are useful,
but when C(Y ) is a measure of variability or a rate (instantaneous risk), or

2.4 Relaxing Linearity Assumption for Continuous Predictors

29

when Y is only incompletely measured for some subjects (e.g., Y is censored
for some subjects), simple smoothers will not work.
The oldest and simplest nonparametric smoother is the moving average.
Suppose that the data consist of the points X = 1, 2, 3, 5, and 8, with the
corresponding Y values 2.1, 3.8, 5.7, 11.1, and 17.2. To smooth the relationship
we could estimate E(Y |X = 2) by (2.1 + 3.8 + 5.7)/3 and E(Y |X = (2 + 3 +
5)/3) by (3.8 + 5.7 + 11.1)/3. Note that overlap is fine; that is one point may
be contained in two sets that are averaged. You can immediately see that the
simple moving average has a problem in estimating E(Y ) at the outer values
of X. The estimates are quite sensitive to the choice of the number of points
(or interval width) to use in “binning” the data.
A moving least squares linear regression smoother is far superior to a
moving flat line smoother (moving average). Cleveland’s108 moving linear
regression smoother loess has become the most popular smoother. To obtain
the smoothed value of Y at X = x, we take all the data having X values
within a suitable interval about x. Then a linear regression is fitted to all
of these points, and the predicted value from this regression at X = x is
taken as the estimate of E(Y |X = x). Actually, loess uses weighted least
squares estimates, which is why it is called a locally weighted least squares
method. The weights are chosen so that points near X = x are given the
most weightb in the calculation of the slope and intercept. Surprisingly, a
good default choice for the interval about x is an interval containing 2/3 of
the data points! The weighting function is devised so that points near the
extremes of this interval receive almost no weight in the calculation of the
slope and intercept.
Because loess uses a moving straight line rather than a moving flat one,
it provides much better behavior at the extremes of the Xs. For example,
one can fit a straight line to the first three data points and then obtain the
predicted value at the lowest X, which takes into account that this X is not
the middle of the three Xs.
loess obtains smoothed values for E(Y ) at each observed value of X.
Estimates for other Xs are obtained by linear interpolation.
The loess algorithm has another component. After making an initial estimate of the trend line, loess can look for outliers o↵ this trend. It can
then delete or down-weight those apparent outliers to obtain a more robust
trend estimate. Now, di↵erent points will appear to be outliers with respect
to this second trend estimate. The new set of outliers is taken into account
and another trend line is derived. By default, the process stops after these
three iterations. loess works exceptionally well for binary Y as long as the
iterations that look for outliers are not done, that is only one iteration is
performed.
For a single X, Friedman’s “super smoother”203 is another efficient and
flexible nonparametric trend estimator. For both loess and the super smoother
b

This weight is not to be confused with the regression
coefficient; rather the weights
P
are w1 , w2 , . . . , wn and the fitting criterion is n
w
(Y
Yˆi )2 .
i
i
i

30

6

7

2 General Aspects of Fitting Regression Models

the amount of smoothing can be controlled by the analyst. Hastie and
Tibshirani270 provided an excellent description of smoothing methods and
developed a generalized additive model for multiple Xs, in which each
continuous predictor is fitted with a nonparametric smoother (see Chapter 16). Interactions are not allowed. Cleveland et al.93 have extended twodimensional smoothers to multiple dimensions without assuming additivity.
Their local regression model is feasible for up to four or so predictors. Local
regression models are extremely flexible, allowing parts of the model to be
parametrically specified, and allowing the analyst to choose the amount of
smoothing or the e↵ective number of degrees of freedom of the fit.
Smoothing splines are related to nonparametric smoothers. Here a knot
is placed at every data point, but a penalized likelihood is maximized to
derive the smoothed estimates. Gray232, 233 developed a general method that
is halfway between smoothing splines and regression splines. He pre-specified,
say, 10 fixed knots, but uses a penalized likelihood for estimation. This allows
the analyst to control the e↵ective number of degrees of freedom used.
Besides using smoothers to estimate regression relationships, smoothers
are valuable for examining trends in residual plots. See Sections 14.6 and 21.2
for examples.

2.4.8 Advantages of Regression Splines over Other
Methods
There are several advantages of regression splines:266
1. Parametric splines are piecewise polynomials and can be fitted using any
existing regression program after the constructed predictors are computed.
Spline regression is equally suitable to multiple linear regression, survival
models, and logistic models for discrete outcomes.
2. Regression coefficients for the spline function are estimated using standard techniques (maximum likelihood or least squares), and statistical
inferences can readily be drawn. Formal tests of no overall association,
linearity, and additivity can readily be constructed. Confidence limits for
the estimated regression function are derived by standard theory.
3. The fitted spline function directly estimates the transformation that a
predictor should receive to yield linearity in C(Y |X). The fitted spline
transformation sometimes suggests a simple transformation (e.g., square
root) of a predictor that can be used if one is not concerned about the
proper number of degrees of freedom for testing association of the predictor
with the response.
4. The spline function can be used to represent the predictor in the final
model. Nonparametric methods do not yield a prediction equation.

2.5 Recursive Partitioning: Tree-Based Models

31

5. Splines can be extended to non-additive models (see below). Multidimensional nonparametric estimators often require burdensome computations.

2.5 Recursive Partitioning: Tree-Based Models
Breiman et al.68 have developed an essentially model-free approach called
classification and regression trees (CART), a form of recursive partitioning.
For some implementations of CART, we say “essentially” model-free since a
model-based statistic is sometimes chosen as a splitting criterion. The essence
of recursive partitioning is as follows.
1. Find the predictor so that the best possible binary split on that predictor
has a larger value of some statistical criterion than any other split on any
other predictor. For ordinal and continuous predictors, the split is of the
form X < c versus X
c. For polytomous predictors, the split involves
finding the best separation of categories, without preserving order.
2. Within each previously formed subset, find the best predictor and best
split that maximizes the criterion in the subset of observations passing the
previous split.
3. Proceed in like fashion until fewer than k observations remain to be split,
where k is typically 20 to 100.
4. Obtain predicted values using a statistic that summarizes each terminal
node (e.g., mean or proportion).
5. Prune the tree backward so that a tree with the same number of nodes
developed on 0.9 of the data validates best on the remaining 0.1 of the
data (average over the 10 cross-validations). Alternatively, shrink the node
estimates toward the mean, using a progressively stronger shrinkage factor,
until the best cross-validation results.
Tree models have the advantage of not requiring any functional form for
the predictors and of not assuming additivity of predictors (i.e., recursive partitioning can identify complex interactions). Trees can deal with missing data
flexibly. They have the disadvantages of not utilizing continuous variables effectively and of overfitting in three directions: searching for best predictors,
for best splits, and searching multiple times. The penalty for the extreme
amount of data searching required by recursive partitioning surfaces when
the tree does not cross-validate optimally until it is pruned all the way back
to two or three splits. Thus reliable trees are often not very discriminating.
Tree models are especially useful in messy situations or settings in which
overfitting is not so problematic, such as confounder adjustment using
propensity scores114 or in missing value imputation. A major advantage of
tree modeling is savings of analyst time, but this is o↵set by the underfitting
needed to make trees validate.

8

9

32

2 General Aspects of Fitting Regression Models

2.6 Multiple Degree of Freedom Tests of Association
When a factor is a linear or binary term in the regression model, the test
of association for that factor with the response involves testing only a single
regression parameter. Nominal factors and predictors that are represented
as a quadratic or spline function require multiple regression parameters to
be tested simultaneously in order to assess association with the response.
For a nominal factor having k levels, the overall ANOVA-type test with k
1 d.f. tests whether there are any di↵erences in responses between the k
categories. It is recommended that this test be done before attempting to
interpret individual parameter estimates. If the overall test is not significant,
it can be dangerous to rely on individual pairwise comparisons because the
type I error will be increased. Likewise, for a continuous predictor for which
linearity is not assumed, all terms involving the predictor should be tested
simultaneously to check whether the factor is associated with the outcome.
This test should precede the test for linearity and should usually precede the
attempt to eliminate nonlinear terms. For example, in the model
C(Y |X) =

0

+

1 X1

+

2 X2

+

2
3 X2 ,

(2.30)

one should test H0 : 2 = 3 = 0 with 2 d.f. to assess association between
X2 and outcome. In the five-knot restricted cubic spline model
C(Y |X) =

0

+

1X

+

2X

0

+

3X

00

+

4X

000

,

(2.31)

the hypothesis H0 : 1 = . . . = 4 = 0 should be tested with 4 d.f. to
assess whether there is any association between X and Y . If this 4 d.f. test
is insignificant, it is dangerous to interpret the shape of the fitted spline
function because the hypothesis that the overall function is flat has not been
rejected.
A dilemma arises when an overall test of association, say one having 4
d.f., is insignificant, the 3 d.f. test for linearity is insignificant, but the 1 d.f.
test for linear association, after deleting nonlinear terms, becomes significant.
Had the test for linearity been borderline significant, it would not have been
warranted to drop these terms in order to test for a linear association. But
with the evidence for nonlinearity not very great, one could attempt to test
for association with 1 d.f. This however is not fully justified, because the 1
d.f. test statistic does not have a 2 distribution with 1 d.f. since pretesting
was done. The original 4 d.f. test statistic does have a 2 distribution with 4
d.f. because it was for a pre-specified test.
For quadratic regression, Grambsch and O’Brien229 showed that the 2
d.f. test of association is nearly optimal when pretesting is done, even when
the true relationship is linear. They considered an ordinary regression model
E(Y |X) = 0 + 1 X + 2 X 2 and studied tests of association between X and
Y . The strategy they studied was as follows. First, fit the quadratic model

2.7 Assessment of Model Fit

33

and obtain the partial test of H0 : 2 = 0, that is the test of linearity. If this
partial F -test is significant at the ↵ = 0.05 level, report as the final test of
association between X and Y the 2 d.f. F -test of H0 : 1 = 2 = 0. If the
test of linearity is insignificant, the model is refitted without the quadratic
term and the test of association is then a 1 d.f. test, H0 : 1 = 0| 2 = 0.
Grambsch and O’Brien demonstrated that the type I error from this twostage test is greater than the stated ↵, and in fact a fairly accurate P -value
can be obtained if it is computed from an F distribution with 2 numerator
d.f. even when testing at the second stage. This is because in the original
2 d.f. test of association, the 1 d.f. corresponding to the nonlinear e↵ect is
deleted if the nonlinear e↵ect is very small; that is one is retaining the most
significant part of the 2 d.f. F statistic.
If we use a 2 d.f. F critical value to assess the X e↵ect even when X 2 is not
in the model, it is clear that the two-stage approach can only lose power and
hence it has no advantage whatsoever. That is because the sum of squares
due to regression from the quadratic model is greater than the sum of squares
computed from the linear model.

2.7 Assessment of Model Fit
2.7.1 Regression Assumptions
In this section, the regression part of the model is isolated, and methods are
described for validating the regression assumptions or modifying the model
to meet the assumptions. The general linear regression model is
C(Y |X) = X =

0

+

1 X1

+

2 X2

+ ... +

k Xk .

(2.32)

The assumptions of linearity and additivity need to be verified. We begin
with a special case of the general model,
C(Y |X) =

0

+

1 X1

+

2 X2 ,

(2.33)

where X1 is binary and X2 is continuous. One needs to verify that the property of the response C(Y ) is related to X1 and X2 according to Figure 2.4.
There are several methods for checking the fit of this model. The first
method below is based on critiquing the simple model, and the other methods
directly “estimate” the model.
1. Fit the simple linear additive model and critically examine residual plots
for evidence of systematic patterns. For least squares fits one can compute
estimated residuals e = Y X ˆ and box plots of e stratified by X1 and
scatterplots of e versus X1 and Ŷ with trend curves. If one is assuming
constant conditional variance of Y , the spread of the residual distribution

34

2 General Aspects of Fitting Regression Models

C(Y)

X1 = 1

X1 = 0

X2
Fig. 2.4 Regression assumptions for one binary and one continuous predictor

against each of the variables can be checked at the same time. If the normality assumption is needed (i.e., if significance tests or confidence limits
are used), the distribution of e can be compared with a normal distribution with mean zero. Advantage: Simplicity. Disadvantages: Standard
residuals can only be computed for continuous uncensored response variables. The judgment of non-randomness is largely subjective, it is difficult
to detect interaction, and if interaction is present it is difficult to check
any of the other assumptions. Unless trend lines are added to plots, patterns may be difficult to discern if the sample size is very large. Detecting
patterns in residuals does not always inform the analyst of what corrective
action to take, although partial residual plots can be used to estimate the
needed transformations if interaction is absent.
2. Make a scatterplot of Y versus X2 using di↵erent symbols according to
values of X1 . Advantages: Simplicity, and one can sometimes see all regression patterns including interaction. Disadvantages: Scatterplots cannot be drawn for binary, categorical, or censored Y . Patterns are difficult
to see if relationships are weak or if the sample size is very large.
3. Stratify the sample by X1 and quantile groups (e.g., deciles) of X2 . Within
each X1 ⇥ X2 stratum an estimate of C(Y |X1 , X2 ) is computed. If X1 is
continuous, the same method can be used after grouping X1 into quantile
groups. Advantages: Simplicity, ability to see interaction patterns, can
handle censored Y if care is taken. Disadvantages: Subgrouping requires
relatively large sample sizes and does not use continuous factors e↵ectively

2.7 Assessment of Model Fit

35

as it does not attempt any interpolation. The ordering of quantile groups is
not utilized by the procedure. Subgroup estimates have low precision (see
p. 495 for an example). Each stratum must contain enough information
to allow trends to be apparent above noise in the data. The method of
grouping chosen (e.g., deciles vs. quintiles vs. rounding) can alter the shape
of the plot.
4. Fit a nonparametric smoother separately for levels of X1 (Section 2.4.7)
relating X2 to Y . Advantages: All regression aspects of the model can be
summarized efficiently with minimal assumptions. Disadvantages: Does
not easily apply to censored Y , and does not easily handle multiple predictors.
5. Fit a flexible parametric model that allows for most of the departures from
the linear additive model that you wish to entertain. Advantages: One
framework is used for examining the model assumptions, fitting the model,
and drawing formal inference. Degrees of freedom are well defined and
all aspects of statistical inference “work as advertised.” Disadvantages:
Complexity, and it is generally difficult to allow for interactions when
assessing patterns of e↵ects.
The first four methods each have the disadvantage that if confidence limits
or formal inferences are desired it is difficult to know how many degrees of
freedom were e↵ectively used so that, for example, confidence limits will have
the stated coverage probability. For method five, the restricted cubic spline
function is an excellent tool for estimating the true relationship between X2
and C(Y ) for continuous variables without assuming linearity. By fitting a
model containing X2 expanded into k 1 terms, where k is the number of
knots, one can obtain an estimate of the function of X2 that could be used
linearly in the model:

where

Ĉ(Y |X) = ˆ0 + ˆ1 X1 + ˆ2 X2 + ˆ3 X20 + ˆ4 X200
= ˆ0 + ˆ1 X1 + fˆ(X2 ),

(2.34)

fˆ(X2 ) = ˆ2 X2 + ˆ3 X20 + ˆ4 X200 ,

(2.35)

and X20 and X200 are constructed spline variables (when k = 4) as described
previously. We call fˆ(X2 ) the spline-estimated transformation of X2 . Plotting
the estimated spline function fˆ(X2 ) versus X2 will generally shed light on
how the e↵ect of X2 should be modeled. If the sample is sufficiently large,
the spline function can be fitted separately for X1 = 0 and X1 = 1, allowing
detection of even unusual interaction patterns. A formal test of linearity in
X2 is obtained by testing H0 : 3 = 4 = 0, using a computationally efficient
score test, for example (Section 9.2.3).
If the model is nonlinear in X2 , either a transformation suggested by the
spline function plot (e.g., log(X2 )) or the spline function itself (by placing
X2 , X20 , and X200 simultaneously in any model fitted) may be used to describe

36

2 General Aspects of Fitting Regression Models

X2 in the model. If a tentative transformation of X2 is specified, say g(X2 ),
the adequacy of this transformation can be tested by expanding g(X2 ) in a
spline function and testing for linearity. If one is concerned only with prediction and not with statistical inference, one can attempt to find a simplifying
transformation for a predictor by plotting g(X2 ) against fˆ(X2 ) (the estimated
spline transformation) for a variety of g, seeking a linearizing transformation
of X2 . When there are nominal or binary predictors in the model in addition to the continuous predictors, it should be noted that there are no shape
assumptions to verify for the binary/nominal predictors. One need only test
for interactions between these predictors and the others.
If the model contains more than one continuous predictor, all may be expanded with spline functions in order to test linearity or to describe nonlinear
relationships. If one did desire to assess simultaneously, for example, the linearity of predictors X2 and X3 in the presence of a linear or binary predictor
X1 , the model could be specified as
C(Y |X) =
+

0

+

5 X3

1 X1

+

+

0
6 X3

2 X2

+

+

0
3 X2

+

00
4 X2

00
7 X3 ,

(2.36)

where X20 , X200 , X30 , and X300 represent components of four knot restricted cubic
spline functions.
The test of linearity for X2 (with 2 d.f.) is H0 : 3 = 4 = 0. The overall
test of linearity for X2 and X3 is H0 : 3 = 4 = 6 = 7 = 0, with 4 d.f.
But as described further in Section 4.1, even though there are many reasons
for allowing relationships to be nonlinear, there are reasons for not testing
the nonlinear components for significance, as this might tempt the analyst to
simplify the model thus distorting inference.229 Testing for linearity is usually
best done to justify to non-statisticians the need for complexity to explain or
predict outcomes.

2.7.2 Modeling and Testing Complex Interactions
For testing interaction between X1 and X2 (after a needed transformation
may have been applied), often a product term (e.g., X1 X2 ) can be added
to the model and its coefficient tested. A more general simultaneous test of
linearity and lack of interaction for a two-variable model in which one variable
is binary (or is assumed linear) is obtained by fitting the model
C(Y |X) =
+

0

+

1 X1

5 X1 X2 +

0
00
2 X2 + 3 X2 + 4 X2
0
00
6 X1 X2 + 7 X1 X2

+

(2.37)

2.7 Assessment of Model Fit

37

and testing H0 : 3 = . . . = 7 = 0. This formulation allows the shape of the
X2 e↵ect to be completely di↵erent for each level of X1 . There is virtually
no departure from linearity and additivity that cannot be detected from this
expanded model formulation if the number of knots is adequate and X1 is
binary. For binary logistic models, this method is equivalent to fitting two
separate spline regressions in X2 .
Interactions can be complex when all variables are continuous. An approximate approach is to reduce the variables to two transformed variables,
in which case interaction may sometimes be approximated by a single product
of the two new variables. A disadvantage of this approach is that the estimates
of the transformations for the two variables will be di↵erent depending on
whether interaction terms are adjusted for when estimating “main e↵ects.” A
good compromise method involves fitting interactions of the form X1 f (X2 )
and X2 g(X1 ):
C(Y |X) =

0

+

1 X1

+

0
5 X2

+

4 X2

+

+

7 X1 X2

+

0
10 X2 X1

+
+

0
2 X1

+

00
6 X2
0
8 X1 X2 +
00
11 X2 X1

10

00
3 X1

+

00
9 X1 X2

(2.38)

(for k = 4 knots for both variables). The test of additivity is H0 : 7 = 8 =
. . . = 11 = 0 with 5 d.f. A test of lack of fit for the simple product interaction
with X2 is H0 : 8 = 9 = 0, and a test of lack of fit for the simple product
interaction with X1 is H0 : 10 = 11 = 0.
A general way to model and test interactions, although one requiring a
larger number of parameters to be estimated, is based on modeling the X1 ⇥
X2 ⇥ Y relationship with a smooth three-dimensional surface. A cubic spline
surface can be constructed by covering the X1 X2 plane with a grid and
fitting a patch-wise cubic polynomial in two variables. The grid is (ui , vj ), i =
1, . . . , k, j = 1, . . . , k, where knots for X1 are (u1 , . . . , uk ) and knots for X2
are (v1 , . . . , vk ). The number of parameters can be reduced by constraining
the surface to be of the form aX1 + bX2 + cX1 X2 in the lower left and
upper right corners of the plane. The resulting restricted cubic spline surface
is described by a multiple regression model containing spline expansions in
X1 and X2 and all cross-products of the restricted cubic spline components
(e.g., X1 X20 ). If the same number of knots k is used for both predictors,
the number of interaction terms is (k 1)2 . Examples of various ways of
modeling interaction are given in Chapter 10. Spline functions made up of
cross-products of all terms of individual spline functions are called tensor
splines.49, 269
The presence of more than two predictors increases the complexity of tests
for interactions because of the number of two-way interactions and because
of the possibility of interaction e↵ects of order higher than two. For example,
in a model containing age, sex, and diabetes, the important interaction could

11

38

2 General Aspects of Fitting Regression Models

be that older male diabetics have an exaggerated risk. However, higher-order
interactions are often ignored unless specified a priori based on knowledge of
the subject matter. Indeed, the number of two-way interactions alone is often
too large to allow testing them all with reasonable power while controlling
multiple comparison problems. Often, the only two-way interactions we can
a↵ord to test are those that were thought to be important before examining
the data. A good approach is to test for all such pre-specified interaction
e↵ects with a single global (pooled) test. Then, unless interactions involving
only one of the predictors are of special interest, one can either drop all
interactions or retain all of them.
For some problems a reasonable approach is, for each predictor separately,
to test simultaneously the joint importance of all interactions involving that
predictor. For p predictors this results in p tests each with p 1 degrees
of freedom. The multiple comparison problem would then be reduced from
p(p 1)/2 tests (if all two-way interactions were tested individually) to p
tests.
In the fields of biostatistics and epidemiology, some types of interactions
that have consistently been found to be important in predicting outcomes
and thus may be pre-specified are the following.

12

1. Interactions between treatment and the severity of disease being treated.
Patients with little disease can receive little benefit.
2. Interactions involving age and risk factors. Older subjects are generally
less a↵ected by risk factors. They had to have been robust to survive to
their current age with risk factors present.
3. Interactions involving age and type of disease. Some diseases are incurable
and have the same prognosis regardless of age. Others are treatable or
have less e↵ect on younger patients.
4. Interactions between a measurement and the state of a subject during a
measurement. Respiration rate measured during sleep may have greater
predictive value and thus have a steeper slope versus outcome than respiration rate measured during activity.
5. Interaction between menopausal status and treatment or risk factors.
6. Interactions between race and disease.
7. Interactions between calendar time and treatment. Some treatments have
learning curves causing secular trends in the associations.
8. Interactions between month of the year and other predictors, due to seasonal e↵ects.
9. Interaction between the quality and quantity of a symptom, for example,
daily frequency of chest pain ⇥ severity of a typical pain episode.
10. Interactions between study center and treatment.

2.7 Assessment of Model Fit

39

2.7.3 Fitting Ordinal Predictors
For the case of an ordinal predictor, spline functions are not useful unless
there are so many categories that in essence the variable is continuous. When
the number of categories k is small (three to five, say), the variable is usually
modeled as a polytomous factor using indicator variables or equivalently as
one linear term and k 2 indicators. The latter coding facilitates testing
for linearity. For more categories, it may be reasonable to stratify the data
by levels of the variable and to compute summary statistics (e.g., logit proportions for a logistic model) or to examine regression coefficients associated
with indicator variables over categories. Then one can attempt to summarize
the pattern with a linear or some other simple trend. Later hypothesis tests
must take into account this data-driven scoring (by using > 1 d.f., for example), but the scoring can save degrees of freedom when testing for interaction
with other factors. In one dataset, the number of comorbid diseases was used
to summarize the risk of a set of diseases that was too large to model. By
plotting the logit of the proportion of deaths versus the number of diseases,
it was clear that the square of the number of diseases would properly score
the variables.
Sometimes it is useful to code an ordinal predictor with k 1 indicator
variables of the form [X
vj ], where j = 2, . . . , k and [h] is 1 if h is true,
0 otherwise.640 Although a test of linearity does not arise immediately from
this coding, the regression coefficients are interpreted as amounts of change
from the previous category. A test of whether the last m categories can be
combined with the category k m does follow easily from this coding.

2.7.4 Distributional Assumptions
The general linear regression model is stated as C(Y |X) = X to highlight its
regression assumptions. For logistic regression models for binary or nominal
responses, there is no distributional assumption if simple random sampling
is used and subjects’ responses are independent. That is, the binary logistic
model and all of its assumptions are contained in the expression logit{Y =
1|X} = X . For ordinary multiple regression with constant variance 2 , we
usually assume that Y X is normally distributed with mean 0 and variance
2
. This assumption can be checked by estimating with ˆ and plotting the
overall distribution of the residuals Y X ˆ, the residuals against Ŷ , and the
residuals against each X. For the latter two, the residuals should be normally
distributed within each neighborhood of Ŷ or X. A weaker requirement is that
the overall distribution of residuals is normal; this will be satisfied if all of the
stratified residual distributions are normal. Note a hidden assumption in both
models, namely, that there are no omitted predictors. Other models, such as
the Weibull survival model or the Cox129 proportional hazards model, also

40

2 General Aspects of Fitting Regression Models

have distributional assumptions that are not fully specified by C(Y |X) = X .
However, regression and distributional assumptions of some of these models
are encapsulated by
C(Y |X) = C(Y = y|X) = d(y) + X

(2.39)

for some choice of C. Here C(Y = y|X) is a property of the response Y
evaluated at Y = y, given the predictor values X, and d(y) is a component of
the distribution of Y . For the Cox proportional hazards model, C(Y = y|X)
can be written as the log of the hazard of the event at time y, or equivalently
as the log of the log of the survival probability at time y, and d(y) can be
thought of as a log hazard function for a “standard” subject.
If we evaluated the property C(Y = y|X) at predictor values X 1 and X 2 ,
the di↵erence in properties is
C(Y = y|X 1 )

C(Y = y|X 2 ) = d(y) + X 1
[d(y) + X
= (X

1

2

(2.40)
]

2

X ) ,

which is independent of y. One way to verify part of the distributional assumption is to estimate C(Y = y|X 1 ) and C(Y = y|X 2 ) for set values of
X 1 and X 2 using a method that does not make the assumption, and to plot
C(Y = y|X 1 ) C(Y = y|X 2 ) versus y. This function should be flat if the
distributional assumption holds. The assumption can be tested formally if
d(y) can be generalized to be a function of X as well as y. A test of whether
d(y|X) depends on X is a test of one part of the distributional assumption.
For example, writing d(y|X) = d(y) + X log(y) where
X

=

1 X1

+

2 X2

+ ... +

k Xk

(2.41)

and testing H0 : 1 = . . . = k = 0 is one way to test whether d(y|X)
depends on X. For semiparametric models such as the Cox proportional
hazards model, the only distributional assumption is the one stated above,
namely, that the di↵erence in properties between two subjects depends only
on the di↵erence in the predictors between the two subjects. Other, parametric, models assume in addition that the property C(Y = y|X) has a specific
shape as a function of y, that is that d(y) has a specific functional form. For
example, the Weibull survival model has a specific assumption regarding the
shape of the hazard or survival distribution as a function of y.
Assessments of distributional assumptions are best understood by applying
these methods to individual models as is demonstrated in later chapters.

2.8 Further Reading

41

2.8 Further Reading
1
2
3

4
5

6

7

8

9
10

11

References [149, 571, 574] have more information about cubic splines.
See Smith574 for a good overview of spline functions.
More material about natural splines may be found in de Boor149 . McNeil et al.444
discuss the overall smoothness of natural splines in terms of the integral of the
square of the second derivative of the regression function, over the range of
the data. Govindarajulu et al.225 compared restricted cubic splines, penalized
splines, and fractional polynomial527 fits and found that the first two methods
agreed with each other more than with estimated fractional polynomials.
A tutorial on restricted cubic splines is in [266].
Durrleman and Simon165 provide examples in which knots are allowed to be
estimated as free parameters, jointly with the regression coefficients. They found
that even though the “optimal” knots were often far from a priori knot locations,
the model fits were virtually identical.
Contrast Hastie and Tibshirani’s generalized nonparametric additive models270
with Stone and Koo’s589 additive model in which each continuous predictor is
represented with a restricted cubic spline function.
Gray232, 233 provided some comparisons with ordinary regression splines, but he
compared penalized regression splines with non-restricted splines with only two
knots. Two knots were chosen so as to limit the degrees of freedom needed by the
regression spline method to a reasonable number. Gray argued that regression
splines are sensitive to knot locations, and he is correct when only two knots
are allowed and no linear tail restrictions are imposed. Two knots also prevent
the (ordinary maximum likelihood) fit from utilizing some local behavior of
the regression relationship. For penalized likelihood estimation using B-splines,
Gray233 provided extensive simulation studies of type I and II error for testing
association in which the true regression function, number of knots, and amount
of likelihood penalization were varied. He studied both normal regression and
Cox regression.
Breiman et al.’s original CART method68 used the Gini criterion for splitting.
Later work has used log-likelihoods.106 Segal,557 LeBlanc and Crowley,381 and
Ciampi et al.104, 105 and and Keleş and Segal336 have extended recursive partitioning to censored survival data using the log-rank statistic as the criterion. Zhang677 extended tree models to handle multivariate binary responses.
Schmoor et al.551 used a more general splitting criterion that is useful in therapeutic trials, namely, a Cox test for main and interacting e↵ects. Davis and
Anderson146 used an exponential survival model as the basis for tree construction. Ahn and Loh7 developed a Cox proportional hazards model adaptation
of recursive partitioning along with bootstrap and cross-validation-based methods to protect against “over-splitting.” The Cox-based regression tree methods of Ciampi et al.104 have a unique feature that allows for construction of
“treatment interaction trees” with hierarchical adjustment for baseline variables. Zhang et al.678 provided a new method for handling missing predictor
values that is simpler than using surrogate splits. See [34, 137, 265] for examples
using recursive partitioning for binary responses in which the prediction trees
did not validate well.
436
has a discussion of other problems with tree models.
For ordinary linear models, the regression estimates are the same as obtained
with separate fits, but standard errors are di↵erent (since a pooled standard
error is used for the combined fit). For Cox129 regression, separate fits can be
slightly di↵erent since each subset would use a separate ranking of Y .
Gray’s penalized fixed-knot regression splines can be useful for estimating joint
e↵ects of two continuous variables while allowing the analyst to control the

42

12

2 General Aspects of Fitting Regression Models
e↵ective number of degrees of freedom in the fit [232, 233, Section 3.2]. When
Y is a non-censored variable, the local regression model of Cleveland et al.,93
a multidimensional scatterplot smoother mentioned in Section 2.4.7, provides a
good graphical assessment of the joint e↵ects of several predictors so that the
forms of interactions can be chosen. See Wang et al.647 and Gustafson243 for
several other flexible approaches to analyzing interactions among continuous
variables.
Study site by treatment interaction is often the interaction that is worried
about the most in multi-center randomized clinical trials, because regulatory
agencies are concerned with consistency of treatment e↵ects over study centers.
However, this type of interaction is usually the weakest and is difficult to assess
when there are many centers due to the number of interaction parameters to
estimate. Schemper540 discusses various types of interactions and a general
nonparametric test for interaction.

2.9 Problems
For problems 1 to 3, state each model statistically, identifying each predictor
with one or more component variables. Identify and interpret each regression
parameter except for coefficients of nonlinear terms in spline functions. State
each hypothesis below as a formal statistical hypothesis involving the proper
parameters, and give the (numerator) degrees of freedom of the test. State
alternative hypotheses carefully with respect to unions or intersections of
conditions and list the type of alternatives to the null hypothesis that the
test is designed to detect.c
1. A property of Y such as the mean is linear in age and blood pressure
and there may be an interaction between the two predictors. Test H0 :
there is no interaction between age and blood pressure. Also test H0 :
blood pressure is not associated with Y (in any fashion). State the e↵ect
of blood pressure as a function of age, and the e↵ect of age as a function
of blood pressure.
2. Consider a linear additive model involving three treatments (control, drug
Z, and drug Q) and one continuous adjustment variable, age. Test H0 :
treatment group is not associated with response, adjusted for age. Also
test H0 : response for drug Z has the same property as the response for
drug Q, adjusted for age.
3. Consider models each with two predictors, temperature and white blood
count (WBC), for which temperature is always assumed to be linearly
related to the appropriate property of the response, and WBC may or
may not be linear (depending on the particular model you formulate for
each question). Test:
a. H0 : WBC is not associated with the response versus Ha : WBC is
linearly associated with the property of the response.
c

In other words, under what assumptions does the test have maximum power?

2.9 Problems

43

b. H0 : WBC is not associated with Y versus Ha : WBC is quadratically
associated with Y . Also write down the formal test of linearity against
this quadratic alternative.
c. H0 : WBC is not associated with Y versus Ha : WBC related to the
property of the response through a smooth spline function; for example,
for WBC the model requires the variables WBC, WBC0 , and WBC00
where WBC0 and WBC00 represent nonlinear components (if there are
four knots in a restricted cubic spline function). Also write down the
formal test of linearity against this spline function alternative.
d. Test for a lack of fit (combined nonlinearity or non-additivity) in an
overall model that takes the form of an interaction between temperature
and WBC, allowing WBC to be modeled with a smooth spline function.
4. For a fitted model Y = a + bX + cX 2 derive the estimate of the e↵ect on
Y of changing X from x1 to x2 .
5. In “The Class of 1988: A Statistical Portrait,” the College Board reported
mean SAT scores for each state. Use an ordinary least squares multiple
regression model to study the mean verbal SAT score as a function of the
percentage of students taking the test in each state. Provide plots of fitted
functions and defend your choice of the “best” fit. Make sure the shape
of the chosen fit agrees with what you know about the variables. Add the
raw data points to plots.
a. Fit a linear spline function with a knot at X = 50%. Plot the data
and the fitted function and do a formal test for linearity and a test
for association between X and Y . Give a detailed interpretation of the
estimated coefficients in the linear spline model, and use the partial
t-test to test linearity in this model.
b. Fit a restricted cubic spline function with knots at X = 6, 12, 58, and
68% (not percentile).d Plot the fitted function and do a formal test of
association between X and Y . Do two tests of linearity that test the
same hypothesis:
i. by using a contrast to simultaneously test the correct set of coefficients against zero (done by the anova function in rms);e
ii. by comparing the R2 from the complex model with that from a simple
linear model using a partial F -test.
Explain why the tests of linearity have the d.f. they have.
c. Using subject matter knowledge, pick a final model (from among the
previous models or using another one) that makes sense.
d

Note: To pre-specify knots for restricted cubic spline functions, use something like
rcs(predictor, c(t1,t2,t3,t4)), where the knot locations are t1, t2, t3,
t4.
e
Note that anova in rms computes all needed test statistics from a single model fit
object.

44

2 General Aspects of Fitting Regression Models

Table 2.4

% Taking SAT Mean Verbal % Taking SAT Mean Verbal
(X)
Score (Y )
(X)
Score (Y )
4
482
24
440
5
498
29
460
5
513
37
448
6
498
43
441
6
511
44
424
7
479
45
417
9
480
49
422
9
483
50
441
10
475
52
408
10
476
55
412
10
487
57
400
10
494
58
401
12
474
59
430
12
478
60
433
13
457
62
433
13
485
63
404
14
451
63
424
14
471
63
430
14
473
64
431
16
467
64
437
17
470
68
446
18
464
69
424
20
471
72
420
22
455
73
432
23
452
81
436

The data are found in Table 2.4 and may be created in R using the sat.r
code on the RMS course web site.
6. Derive the formulas for the restricted cubic spline component variables
without cubing or squaring any terms.
7. Prove that each component variable is linear in X when X
tk , the
last knot, using general principles and not algebra or calculus. Derive an
expression for the restricted spline regression function when X tk .
8. Consider a two–stage procedure in which one tests for linearity of the e↵ect
of a predictor X on a property of the response C(Y |X) against a quadratic
alternative. If the two–tailed test of linearity is significant at the ↵ level,
a two d.f. test of association between X and Y is done. If the test for
linearity is not significant, the square term is dropped and a linear model
is fitted. The test of association between X and Y is then (apparently) a
one d.f. test.

2.9 Problems

45

a. Write a formal expression for the test statistic for association.
b. Write an expression for the nominal P –value for testing association
using this strategy.
c. Write an expression for the actual P –value or alternatively for the type–
I error if using a fixed critical value for the test of association.
d. For the same two–stage strategy consider an estimate of the e↵ect on
C(Y |X) of increasing X from a to b. Write a brief symbolic algorithm
for deriving a true two–sided 1 ↵ confidence interval for the b : a e↵ect
(di↵erence in C(Y )) using the bootstrap.

Chapter 3

Missing Data

3.1 Types of Missing Data
There are missing data in the majority of datasets one is likely to encounter.
Before discussing some of the problems of analyzing data in which some
variables are missing for some subjects, we define some nomenclature.

Missing completely at random (MCAR)
Data are missing for reasons that are unrelated to any characteristics or responses for the subject, including the value of the missing value, were it to
be known. Examples include missing laboratory measurements because of a
dropped test tube (if it was not dropped because of knowledge of any measurements), a study that ran out of funds before some subjects could return
for follow-up visits, and a survey in which a subject omitted her response to
a question for reasons unrelated to the response she would have made or to
any other of her characteristics.

Missing at random (MAR)
Data are not missing at random, but the probability that a value is missing
depends on values of variables that were actually measured. As an example,
consider a survey in which females are less likely to provide their personal
income in general (but the likelihood of responding is independent of her
actual income). If we know the sex of every subject and have income levels
for some of the females, unbiased sex-specific income estimates can be made.
That is because the incomes we do have for some of the females are a random
sample of all females’ incomes. Another way of saying that a variable is MAR
is that given the values of other available variables, subjects having missing

47

1

48

3 Missing Data

values are only randomly di↵erent from other subjects.530 Or to paraphrase
Greenland and Finkle,237 for MAR the missingness of a covariable cannot
depend on unobserved covariable values; for example whether a predictor is
observed cannot depend on another predictor when the latter is missing but
it can depend on the latter when it is observed. MAR and MCAR data are
also called ignorable non-responses.

Informative missing (IM)
The tendency for a variable to be missing is a function of data that are not
available, including the case when data tend to be missing if their true values
are systematically higher or lower. An example is when subjects with lower
income levels or very high incomes are less likely to provide their personal income in an interview. IM is also called nonignorable non-response and missing
not at random (MNAR).
IM is the most difficult type of missing data to handle. In many cases, there
is no fix for IM nor is there a way to use the data to test for the existence of
IM. External considerations must dictate the choice of missing data models,
and there are few clues for specifying a model under IM. MCAR is the easiest
case to handle. Our ability to correctly analyze MAR data depends on the
availability of other variables (the sex of the subject in the example above).
Most of the methods available for dealing with missing data assume the data
are MAR. Fortunately, even though the MAR assumption is not testable, it
may hold approximately if enough variables are included in the imputation
models251 .

3.2 Prelude to Modeling
No matter whether one deletes incomplete cases, carefully imputes (estimates) missing data, or uses a full maximum likelihood or Bayesian techniques to incorporate partial data, it is beneficial to characterize patterns
of missingness using exploratory data analysis techniques. These techniques
include binary logistic models and recursive partitioning for predicting the
probability that a given variable is missing. Patterns of missingness should
be reported to help readers understand the limitations of incomplete data. If
you do decide to use imputation, it is also important to describe how variables are simultaneously missing. A cluster analysis of missing value status
of all the variables is useful here. This can uncover cases where imputation
is not as e↵ective. For example, if the only variable moderately related to
diastolic blood pressure is systolic pressure, but both pressures are missing
on the same subjects, systolic pressure cannot be used to estimate diastolic
blood pressure. R functions naclus and naplot in the Hmisc package (see

3.4 Problems with Simple Alternatives to Imputation

49

p. 144) can help detect how variables are simultaneously missing. Recursive
partitioning (regression tree) algorithms (see Section 2.5) are invaluable for
describing which kinds of subjects are missing on a variable. Logistic regression is also an excellent tool for this purpose. A later example (p. 306)
demonstrates these procedures.
It can also be helpful to explore the distribution of non-missing Y by the
number of missing variables in X (including zero, i.e., complete cases on X).

3.3 Missing Values for Di↵erent Types of Response
Variables
When the response variable Y is collected serially but some subjects drop
out of the study before completion, there are many ways of dealing with
partial information41, 405, 473 including multiple imputation in phases,373 or
efficiently analyzing all available serial data using a full likelihood model.
When Y is the time until an event, there are actually no missing values of Y
but follow-up will be curtailed for some subjects. That leaves the case where
the response is completely measured once.
It is common practice to discard subjects having missing Y . Before doing
so, at minimum an analysis should be done to characterize the tendency
for Y to be missing, as just described. For example, logistic regression or
recursive partitioning can be used to predict whether Y is missing and to
test for systematic tendencies as opposed to Y being missing completely at
random. In many models, though, more efficient and less biased estimates of
regression coefficients can be made by also utilizing observations missing on
Y that are non-missing on X. Hence there is a definite place for imputation
of Y . von Hippel637 found advantages of using all variables to impute all
others, and once imputation is finished, discarding those observations having
missing Y . However if missing Y values are MCAR, up-front deletion of cases
having missing Y may sometimes be preferred, as imputation requires correct
specification of the imputation model.

3.4 Problems with Simple Alternatives to Imputation
Incomplete predictor information is a very common missing data problem.
Statistical software packages use casewise deletion in handling missing predictors; that is, any subject having any predictor or Y missing will be excluded
from a regression analysis. Casewise deletion results in regression coefficient
estimates that can be terribly biased, imprecise, or both346 . First consider an
example where bias is the problem. Suppose that the response is death and
the predictors are age, sex, and blood pressure, and that age and sex were

2

50

3

4

3 Missing Data

recorded for every subject. Suppose that blood pressure was not measured
for a fraction of 0.10 of the subjects, and the most common reason for not
obtaining a blood pressure was that the subject was about to die. Deletion
of these very sick patients will cause a major bias (downward) in the model’s
intercept parameter. In general, casewise deletion will bias the estimate of
the model’s intercept parameter (as well as others) when the probability of
a case being incomplete is related to Y and not just to X [415, Example
3.3]. van der Heijden et al.621 discuss how complete case analysis (casewise
deletion) usually assumes MCAR.
Now consider an example in which casewise deletion of incomplete records
is inefficient. The inefficiency comes from the reduction of sample size, which
causes standard errors to increase,159 confidence intervals to widen, and power
of tests of association and tests of lack of fit to decrease. Suppose that the
response is the presence of coronary artery disease and the predictors are
age, sex, LDL cholesterol, HDL cholesterol, blood pressure, triglyceride, and
smoking status. Suppose that age, sex, and smoking are recorded for all
subjects, but that LDL is missing in 0.18 of the subjects, HDL is missing
in 0.20, and triglyceride is missing in 0.21. Assume that all missing data are
MCAR and that all of the subjects missing LDL are also missing HDL and
that overall 0.28 of the subjects have one or more predictors missing and
hence would be excluded from the analysis. If total cholesterol were known
on every subject, even though it does not appear in the model, it (along
perhaps with age and sex) can be used to estimate (impute) LDL and HDL
cholesterol and triglyceride, perhaps using regression equations from other
studies. Doing the analysis on a “filled in” dataset will result in more precise
estimates because the sample size would then include the other 0.28 of the
subjects.
In general, observations should only be discarded if the MCAR assumption is justified, there is a rarely missing predictor of overriding importance
that cannot be reliably imputed from other information, or if the fraction of
observations excluded is very small and the original sample size is large. Even
then, there is no advantage of such deletion other than saving analyst time.
If a predictor is MAR but its missingness depends on Y , casewise deletion is
biased.
The first blood pressure example points out why it can be dangerous to
handle missing values by adding a dummy variable to the model. Many analysts would set missing blood pressures to a constant (it doesn’t matter which
constant) and add a variable to the model such as is.na(blood.pressure)
in R notation. The coefficient for the latter dummy variable will be quite
large in the earlier example, and the model will appear to have great ability
to predict death. This is because some of the left-hand side of the model
contaminates the right-hand side; that is, is.na(blood.pressure) is correlated with death. For categorical variables, another common practice is to
add a new category to denote missing, adding one more degree of freedom

3.5 Strategies for Developing an Imputation Model

51

to the predictor and changing its meaning.a Jones319 , Allison [12, pp. 9-11],
Donders et al.158 , Knol et al.346 and van der Heijden et al.621 describe why
both of these missing-indicator methods are invalid even when MCAR holds.

3.5 Strategies for Developing an Imputation Model
Except in special circumstances that usually involve only very simple models,
the primary alternative to deleting incomplete observations is imputation of
the missing values. Many non-statisticians find the notion of estimating data
distasteful, but the way to think about imputation of missing values is that
“making up” data is better than discarding valuable data. It is especially distressing to have to delete subjects who are missing on an adjustment variable
when a major variable of interest is not missing. So one goal of imputation
is to use as much information as possible for examining any one predictor’s
adjusted association with Y . The overall goal of imputation is to preserve the
information and meaning of the non-missing data.
At this point the analyst must make some decisions about the information
to use in computing predicted values for missing values.
1. Imputation of missing values for one of the variables can ignore all other
information. Missing values can be filled in by sampling non-missing values
of the variable, or by using a constant such as the median or mean nonmissing value.
2. Imputation algorithms can be based only on external information not otherwise used in the model for Y in addition to variables included in later
modeling. For example, family income can be imputed on the basis of location of residence when such information is to remain confidential for other
aspects of the analysis or when such information would require too many
degrees of freedom to be spent in the ultimate response model.
3. Imputations can be derived by only analyzing interrelationships among
the Xs.
4. Imputations can use relationships among the Xs and between X and Y .
5. Imputations can use X, Y , and auxiliary variables not in the model predicting Y .
6. Imputations can take into account the reason for non-response if known.
The model to estimate the missing values in a sometimes-missing (target)
variable should include all variables that are either
1. related to the missing data mechanism;
a

This may work if values are “missing” because of “not applicable”, e.g. one has a
measure of marital happiness, dichotomized as high or low, but the sample contains
some unmarried people. One could have a 3-category variable with values high, low,
and unmarried (Paul Allison, IMPUTE e-mail list, 4Jul09).

5

52

3 Missing Data

2. have distributions that di↵er between subjects that have the target variable
missing and those that have it measured;
3. are associated with the target variable when it is not missing; or
4. are included in the final response model42 .
The imputation and analysis (response) models should be “congenial” or the
imputation model should be more general than the response model or make
well-founded assumptions251 .
When a variable, say Xj , is to be included as a predictor of Y , and Xj
is sometimes missing, ignoring the relationship between Xj and Y for those
observations for which both are known will bias regression coefficients for
Xj toward zero in the outcome model.414 On the other hand, using Y to
singly impute Xj using a conditional mean will cause a large inflation in
the apparent importance of Xj in the final model. In other words, when the
missing Xj are replaced with a mean that is conditional on Y without a
random component, this will result in a falsely strong relationship between
the imputed Xj values and Y .
At first glance it might seem that using Y to impute one or more of the Xs,
even with allowance for the correct amount of random variation, would result
in a circular analysis in which the importance of the Xs will be exaggerated.
But the relationship between X and Y in the subset of imputed observations
will only be as strong as the associations between X and Y that are evidenced
by the non-missing data. In other words, regression coefficients estimated
from a dataset that is completed by imputation will not in general be biased
high as long as the imputed values have similar variation as non-missing data
values.
The next important decision about developing imputation algorithms is
the choice of how missing values are estimated.
1. Missings can be estimated using single “best guesses” (e.g., predicted conditional expected values or means) based on relationships between nonmissing values. This is called single imputation of conditional means.
2. Missing Xj (or Y ) can be estimated using single individual predicted values, where by predicted value we mean a random variable value from the
whole conditional distribution of Xj . If one uses ordinary multiple regression to estimate Xj from Y and the other Xs, a random residual would
be added to the predicted mean value. If assuming a normal distribution
for Xj conditional on the other data, such a residual could be computed
by a Gaussian random number generator given an estimate of the residual
standard deviation. If normality is not assumed, the residual could be a
randomly chosen residual from the actual computed residuals. When m
missing values need imputation for Xj , the residuals could be sampled
with replacement from the entire vector of residuals as in the bootstrap.
Better still according to Rubin and Schenker530 would be to use the “approximate Bayesian bootstrap” which involves sampling n residuals with
replacement from the original n estimated residuals (from observations not

3.5 Strategies for Developing an Imputation Model

53

missing on Xj ), then sampling m residuals with replacement from the first
sampled set.
3. More than one random predicted value (as just defined) can be generated
for each missing value. This process is called multiple imputation and it
has many advantages over the other methods in general. This is discussed
in Section 3.8.
4. Matching methods can be used to obtain random draws of other subject’s
values to replace missing values. Nearest neighbor matching can be used
to select a subject that is “close” to the subject in need of imputation,
on the basis of a series of variables. This method requires the analyst to
make decisions about what constitutes “closeness.” To simplify the matching process into a single dimension, Little413 proposed the predictive mean
matching method where matching is done on the basis of predicted values
from a regression model for predicting the sometimes-missing variable (section 3.7). According to Little, in large samples predictive mean matching
may be more robust to model misspecification than the method of adding
a random residual to the subject’s predicted value, but because of difficulties in finding matches the random residual method may be better in
smaller samples. The random residual method may be easier to use when
multiple imputations are needed, but care must be taken to create the
correct degree of uncertainty in residuals.
What if Xj needs to be imputed for some subjects based on other variables
that themselves may be missing on the same subjects missing on Xj ? This is
a place where recursive partitioning with “surrogate splits” in case of missing
predictors may be a good method for developing imputations (see Section 2.5
and p. 144). If using regression to estimate missing values, an algorithm
to cycle through all sometimes-missing variables for multiple iterations may
perform well. This algorithm is used by the R transcan function described
in Section 4.7.4 as well as the to–be–described aregImpute function. First,
all missing values are initialized to medians (modes for categorical variables).
Then every time missing values are estimated for a certain variable, those
estimates are inserted the next time the variable is used to predict other
sometimes-missing variables.
If you want to assess the importance of a specific predictor that is frequently missing, it is a good idea to perform a sensitivity analysis in which
all observations containing imputed values for that predictor are temporarily
deleted. The test based on a model that included the imputed values may be
diluted by the imputation or it may test the wrong hypothesis, especially if
Y is not used in imputing X.
Little argues for down-weighting observations containing imputations, to
obtain a more accurate variance–covariance matrix. For the ordinary linear
model, the weights have been worked out for some cases [414, p. 1231].

6

7

54

3 Missing Data

3.6 Single Conditional Mean Imputation

8

For a continuous or binary X that is unrelated to all other predictor variables, the mean or median may be substituted for missing values without
much loss of efficiency,159 although regression coefficients will be biased low
since Y was not utilized in the imputation. When the variable of interest
is related to the other Xs, it is far more efficient to use an individual predictive model for each X based on the other variables.78, 520, 605 The “best
guess” imputation method fills in missings with predicted expected values
using the multivariable imputation model based on non-missing datab . It is
true that conditional means are the best estimates of unknown values, but
except perhaps for binary logistic regression614, 616 their use will result in biased estimates and very biased (low) variance estimates. The latter problem
arises from the reduced variability of imputed values [171, p. 464].
Tree-based models (Section 2.5) may be very useful for imputation since
they do not require linearity or additivity assumptions, although such models
often have poor discrimination when they don’t overfit. When a continuous
X being imputed needs to be non-monotonically transformed to best relate
it to the other Xs (e.g., blood pressure vs. heart rate), trees and ordinary
regression are inadequate. Here a general transformation modeling procedure
(Section 4.7) may be needed.
Schemper et al.546, 548 proposed imputing missing binary covariables by
predicted probabilities. For categorical sometimes-missing variables, imputation models can be derived using polytomous logistic regression or a classification tree method. For missing values, the most likely value for each subject
(from the series of predicted probabilities from the logistic or recursive partitioning model) can be substituted to avoid creating a new category that is
falsely highly correlated with Y . For an ordinal X, the predicted mean value
(possibly rounded to the nearest actual data value) or median value from an
ordinal logistic model is sometimes useful.

3.7 Predictive Mean Matching
In predictive mean matching 415 (PMM), one replaces a missing (NA) value
for the target variable being imputed with the actual value from a donor
observation. Donors are identified by matching in only one dimension, namely
the predicted value (e.g., predicted mean) of the target. Key considerations
are how to
1. model the target when it is not NA
b

Predictors of the target variable include all the other Xs along with auxiliary
variables that are not included in the final outcome model, as long as they precede
the variable being imputed in the causal chain.

3.8 Multiple Imputation

55

2. match donors on predicted values
3. avoid overuse of “good” donors to disallow excessive ties in imputed data
4. account for all uncertainties (section 3.8).
The predictive model for each target variable uses any outcome variables, all
predictors in the final outcome model, plus any needed auxiliary variables.
The modeling method should be flexible, not assuming linearity. Many methods will suffice; parametric additive models are often good choices. Beauties
of PMM include the lack of need for distributional assumptions (as no residuals are calculated), and predicted values need only be monotonically related
to real predicted valuesc
In the original PMM method the donor for an NA was the complete observation whose predicted target was closest to the predicted value of the target
from all complete observationsd . This approach can result in some donors
being used repeatedly. This can be addressed by sampling from a multinomial distribution, where the probabilities are scaled distances of all potential
donors’ predictions to the predicted value y ⇤ of the missing target. Tukey’s
tricube function (used in loess) is a good weighting function, implemented in
the Hmisc aregImpute function:
wi = (1
di

min(di /s, 1)3 )3 ,
= |yˆi

y⇤ |

s = 0.2 ⇥ mean|yˆi

(3.1)
⇤

y |.

s above is a good default scale factor, and the wi are scaled so that

P

wi = 1.

3.8 Multiple Imputation
Imputing missing values and then doing an ordinary analysis as if the imputed
values were real measurements is usually better than excluding subjects with
incomplete data. However, ordinary formulas for standard errors and other
statistics are invalid unless imputation is taken into account.644 Methods for
properly accounting for having incomplete data can be complex. The bootstrap (described later) is an easy method to implement, but the computations
can be slowe .
c

Thus when modeling binary or categorical targets one can frequently take least
squares shortcuts in place of maximum likelihood for binary, ordinal, or multinomial
logistic models.
d 656
discusses an alternative method based on choosing a donor observation at
random from the q closest matches (q = 3, for example).
e
To use the bootstrap to correctly estimate variances of regression coefficients, one
must repeat the imputation process and the model fitting perhaps 100 times using a
resampling procedure171, 561 (see Section 5.2). Still, the bootstrap can estimate the

56

3 Missing Data

Multiple imputation uses random draws from the conditional distribution
of the target variable given the other variables (and any additional information that is relevant)410, 414, 531 (but see [448]). The additional information
used to predict the missing values can contain any variables that are potentially predictive, including variables measured in the future; the causal chain
is not relevant.414, 456 When a regression model is used for imputation, the
process involves adding a random residual to the “best guess” for missing
values, to yield the same conditional variance as the original variable. Methods for estimating residuals were listed in Section 3.5. To properly account
for variability due to unknown values, the imputation is repeated M times,
where M
3. Each repetition results in a “completed” dataset that is analyzed using the standard method. Parameter estimates are averaged over
these multiple imputations to obtain better estimates than those from single imputation. The variance–covariance matrix of the averaged parameter
estimates, adjusted for variability due to imputation, is estimated using415
V =M

1

M
X
i

9

Vi +

M +1
B,
M

(3.2)

where Vi is the ordinary complete data estimate of the variance–covariance
matrix for the model parameters from the ith imputation, and B is the
between-imputation sample variance–covariance matrix, the diagonal entries
of which are the ordinary sample variances of the M parameter estimates.
After running aregImpute (or MICE) you can run the Hmisc packages’s
fit.mult.impute function to fit the chosen model separately for each artificially completed dataset corresponding to each imputation. After fit.mult.impute
fits all of the models, it averages the sets of regression coefficients and computes variance and covariance estimates that are adjusted for imputation
(using Eq. 3.2).
White and Royston655 provide a method for multiply imputing missing
covariate values using censored survival time data in the context of the Cox
proportional hazards model.
White et al.656 recommend choosing the number of imputations M so
that the key inferential statistics are very reproducible should the imputation
analysis be repeated. They suggest the use of 100f imputations when f is
the fraction of cases that are incomplete. See also [83, Section 2.7] and227 .
Extreme amount of missing data does not prevent one from using multiple
imputation, because alternatives are worse314 . Horton and Lipsitz295 also
have a good overview of multiple imputation and a review of several software
packages that implement PMM.
Caution: Multiple imputation methods can generate imputations having very reasonable distributions but still not having the property that final
right variance for the wrong parameter estimates if the imputations are not done
correctly.

3.8 Multiple Imputation

57

response model regression coefficients have nominal confidence interval coverage. Among other things, it is worth checking that imputations generate
the correct collinearities among covariates.

3.8.1 The aregImpute and Other Chained Equations
Approaches
A flexible approach to multiple imputation that handles a wide variety of
target variables to be imputed and allows for multiple variables to be missing on the same subject is the chained equation method. With a chained
equations approach, each target variable is predicted by a regression model
conditional on all other variables in the model, plus other variables. An iterative process cycles through all target variables to impute all missing values620 . This approach is used in the MICE algorithm (multiple imputation
using chained equations) implemented in R and other systems. The chained
equation method does not attempt to use the full Bayesian multivariate model
for all target variables, which makes it more flexible and easy to use but leaves
it open to creating improper imputations, e.g., imputing conflicting values for
di↵erent target variables. However, simulation studies620 so far have demonstrated very good performance of imputation based on chained equations in
non-complex situations.
The aregImpute algorithm456 takes all aspects of uncertainty into account
using the bootstrap while using the same estimation procedures as transcan
(section 4.7). Di↵erent bootstrap resamples used for each imputation by fitting a flexible additive model on a sample with replacement from the original data. This model is used to predict all of the original missing and nonmissing values for the target variable for the current imputation. aregImpute
uses flexible parametric additive regression spline models to predict target
variables. There is an option to allow target variables to be optimally transformed, even non-monotonically (but this can overfit). The function implements regression imputation based on adding random residuals to predicted
means, but its real value lies in implementing a wide variety of PMM algorithms.
The default method used by aregImpute is (weighted) PMM so that
no residuals or distributional assumptions are required. The default PMM
matching used is van Buuren’s “Type 1” matching [83, Section 3.4.2] to capture the right amount of uncertainty. Here one computes predicted values
for missing values using a regression fit on the bootstrap sample, and finds
donor observations by matching those predictions to predictions from potential donors using the regression fit from the original sample of complete
observations. When a predictor of the target variable is missing, it is first
imputed from its last imputation when it was a target variable. The first 3
iterations of the process are ignored (“burn-in”). aregImpute seems to per-

58

3 Missing Data

Table 3.1 Summary of Methods for Dealing with Missing Values

Method
Deletion Single Multiple
Allows nonrandom missing
–
x
x
Reduces sample size
x
–
–
ˆ
Apparent S.E. of too low
–
x
–
Increases real S.E. of ˆ
x
–
–
ˆ biased
if not MCAR x
–

10

form as well as MICE but runs significantly faster and allows for nonlinear
relationships.
Here is an example using the R Hmisc and rms packages.
a

f

are gIm put e (⇠ age + s e x + bp + death +
heart.attack.before.death ,
data=mydata , n . i m p u t e =5)
f i t . m u l t . i m p u t e ( death ⇠ r c s ( age , 3 ) + s e x +
r c s ( bp , 5 ) , lrm , a , data=mydata )

3.9 Diagnostics

11

One diagnostic that can be helpful in assessing the MCAR assumption is to
compare the distribution of non-missing Y for those subjects having complete X with those having incomplete X. On the other hand, Yucel and Zaslavsky676 developed a diagnostic that is useful for checking the imputations
themselves. In solving a problem related to imputing binary variables using
continuous data models, they proposed a simple approach. Suppose we were
interested in the reasonableness of imputed values for a sometimes-missing
predictor Xj . Duplicate the entire dataset, but in the duplicated observations
set all values of Xj to missing. Develop imputed values for the missing values
of Xj , and in the observations of the duplicated portion of the dataset corresponding to originally non-missing values of Xj , compare the distribution
of imputed Xj with the original values of Xj .

3.10 Summary and Rough Guidelines
Table 3.1 summarizes the advantages and disadvantages of three methods of
dealing with missing data. Here “Single” refers to single conditional mean imputation (which cannot utilize Y ) and “Multiple” refers to multiple randomdraw imputation (which can incorporate Y ).

3.10 Summary and Rough Guidelines

59

The following contains crude guidelines. Simulation studies are needed to
refine the recommendations. Here f refers to the proportion of observations
having any variables missing.
f < 0.03: It doesn’t matter very much how you impute missings or whether
you adjust variance of regression coefficient estimates for having imputed
data in this case. For continuous variables imputing missings with the
median non-missing value is adequate; for categorical predictors the most
frequent category can be used. Complete case analysis is also an option
here. Multiple imputation may be needed to check that the simple approach “worked.”
f 0.03: Use multiple imputation with number of imputations equal to
max(5, 100f ). Fewer imputations may be possible with very large sample
sizes. Type 1 predictive mean matching is usually preferred, with weighted
selection of donors. Account for imputation in estimating the covariance
matrix for final parameter estimates. Use the t distribution instead of the
Gaussian distribution for tests and confidence intervals, if possible, using
the estimated d.f. for the parameter estimates.
Multiple predictors frequently missing: More imputations may be required.
Perform a “sensitivity to order” analysis by creating multiple imputations
using di↵erent orderings of sometimes missing variables. It may be beneficial to place the variable with the highest number of NAs first so that
initialization of other missing variables to medians will have less impact.
It is important to note that the reasons for missing data are more important
determinants of how missing values should be handled than is the quantity
of missing values.
If the main interest is prediction and not interpretation or inference about
individual e↵ects, it is worth trying a simple imputation (e.g., median or
normal value substitution) to see if the resulting model predicts the response
almost as well as one developed after using customized imputation. But it
is not appropriate to use the dummy variable or extra category method in
then, because these methods steal information from Y and bias all ˆs. Clark
and Altman107 presented a nice example of the use of multiple imputation for
developing a prognostic model. Marshall et al.435 developed a useful method
for obtaining predictions on future observations when some of the needed
predictors are unavailable. Their method uses an approximate re-fit of the
original model for available predictors only, utilizing only the coefficient estimates and covariance matrix from the original fit. Little and An411 also have
an excellent review of imputation methods and developed several approximate formulas for understanding properties of various estimators. They also
developed a method combining imputation of missing values with propensity
score modeling of the probability of missingness.

12

60

3 Missing Data

3.11 Further Reading
1

2

3

4

5

6
7

8

9

These types of missing data are well described in an excellent review article
on missing data by Schafer and Graham537 . A good introductory article on
missing data and imputation is by Donders et al.158 and a good overview of
multiple imputation is by White et al.656 and Harel and Zhou251 . Paul Allison’s
booklet12 and van Buuren’s book83 are also excellent practical treatments.
Crawford et al.135 give an example where responses are not MCAR for which
deleting subjects with missing responses resulted in a biased estimate of the
response distribution. They found that multiple imputation of the response resulted in much improved estimates. Wood et al.667 have a good review of how
missing response data are typically handled in randomized trial reports, with
recommendations for improvements. Barnes et al.41 have a good overview of
imputation methods and a comparison of bias and confidence interval coverage for the methods when applied to longitudinal data with a small number
of subjects. Twist et al.610 found instability in using multiple imputation of
longitudinal data, and advantages of using instead full likelihood models.
See van Buuren et al.619 for an example in which subjects having missing baseline blood pressure had shorter survival time. Joseph et al.320 provide examples demonstrating difficulties with casewise deletion and single imputation,
and comment on the robustness of multiple imputation methods to violations
of assumptions.
Another problem with the missingness indicator approach arises when more
than one predictor is missing and these predictors are missing on almost the
same subjects. The missingness indicator variables will be collinear; that is
impossible to disentangle.319
See [616, pp. 2645–2646] for several problems with the “missing category” approach. A clear example is in158 where covariates X1 , X2 have true 1 = 1, 2 =
0 and X1 is MCAR. Adding a missingness indicator for X1 as a covariate resulted in ˆ1 = 0.55, ˆ2 = 0.51 because in the missing observations the constant
X1 was uncorrelated with X2 . D’Agostino and Rubin143 developed methods for
propensity score modeling that allow for missing data. They mentioned that extra categories may be added to allow for missing data in propensity models and
that adding indicator variables describing patterns of missingness will also allow
the analyst to match on missingness patterns when comparing non-randomly
assigned treatments.
Harel and Zhou251 and Siddique564 discuss the approximate Bayesian bootstrap
further.
Kalton and Kasprzyk325 proposed a hybrid approach to imputation in which
missing values are imputed with the predicted value for the subject plus the
residual from the subject having the closest predicted value to the subject being
imputed.
Miller et al.451 studied the e↵ect of ignoring imputation when conditional mean
fill-in methods are used, and showed how to formalize such methods using linear
models.
van Buuren et al.619 presented an excellent case study in multiple imputation
in the context of survival analysis. Barzi and Woodward42 present a nice review
of multiple imputation with detailed comparison of results (point estimates and
confidence limits for the e↵ect of the sometimes-missing predictor) for various
imputation methods. Barnard and Rubin40 derived an estimate of the d.f. associated with the imputation-adjusted variance matrix for use in a t-distribution
approximation for hypothesis tests about imputation-averaged coefficient estimates. When d.f. is not very large, the t approximation will result in more

3.12 Problems

10

11
12

61

accurate P -values than using a normal approximation that we use with Wald
statistics after inserting Equation 3.2 as the variance matrix.
Little and An411 present imputation methods based on flexible additive regression models using penalized cubic splines. Horton and Kleinman294 compare
several software packages for handling missing data and have comparisons of
results with that of aregImpute. Moons et al.456 compared aregImpute with
MICE.
He and Zaslavsky276 formalized the duplication approach to imputation diagnostics.
A good general reference on missing data is Little and Rubin,415 and Volume 16,
Nos. 1 to 3 of Statistics in Medicine, a large issue devoted to incomplete covariable data. Vach613 is an excellent text describing properties of various methods
of dealing with missing data in binary logistic regression (see also [614, 615, 617]).
These references show how to use maximum likelihood to explicitly model the
missing data process. Little and Rubin show how imputation can be avoided
if the analyst is willing to assume a multivariate distribution for the joint distribution of X and Y . Since X usually contains a strange mixture of binary,
polytomous, and continuous but highly skewed predictors, it is unlikely that this
approach will work optimally in many problems. That’s the reason the imputation approach is emphasized. See Rubin531 for a comprehensive source on multiple imputation. See Little,412 Vach and Blettner,616 Rubin and Schenker,530
Zhou et al.,684 Greenland and Finkle,237 and Hunsberger et al.306 for excellent reviews of missing data problems and approaches to solving them. Reilly
and Pepe have a nice comparison of the “hot-deck” imputation method with
a maximum likelihood-based method.518 White and Carlin654 studied bias of
multiple imputation vs. complete case analysis.

3.12 Problems
The SUPPORT Study (Study to Understand Prognoses Preferences Outcomes and Risks of Treatments) was a five-hospital study of 10,000 critically
ill hospitalized adultsf345 . Patients were followed for in-hospital outcomes and
for long-term survival. We analyze 35 variables and a random sample of 1000
patients from the study.
1. Explore the variables and patterns of missing data in the SUPPORT
dataset.
a. Print univariable summaries of all variables. Make a plot (showing all
variables on one page) that describes especially the continuous variables.
b. Make a plot showing the extent of missing data and tendencies for some
variables to be missing on the same patients. Functions in the Hmisc
package may be useful.
c. Total hospital costs (variable totcst) were estimated from hospitalspecific Medicare cost-to-charge ratios. Characterize what kind of patients have missing totcst. For this characterization use the followf

The dataset is on the book’s dataset wiki and may be automatically fetched over
the internet and loaded using the Hmisc package’s command getHdata(support).

62

3 Missing Data

ing patient descriptors: age, sex, dzgroup, num.co, edu, income,
scoma, meanbp, hrt, resp, temp.
2. Prepare for later development of a model to predict costs by developing
reliable imputations for missing costs. Remove the observation having zero
totcst.g
a. The cost estimates are not available on 105 patients. Total hospital
charges (bills) are available on all but 25 patients. Relate these two
variables to each other with an eye toward using charges to predict
totcst when totcst is missing. Make graphs that will tell whether
linear regression or linear regression after taking logs of both variables
is better.
b. Impute missing total hospital costs in SUPPORT based on a regression
model relating charges to costs, when charges are available. You may
want to use a statement like the following in R:
support

transform ( support ,
totcst = i f e l s e ( is.na ( totcst ) ,
( expression in charges ) , totcst ))

If in the previous problem you felt that the relationship between costs
and charges should be based on taking logs of both variables, the “expression in charges” above may look something like exp(intercept +
slope * log(charges)), where constants are inserted for intercept
and slope.
c. Compute the likely error in approximating total cost using charges by
computing the median absolute di↵erence between predicted and observed total costs in the patients having both variables available. If you
used a log transformation, also compute the median absolute percent
error in imputing total costs by anti-logging the absolute di↵erence in
predicted logs.
3. State briefly why single conditional medianh imputation is OK here.
4. Use transcan to develop single imputations for total cost, commenting on
the strength of the model fitted by transcan as well as how strongly each
variable can be predicted from all the others.
5. Use predictive mean matching to multiply impute cost 10 times per missing
observation. Describe graphically the distributions of imputed values and
briefly compare these to distributions of non-imputed values. State in a
simple way what the sample variance of multiple imputations for a single
observation of a continuous predictor is approximating.
g

You can use the R command subset(support, is.na(totcst) | totcst >
0). The is.na condition tells R that it is permissible to include observations having
missing totcst without setting all columns of such observations to NA.
h
We are anti-logging predicted log costs and we assume log cost has a symmetric
distribution

3.12 Problems

63

6. Using the multiple imputed values, develop an overall least squares model
for total cost (using the log transformation) making optimal use of partial
information, with variances computed so as to take imputation (except for
cost) into account. The model should use the predictors in Problem 1 and
should not assume linearity in any predictor but should assume additivity.
Interpret one of the resulting ratios of imputation-corrected variance to
apparent variance and explain why ratios greater than one do not mean
that imputation is inefficient.

Chapter 4

Multivariable Modeling Strategies

Chapter 2 dealt with aspects of modeling such as transformations of predictors, relaxing linearity assumptions, modeling interactions, and examining
lack of fit. Chapter 3 dealt with missing data, focusing on utilization of incomplete predictor information. All of these areas are important in the overall
scheme of model development, and they cannot be separated from what is to
follow. In this chapter we concern ourselves with issues related to the whole
model, with emphasis on deciding on the amount of complexity to allow in
the model and on dealing with large numbers of predictors. The chapter concludes with three default modeling strategies depending on whether the goal
is prediction, estimation, or hypothesis testing.
There are many choices to be made when deciding upon a global modeling
strategy, including choice between
•
•
•
•

parametric and nonparametric procedures
parsimony and complexity
parsimony and good discrimination ability
interpretable models and black boxes.

This chapter addresses some of these issues. One general theme of what follows is the idea that in statistical inference when a method is capable of
worsening performance of an estimator or inferential quantity (i.e., when the
method is not systematically biased in one’s favor), the analyst is allowed to
benefit from the method. Variable selection is an example where the analysis
is systematically tilted in one’s favor by directly selecting variables on the
basis of P -values of interest, and all elements of the final result (including
regression coefficients and P -values) are biased. On the other hand, the next
section is an example of the “capitalize on the benefit when it works, and
the method may hurt” approach because one may reduce the complexity of
an apparently weak predictor by removing its most important component—
nonlinear e↵ects—from how the predictor is expressed in the model. The
method hides tests of nonlinearity that would systematically bias the final
result.

65

1

66

4 Multivariable Modeling Strategies

The book’s web site contains a number of simulation studies and references
to others that support the advocated approaches.

4.1 Prespecification of Predictor Complexity Without
Later Simplification
There are rare occasions in which one actually expects a relationship to be
linear. For example, one might predict mean arterial blood pressure at two
months after beginning drug administration using as baseline variables the
pretreatment mean blood pressure and other variables. In this case one expects the pretreatment blood pressure to linearly relate to follow-up blood
pressure, and modeling is simplea . In the vast majority of studies, however,
there is every reason to suppose that all relationships involving nonbinary
predictors are nonlinear. In these cases, the only reason to represent predictors linearly in the model is that there is insufficient information in the
sample to allow us to reliably fit nonlinear relationships.b
Supposing that nonlinearities are entertained, analysts often use scatter
diagrams or descriptive statistics to decide how to represent variables in a
model. The result will often be an adequately fitting model, but confidence
limits will be too narrow, P -values too small, R2 too large, and calibration
too good to be true. The reason is that the “phantom d.f.” that represented
potential complexities in the model that were dismissed during the subjective
2
assessments are forgotten in computing standard errors, P -values, and Radj
.
The same problem is created when one entertains several transformations
p
(log, , etc.) and uses the data to see which one fits best, or when one tries
to simplify a spline fit to a simple transformation.
An approach that solves this problem is to prespecify the complexity with
which each predictor is represented in the model, without later simplification
of the model. The amount of complexity (e.g., number of knots in spline functions or order of ordinary polynomials) one can a↵ord to fit is roughly related
to the “e↵ective sample size.” It is also very reasonable to allow for greater
complexity for predictors that are thought to be more powerfully related to
Y . For example, errors in estimating the curvature of a regression function are
consequential in predicting Y only when the regression is somewhere steep.
Once the analyst decides to include a predictor in every model, it is fair to
use general measures of association to quantify the predictive potential for
a variable. For example, if a predictor has a low rank correlation with the
response, it will not “pay” to devote many degrees of freedom to that prea

Even then, the two blood pressures may need to be transformed to meet distributional assumptions.
b
Shrinkage (penalized estimation) is a general solution (see Section 4.5). One can
always use complex models that are “penalized towards simplicity,” with the amount
of penalization being greater for smaller sample sizes.

worst error: over-simplification of an important predictor (linear when it isn't, especially if it
is not monotonic)
second worse error: over-simplify a monotonic but not linear predictor
need a measure for how steep the relationship is (leave us completely in the dark about
linearity and monotonicity of the relationship)
1. Just spline everything (saturated additive model) - will be wrong if truly steep and
linear, but that is okay.

4.1 Prespecification of Predictor Complexity

67

dictor in a spline function having many knots. On the other hand, a potent
predictor (with a high rank correlation) not known to act linearly might be
assigned five knots if the sample size allows.
When the e↵ective sample size available is sufficiently large so that a saturated main e↵ects model may be fitted, a good approach to gauging predictive
potential is the following.
• Let all continuous predictors be represented as restricted cubic splines with
k knots, where k is the maximum number of knots the analyst entertains
for the current problem.
• Let all categorical predictors retain their original categories except for
pooling of very low prevalence categories (e.g., ones containing < 6 observations).
• Fit this general main e↵ects model.
• Compute the partial 2 statistic for testing the association of each predictor with the response, adjusted for all other predictors. In the case of
ordinary regression, convert partial F statistics to 2 statistics or partial
R2 values.
• Make corrections for chance associations to “level the playing field” for
predictors having greatly varying d.f., e.g., subtract the d.f. from the partial 2 (the expected value of 2p is p under H0 ).
• Make certain that tests of nonlinearity are not revealed as this would bias
the analyst.
• Sort the partial association statistics in descending order.
Commands in the rms package can be used to plot only what is needed.
Here is an example for a logistic model.
lrm ( y ⇠ s e x + r a c e + r c s ( age , 5 ) + r c s ( weight , 5 ) +
r c s ( height , 5 ) + r c s ( blood.pressure , 5 ) )
p l o t ( anova ( f ) )
f

This approach, and the rank correlation approach about to be discussed,
do not require the analyst to really prespecify predictor complexity, so how
are they not biased in our favor? There are two reasons: the analyst has already agreed to retain the variable in the model even if the strength of the
association is very low, and the assessment of association does not reveal
the degree of nonlinearity of the predictor to allow the analyst to “tweak”
the number of knots or to discard nonlinear terms. Any predictive ability a
variable might have may be concentrated in its nonlinear e↵ects, so using
the total association measure for a predictor to save degrees of freedom by
restricting the variable to be linear may result in no predictive ability. Likewise, a low association measure between a categorical variable and Y might
lead the analyst to collapse some of the categories based on their frequencies.
This often helps, but sometimes the categories that are so combined are the
ones that are most di↵erent from one another. So if using partial tests or
rank correlation to reduce degrees of freedom can harm the model, one might
argue that it is fair to allow this strategy to also benefit the analysis.

F test is normalized by
the number of
parameters, so it can
go down as you add
parameters - chi2 does
not do this (so Frank
prefers)

68

2

3

4 Multivariable Modeling Strategies

When collinearities or confounding are not problematic, a quicker approach
based on pairwise measures of association can be useful. This approach will
not have numerical problems (e.g., singular covariance matrix). When Y is
binary or continuous (but not censored), a good general-purpose measure of
association that is useful in making decisions about the number of parameters
to devote to a predictor is an extension of Spearman’s ⇢ rank correlation.
This is the ordinary R2 from predicting the rank of Y based on the rank of
X and the square of the rank of X. This ⇢2 will detect not only nonlinear
relationships (as will ordinary Spearman ⇢) but some non-monotonic ones
as well. It is important that the ordinary Spearman ⇢ not be computed, as
this would tempt the analyst to simplify the regression function (towards
monotonicity) if the generalized ⇢2 does not significantly exceed the square
of the ordinary Spearman ⇢. For categorical predictors, ranks are not squared
but instead the predictor is represented by a series of dummy variables. The
resulting ⇢2 is related to the Kruskal–Wallis test. See pp. 466 for an example.
Note that bivariable correlations can be misleading if marginal relationships
vary greatly from ones obtained after adjusting for other predictors.
Once one expands a predictor into linear and nonlinear terms and estimates the coefficients, the best way to understand the relationship between
predictors and response is to graph this estimated relationshipc . If the plot
appears almost linear or the test of nonlinearity is very insignificant there
is a temptation to simplify the model. The Grambsch and O’Brien result
described in Section 2.6 demonstrates why this is a bad idea.
From the above discussion a general principle emerges. Whenever the response variable is informally or formally linked, in an unmasked fashion, to
particular parameters that may be deleted from the model, special adjustments must be made in P -values, standard errors, test statistics, and confidence limits, in order for these statistics to have the correct interpretation.
Examples of strategies that are improper without special adjustments (e.g.,
using the bootstrap) include examining a frequency table or scatterplot to
decide that an association is too weak for the predictor to be included in
the model at all or to decide that the relationship appears so linear that all
nonlinear terms should be omitted. It is also valuable to consider the reverse
situation; that is, one posits a simple model and then additional analysis or
outside subject matter information makes the analyst want to generalize the
model. Once the model is generalized (e.g., nonlinear terms are added), the
test of association can be recomputed using multiple d.f. So another general
principle is that when one makes the model more complex, the d.f. properly increases and the new test statistics for association have the claimed
distribution. Thus moving from simple to more complex models presents no
problems other than conservatism if the new complex components are truly
unnecessary.
c

One can also perform a joint test of all parameters associated with nonlinear e↵ects.
This can be useful in demonstrating to the reader that some complexity was actually
needed.

4.3 Variable Selection

69

4.2 Checking Assumptions of Multiple Predictors
Simultaneously
Before developing a multivariable model one must decide whether the assumptions of each continuous predictor can be verified by ignoring the e↵ects
of all other potential predictors. In some cases, the shape of the relationship between a predictor and the property of response will be di↵erent if an
adjustment is made for other correlated factors when deriving regression estimates. Also, failure to adjust for an important factor can frequently alter the
nature of the distribution of Y . Occasionally, however, it is unwieldy to deal
simultaneously with all predictors at each stage in the analysis, and instead
the regression function shapes are assessed separately for each continuous
predictor.

4.3 Variable Selection
The material covered to this point dealt with a prespecified list of variables
to be included in the regression model. For reasons of developing a concise
model or because of a fear of collinearity or of a false belief that it is not
legitimate to include “insignificant” regression coefficients when presenting
results to the intended audience, stepwise variable selection is very commonly
employed. Variable selection is used when the analyst is faced with a series of
potential predictors but does not have (or use) the necessary subject matter
knowledge to enable her to prespecify the “important” variables to include
in the model. But using Y to compute P -values to decide which variables
to include is similar to using Y to decide how to pool treatments in a five–
treatment randomized trial, and then testing for global treatment di↵erences
using fewer than four degrees of freedom.
Stepwise variable selection has been a very popular technique for many
years, but if this procedure had just been proposed as a statistical method, it
would most likely be rejected because it violates every principle of statistical
estimation and hypothesis testing. Here is a summary of the problems with
this method.
1. It yields R2 values that are biased high.
2. The ordinary F and 2 test statistics do not have the claimed distributiond .229 Variable selection is based on methods (e.g., F tests for nested
models) that were intended to be used to test only prespecified hypotheses.
d

Lockhart et al.418 provide an example with n = 100 and 10 orthogonal predictors
where all true s are zero. The test statistic for the first variable to enter has type I
error of 0.39 when the nominal ↵ is set to 0.05.

Bootstrapping is not a good idea for variable selection - if you are using a boostrap to
tell you which are frequently selected, are going to be the ones with the smallest pvalues in the original analysis. It's not telling you anything new - just telling you the
ranking of the p-values in the first fit.

70

4 Multivariable Modeling Strategies

3. The method yields standard errors of regression coefficient estimates that
are biased low and confidence intervals for e↵ects and predicted values
that are falsely narrow.16
4. It yields P -values that are too small (i.e., there are severe multiple comparison problems) and that do not have the proper meaning, and the proper
correction for them is a very difficult problem.
5. It provides regression coefficients that are biased high in absolute value
and need shrinkage. Even if only a single predictor were being analyzed
and one only reported the regression coefficient for that predictor if its
association with Y were “statistically significant,” the estimate of the regression coefficient ˆ is biased (too large in absolute value). To put this
in symbols for the case where we obtain a positive association ( ˆ > 0),
E( ˆ|P < 0.05, ˆ > 0) > .97
6. In observational studies, variable selection to determine confounders for
adjustment results in residual confounding236 .
7. Rather than solving problems caused by collinearity, variable selection is
made arbitrary by collinearity.
8. It allows us to not think about the problem.
The problems of P -value-based variable selection are exacerbated when the
analyst (as she so often does) interprets the final model as if it were prespecified. Copas and Long122 stated one of the most serious problems with
stepwise modeling eloquently when they said, “The choice of the variables
to be included depends on estimated regression coefficients rather than their
true values, and so Xj is more likely to be included if its regression coefficient
is over-estimated than if its regression coefficient is underestimated.” Derksen
and Keselman152 studied stepwise variable selection, backward elimination,
and forward selection, with these conclusions:
1. “The degree of correlation between the predictor variables a↵ected the frequency with which authentic predictor variables found their way into the
final model.
2. The number of candidate predictor variables a↵ected the number of noise
variables that gained entry to the model.
3. The size of the sample was of little practical importance in determining the
number of authentic variables contained in the final model.
4. The population multiple coefficient of determination could be faithfully estimated by adopting a statistic that is adjusted by the total number of
candidate predictor variables rather than the number of variables in the
final model.”

They found that variables selected for the final model represented noise 0.20
to 0.74 of the time and that the final model usually contained less than half
of the actual number of authentic predictors. Hence there are many reasons
for using methods such as full-model fits or data reduction, instead of using
any stepwise variable selection algorithm.
If stepwise selection must be used, a global test of no regression should
be made before proceeding, simultaneously testing all candidate predictors

She!

4.3 Variable Selection

71

and having degrees of freedom equal to the number of candidate variables
(plus any nonlinear or interaction terms). If this global test is not significant,
selection of individually significant predictors is usually not warranted.
The method generally used for such variable selection is forward selection
of the most significant candidate or backward elimination of the least significant predictor in the model. One of the recommended stopping rules is
based on the “residual 2 ” with degrees of freedom equal to the number of
candidate variables remaining at the current step. The residual 2 can be
tested for significance (if one is able to forget that because of variable selection this statistic does not have a 2 distribution), or the stopping rule can
be based on Akaike’s information criterion (AIC33 ), here residual 2 2⇥
d.f.252 Of course, use of more insight from knowledge of the subject matter
will generally improve the modeling process substantially. It must be remembered that no currently available stopping rule was developed for data-driven
variable selection. Stopping rules such as AIC or Mallows’ Cp are intended
for comparing a limited number of prespecified models [65, Section 1.3]340e .
If the analyst insists on basing the stopping rule on P -values, the optimum
(in terms of predictive accuracy) ↵ to use in deciding which variables to
include in the model is ↵ = 1.0 unless there are a few powerful variables
and several completely irrelevant variables. A reasonable ↵ that does allow
for deletion of some variables is ↵ = 0.5.584 These values are far from the
traditional choices of ↵ = 0.05 or 0.10.
Even though forward stepwise variable selection is the most commonly
used method, the step-down method is preferred for the following reasons.
1. It usually performs better than forward stepwise methods, especially when
collinearity is present.430
2. It makes one examine a full model fit, which is the only fit providing
accurate standard errors, error mean square, and P -values.
3. The method of Lawless and Singhal377 allows extremely efficient step-down
modeling using Wald statistics, in the context of any fit from least squares
or maximum likelihood. This method requires passing through the data
matrix only to get the initial full fit.
For a given dataset, bootstrapping (Efron et al.147, 169, 174, 175 ) can help
decide between using full and reduced models. Bootstrapping can be done
e

AIC works successfully when the models being entertained are on a progression
defined by a single parameter, e.g. a common shrinkage coefficient or the single number
of knots to be used by all continuous predictors. AIC can also work when the model
that is best by AIC is much better than the runner-up so that if the process were
bootstrapped the same model would almost always be found. When used for one
variable at a time variable selection. AIC is just a restatement of the P -value, and
as such, doesn’t solve the severe problems with stepwise variable selection other than
forcing us to use slightly more sensible ↵ values. Some statisticians try to deal with
multiplicity problems caused by stepwise variable selection by making ↵ smaller than
0.05. This increases bias by giving variables whose e↵ects are estimated with error
a greater relative chance of being selected. Variable selection does not compete well
with shrinkage methods that simultaneously model all potential predictors.

4

5
6

72

7
8

4 Multivariable Modeling Strategies

on the whole model and compared with bootstrapped estimates of predictive
accuracy based on stepwise variable selection for each resample. Unless most
predictors are either very significant or clearly unimportant, the full model
usually outperforms the reduced model.
Full model fits have the advantage of providing meaningful confidence
intervals using standard formulas. Altman and Andersen16 gave an example
in which the lengths of confidence intervals of predicted survival probabilities
were 60% longer when bootstrapping was used to estimate the simultaneous
e↵ects of variability caused by variable selection and coefficient estimation, as
compared with confidence intervals computed ignoring how a “final” model
came to be. On the other hand, models developed on full fits after data
reduction will be optimum in many cases.
In some cases you may want to use the full model for prediction and variable selection for a “best bet” parsimonious list of independently important
predictors. This could be accompanied by a list of variables selected in 50
bootstrap samples to demonstrate the imprecision in the “best bet.”
Sauerbrei and Schumacher536 present a method to use bootstrapping to
actually select the set of variables. However, there are a number of drawbacks
to this approach35 :
1. The choice of an ↵ cuto↵ for determining whether a variable is retained in
a given bootstrap sample is arbitrary.
2. The choice of a cuto↵ for the proportion of bootstrap samples for which a
variable is retained, in order to include that variable in the final model, is
somewhat arbitrary.
3. Selection from among a set of correlated predictors is arbitrary, and all
highly correlated predictors may have a low bootstrap selection frequency.
It may be the case that none of them will be selected for the final model
even though when considered individually each of them may be highly
significant.
4. By using the bootstrap to choose variables, one must use the double bootstrap to resample the entire modeling process in order to validate the model
and to derive reliable confidence intervals. This may be computationally
prohibitive.
5. The bootstrap did not improve upon traditional backward stepdown variable selection. Both methods fail at identifying the “correct” variables.
For some applications the list of variables selected may be stabilized by
grouping variables according to subject matter considerations or empirical
correlations and testing each related group with a multiple degree of freedom
test. Then the entire group may be kept or deleted and, if desired, groups that
are retained can be summarized into a single variable or the most accurately
measured variable within the group can replace the group. See Section 4.7
for more on this.
Kass and Raftery330 showed that Bayes factors have several advantages
in variable selection, including the selection of less complex models that may

4.3 Variable Selection

73

agree better with subject matter knowledge. However, as in the case with
more traditional stopping rules, the final model may still have regression
coefficients that are too large. This problem is solved by Tibshirani’s lasso
method,601, 602 which is a penalized estimation technique in which the estimated regression coefficients are constrained so that the sum of their scaled
absolute values falls below some constant k chosen by cross-validation. This
kind of constraint forces some regression coefficient estimates to be exactly
zero, thus achieving variable selection while shrinking the remaining coefficients toward zero to reflect the overfitting caused by data-based model
selection.
A final problem with variable selection is illustrated by comparing this
approach with the sensible way many economists develop regression models. Economists frequently use the strategy of deleting only those variables
that are “insignificant” and whose regression coefficients have a nonsensible
direction. Standard variable selection on the other hand yields biologically
implausible findings in many cases by setting certain regression coefficients
exactly to zero. In a study of survival time for patients with heart failure,
for example, it would be implausible that patients having a specific symptom
live exactly as long as those without the symptom just because the symptom’s regression coefficient was “insignificant.” The lasso method shares this
difficulty with ordinary variable selection methods and with any method that
in the Bayesian context places nonzero prior probability on being exactly
zero.
Many papers claim that there were insufficient data to allow for multivariable modeling, so they did “univariable screening” wherein only “significant”
variables (i.e., those that are separately significantly associated with Y ) were
entered into the model.f This is just a forward stepwise variable selection in
which insignificant variables from the first step are not reanalyzed in later
steps. Univariable screening is thus even worse than stepwise modeling as
it can miss important variables that are only important after adjusting for
other variables.592 Overall, neither univariable screening nor stepwise variable selection in any way solves the problem of “too many variables, too few
subjects,” and they cause severe biases in the resulting multivariable model
fits while losing valuable predictive information from deleting marginally significant variables.
The online course notes contain a simple simulation study of stepwise
selection using R.

9

10

f

This is akin to doing a t-test to compare the two treatments (out of 10, say) that
are apparently most di↵erent from each other.

AIC is just as bad as p-value stepwise (same thing, except p-value is .13)

74

4 Multivariable Modeling Strategies

Table 4.1 Limiting Sample Sizes for Various Response Variables

Type of Response Variable
Continuous
Binary
Ordinal (k categories)
Failure (survival) time

Limiting Sample Size m
n (total sample size)
min(n1 , n2 ) h
Pk
n n12 i=1 n3i i
<number of failures j

in the continuous case, n_i=1

with ordinal, you want
4.4 Overfitting and Limits on Number of Predictors balanced categories

11

12

When a model is fitted that is too complex, that it, has too many free parameters to estimate for the amount of information in the data, the worth
of the model (e.g., R2 ) will be exaggerated and future observed values will
not agree with predicted values. In this situation, overfitting is said to be
present, and some of the findings of the analysis come from fitting noise and
not just signal, or finding spurious associations between X and Y . In this section general guidelines for preventing overfitting are given. Here we concern
ourselves with the reliability or calibration of a model, meaning the ability of
the model to predict future observations as well as it appeared to predict the
responses at hand. For now we avoid judging whether the model is adequate
for the task, but restrict our attention to the likelihood that the model has
significantly overfitted the data.
In typical low signal–to–noise ratio situationsg , model validations on independent datasets have found the minimum training sample size for which
the fitted model has an independently validated predictive discrimination
that equals the apparent discrimination seen with in training sample. Similar
validation experiments have considered the margin of error in estimating an
absolute quantity such as event probability. Studies such as263, 265, 573 have
shown that in many situations a fitted regression model is likely to be reliable when the number of predictors (or candidate predictors if using variable
selection) p is less than m/10 or m/20, where m is the “limiting sample size”
m
given in Table 4.1. A good average requirement is p < 15
. For example,
573
Smith et al.
found in one series of simulations that the expected error in
Cox model predicted five–year survival probabilities was below 0.05 when
p < m/20 for “average” subjects and below 0.10 when p < m/20 for “sick”
subjects, where m is the number of deaths. For “average” subjects, m/10 was
adequate for preventing expected errors > 0.1. Note: The number of nonintercept parameters in the model (p) is usually greater than the number of
predictors.
Narrowly distributed predictor variables (e.g., if all subjects’
g

These are situations where the true R2 is low, unlike tightly controlled experiments
and mechanistic models where signal:noise ratios can be quite high. In those situm
ations, many parameters can be estimated from small samples, and the 15
rule of
thumb can be significantly relaxed.

4.5 Shrinkage

75

ages are between 30 and 45 or only 5% of subjects are female) will require
even higher sample sizes. Note that the number of candidate variables must
include all variables screened for association with the response, including
nonlinear terms and interactions. Instead of relying on the rules of thumb in
the table, the shrinkage factor estimate presented in the next section can be
used to guide the analyst in determining how many d.f. to model (see p. 88).

4.5 Shrinkage
The term shrinkage is used in regression modeling to denote two ideas. The
first meaning relates to the slope of a calibration plot, which is a plot of
observed responses against predicted responses. When a dataset is used to
fit the model parameters as well as to obtain the calibration plot, the usual
estimation process will force the slope of observed versus predicted values to
be one. When, however, parameter estimates are derived from one dataset
and then applied to predict outcomes on an independent dataset, overfitting
will cause the slope of the calibration plot (i.e., the shrinkage factor) to be less
than one, a result of regression to the mean. Typically, low predictions will be
too low and high predictions too high. Predictions near the mean predicted
value will usually be quite accurate. The second meaning of shrinkage is a
statistical estimation method that preshrinks regression coefficients towards
zero so that the calibration plot for new data will not need shrinkage as its
calibration slope will be one.
We turn first to shrinkage as an adverse result of traditional modeling.
In ordinary linear regression, we know that all of the coefficient estimates
are exactly unbiased estimates of the true e↵ect when the model fits. Isn’t
the existence of shrinkage and overfitting implying that there is some kind
of bias in the parameter estimates? The answer is no because each separate
coefficient has the desired expectation. The problem lies in how we use the
c

See [480]. If one considers the power of a two-sample binomial test compared
with a Wilcoxon test if the response could be made continuous and the proportional odds assumption holds, the e↵ective sample size for a binary response is
3n1 n2 /n ⇡ 3 min(n1 , n2 ) if n1 /n is near 0 or 1 [658, Eq. 10, 15]. Here n1 and n2 are
the marginal frequencies of the two response levels.
d
Based on the power of a proportional odds model two-sample test when the marginal
cell sizes for the response are n1 , . . . , nk , compared with all cell sizes equal to unity
(response is continuous) [658, Eq, 3]. If all cell sizes are equal, the relative efficiency of
having k response categories compared with a continuous response is 1 1/k2 [658, Eq.
14]; for example, a five-level response is almost as efficient as a continuous one if
proportional odds holds across category cuto↵s.
e
This is approximate, as the e↵ective sample size may sometimes be boosted somewhat by censored observations, especially for non-proportional hazards methods such
as Wilcoxon-type tests.48

underfit by the amount
that you think the
validation model will be
off

76

4 Multivariable Modeling Strategies

coefficients. We tend not to pick out coefficients at random for interpretation
but we tend to highlight very small and very large coefficients.
A simple example may suffice. Consider a clinical trial with 10 randomly
assigned treatments such that the patient responses for each treatment are
normally distributed. We can do an ANOVA by fitting a multiple regression model with an intercept and nine dummy variables. The intercept is an
unbiased estimate of the mean response for patients on the first treatment,
and each of the other coefficients is an unbiased estimate of the di↵erence
in mean response between the treatment in question and the first treatment.
ˆ0 + ˆ1 is an unbiased estimate of the mean response for patients on the
second treatment. But if we plotted the predicted mean response for patients
against the observed responses from new data, the slope of this calibration
plot would typically be smaller than one. This is because in making this plot
we are not picking coefficients at random but we are sorting the coefficients
into ascending order. The treatment group having the lowest sample mean
response will usually have a higher mean in the future, and the treatment
group having the highest sample mean response will typically have a lower
mean in the future. The sample mean of the group having the highest sample
mean is not an unbiased estimate of its population mean.
As an illustration, let us draw 20 samples of size n = 50 from a uniform
distribution for which the true mean is 0.5. Figure 4.1 displays the 20 means
sorted into ascending order, similar to plotting Y versus Ŷ = X ˆ based
on least squares after sorting by X ˆ. Bias in the very lowest and highest
estimates is evident.
s e t . s e e d (123)
n
50
y
r u n i f ( 2 0 ⇤n )
group
r e p ( 1 : 2 0 , each=n )
ybar
t a p p l y ( y , group , mean )
ybar
s o r t ( ybar )
p l o t ( 1 : 2 0 , ybar , t y p e= ’ n ’ , a x e s=FALSE, y l i m=c ( . 3 , . 7 ) ,
x l a b= ’ Group ’ , y l a b= ’ Group Mean ’ )
l i n e s ( 1 : 2 0 , ybar )
p o i n t s ( 1 : 2 0 , ybar , pch =20 , c e x=. 5 )
axis (2)
a x i s ( 1 , a t = 1 : 2 0 , l a b e l s=FALSE)
f o r ( j i n 1 : 2 0 ) a x i s ( 1 , a t=j , l a b e l s=names ( ybar ) [ j ] )
a b l i n e ( h=. 5 , c o l=g r a y ( . 8 5 ) )

When we want to highlight a treatment that is not chosen at random (or a
priori), the data-based selection of that treatment needs to be compensated
for in the estimation process.k It is well known that the use of shrinkage
methods such as the James–Stein estimator to pull treatment means toward
k

It is interesting that researchers are quite comfortable with adjusting P -values for
post hoc selection of comparisons using, for example, the Bonferroni inequality, but
they do not realize that post hoc selection of comparisons also biases point estimates.

4.5 Shrinkage

77
0.7

0.6

Group Mean

when you identify something
because it's running high,
you will have regression to
the mean (and it could appear
causal...but may just be RTTM)

●

●

●

0.5

●
●

●

●

●

●

●

●

●

●

●

●

●

set up a selection on the basis of
things that are observed - you
are creating a bias in that
direction.

●

●

●
●

regression to the mean.

0.4

0.3

16 6 17 2 10 14 20 9

8

7 11 18 5

4

3

1 15 13 19 12

Group

Fig. 4.1 Sorted means from 20 samples of size 50 from a uniform [0, 1] distribution.
The reference line at 0.5 depicts the true population value of all of the means.

the grand mean over all treatments results in estimates of treatment-specific
means that are far superior to ordinary stratified means.173
Turning from a cell means model to the general case where predicted values
are general linear combinations X ˆ, the slope
of properly transformed
responses Y against X ˆ (sorted into ascending order) will be less than one
on new data. Estimation of the shrinkage coefficient allows quantification of
the amount of overfitting present, and it allows one to estimate the likelihood
that the model will reliably predict new observations. van Houwelingen and le
Cessie [625, Eq. 77] provided a heuristic shrinkage estimate that has worked
well in several examples:
ˆ=

model 2
model

p
2

almost the ratio of
adjusted R2 to
(4.1) unadjusted

,

where p is the total degrees of freedom for the predictors and model 2 is
the likelihood ratio 2 statistic for testing the joint influence of all predictors
simultaneously (see Section 9.3.1). For ordinary linear models, van Houwelingen and le Cessie proposed a shrinkage factor ˆ that can be shown to equal
2
n p 1 Radj
n 1
R2 ,

where the adjusted R2 is given by
2
Radj
=1

(1

R2 )

14

n
n

13

1
p

1

.

(4.2)

78

4 Multivariable Modeling Strategies

For such linear models with an intercept

0,

the shrunken estimate of

ˆs = (1 ˆ )Y + ˆ ˆ0
0
ˆs = ˆ ˆj , j = 1, . . . , p,
j

15

is

(4.3)

where Y is the mean of the response vector. Again, when stepwise fitting is
used, the p in these equations is much closer to the number of candidate degrees of freedom rather than the number in the “final” model. See Section 5.3
for methods of estimating using the bootstrap (p. 117) or cross-validation.
Now turn to the second usage of the term shrinkage. Just as clothing is
sometimes preshrunk so that it will not shrink further once it is purchased,
better calibrated predictions result when shrinkage is built into the estimation process in the first place. The object of shrinking regression coefficient
estimates is to obtain a shrinkage coefficient of = 1 on new data. Thus by
somewhat discounting ˆ we make the model underfitted on the data at hand
(i.e., apparent < 1) so that on new data extremely low or high predictions
are correct.
Ridge regression380, 625 is one technique for placing restrictions on the parameter estimates that results in shrinkage. A ridge parameter must be chosen
to control the amount of shrinkage. Penalized maximum likelihood estimation,232, 267, 380, 631 a generalization of ridge regression, is a general shrinkage
procedure. A method such as cross-validation or optimization of a modified
AIC must be used to choose an optimal penalty factor. An advantage of penalized estimation is that one can di↵erentially penalize the more complex
components of the model such as nonlinear or interaction e↵ects. A drawback
of ridge regression and penalized maximum likelihood is that the final model
is difficult to validate unbiasedly since the optimal amount of shrinkage is
usually determined by examining the entire dataset. Penalization is one of
the best ways to approach the “too many variables, too little data” problem.
See Section 9.10 for details.

4.6 Collinearity
When at least one of the predictors can be predicted well from the other
predictors, the standard errors of the regression coefficient estimates can be
inflated and corresponding tests have reduced power.213 In stepwise variable
selection, collinearity can cause predictors to compete and make the selection
of “important” variables arbitrary. Collinearity makes it difficult to estimate
and interpret a particular regression coefficient because the data have little
information about the e↵ect of changing one variable while holding another
(highly correlated) variable constant [98, Chap .9]. However, collinearity does
not a↵ect the joint influence of highly correlated variables when tested simultaneously. Therefore, once groups of highly correlated predictors are identi-

4.7 Data Reduction

only when you
want to hold something
constant that is
collinear with something
else that makes
you have a loss of power

79

fied, the problem can be rectified by testing the contribution of an entire set
with a multiple d.f. test rather than attempting to interpret the coefficient
or one d.f. test for a single predictor.
Collinearity does not a↵ect predictions made on the same dataset used to
estimate the model parameters or on new data that have the same degree
of collinearity as the original data [463, pp. 379–381] as long as extreme
extrapolation is not attempted. Consider as two predictors the total and LDL
cholesterols that are highly correlated. If predictions are made at the same
combinations of total and LDL cholesterol that occurred in the training data,
no problem will arise. However, if one makes a prediction at an inconsistent
combination of these two variables, the predictions may be inaccurate and
have high standard errors.
When the ordinary truncated power basis is used to derive component
variables for fitting linear and cubic splines, as was described earlier, the
component variables can be very collinear. It is very unlikely that this will
result in any problems, however, as the component variables are connected
algebraically. Thus it is not possible for a combination of, for example, x and
max(x 10, 0) to be inconsistent with each other. Collinearity problems are
then more likely to result from partially redundant subsets of predictors as
in the cholesterol example above.
One way to quantify collinearity is with variance inflation factors or VIF ,
which in ordinary least squares are diagonals of the inverse of the X 0 X matrix
scaled to have unit variance (except that a column of 1s is retained corresponding to the intercept). Note that some authors compute VIF from the
correlation matrix form of the design matrix, omitting the intercept. V IFi is
1/(1 Ri2 ) where Ri2 is the squared multiple correlation coefficient between
column i and the remaining columns of the design matrix. For models that are
fitted with maximum likelihood estimation, the information matrix is scaled
to correlation form, and VIF is the diagonal of the inverse of this scaled matrix.144, 648 Then the VIF are similar to those from a weighted correlation
matrix of the original columns in the design matrix. Note that indexes such
as VIF are not very informative as some variables are algebraically connected
to each other.
The SAS VARCLUS procedure534 and R varclus function can identify
collinear predictors. Summarizing collinear variables using a summary score
is more powerful and stable than arbitrary selection of one variable in a group
of collinear variables (see the next section).

4.7 Data Reduction
The sample size need not be as large as shown in Table 4.1 if the model
is validated independently and if you don’t care that the model may fail
to validate. However, it is likely that the model will be overfitted and will

16

17

80

4 Multivariable Modeling Strategies

if you maximize the
across subject variation,
it will be a good predictor

not validate if the sample size does not meet the guidelines. Use of data
reduction methods before model development is strongly recommended if the
conditions in Table 4.1 are not satisfied, and if shrinkage is not incorporated
into parameter estimation. Methods such as shrinkage and data reduction
take the 1st PC (maybe 2nd reduce the e↵ective d.f. of the model, making it more likely for the model
maybe 3rd) and put them to validate on future data. Some available data reduction methods are given
below.
in the model - this will
make a great prediction
1. Use the literature to eliminate unimportant variables.
2. Eliminate variables whose distributions are too narrow.
model, but don't try to
3. Eliminate candidate predictors that are missing in a large number of subinterpret
jects, especially if those same predictors are likely to be missing for future
applications of the model.
4. Use a statistical data reduction method such as incomplete principal component regression, nonlinear generalizations of principal components such
as principal surfaces, sliced inverse regression, variable clustering, or ordi18
nary cluster analysis on a measure of similarity between variables.
See Chapters 8 and 14 for detailed case studies in data reduction.

NOT using the outcome variable.

4.7.1 Redundancy Analysis
There are many approaches to data reduction. One rigorous approach involves
removing predictors that are easily predicted from other predictors, using
flexible parametric additive regression models. This approach is unlikely to
result in a major reduction in the number of regression coefficients to estimate
against Y , but will usually provide insights useful for later data reduction
over and above the insights given by methods based on pairwise correlations
instead of multiple R2 .
The Hmisc redun function implements the following redudancy checking
algorithm.
• Expand each continuous predictor into restricted cubic spline basis functions. Expand categorical predictors into dummy variables.
• Use OLS to predict each predictor with all component terms of all remaining predictors (similar to what the Hmisc transcan function does).
When the predictor is expanded into multiple terms, use the first canonical
variatel .
• Remove the predictor that can be predicted from the remaining set with
the highest adjusted or regular R2 .
• Predict all remaining predictors from their complement.
l

There is an option to force continuous variables to be linear when they are being
predicted.

4.7 Data Reduction

81

• Continue in like fashion until no variable still in the list of predictors can
be predicted with an R2 or adjusted R2 greater than a specified threshold
or until dropping the variable with the highest R2 (adjusted or ordinary)
would cause a variable that was dropped earlier to no longer be predicted
at the threshold from the now smaller list of predictors.
Special consideration must be given to categorical predictors. One way to
consider a categorical variable redundant is if a linear combination of dummy
variables representing it can be predicted from a linear combination of other
variables. For example, if there were 4 cities in the data and each city’s rainfall
was also present as a variable, with virtually the same rainfall reported for
all observations for a city, city would be redundant given rainfall (or viceversa). If two cities had the same rainfall, city might be declared redundant
even though tied cities might be deemed non-redundant in another setting. A
second, more stringent way to check for redundancy of a categorical predictor
is to ascertain whether all dummy variables created from the predictor are
individually redundant. The redun function implements both approaches.
Examples of use of redun are given in two case studies.

19

4.7.2 Variable Clustering
Although the use of subject matter knowledge is usually preferred, statistical
clustering techniques can be useful in determining independent dimensions
that are described by the entire list of candidate predictors. Once each dimension is scored (see below), the task of regression modeling is simplified,
and one quits trying to separate the e↵ects of factors that are measuring the
same phenomenon. One type of variable clustering534 is based on a type of
oblique-rotation principal component (PC) analysis that attempts to separate
variables so that the first PC of each group is representative of that group
(the first PC is the linear combination of variables having maximum variance subject to normalization constraints on the coefficients139, 141 ). Another
approach, that of doing a hierarchical cluster analysis on an appropriate similarity matrix (such as squared correlations) will often yield the same results.
For either approach, it is often advisable to use robust (e.g., rank-based)
measures for continuous variables if they are skewed, as skewed variables can
greatly a↵ect ordinary correlation coefficients. Pairwise deletion of missing
values is also advisable for this procedure—casewise deletion can result in a
small biased sample.
When variables are not monotonically related to each other, Pearson or
Spearman squared correlations can miss important associations and thus are
not always good similarity measures. A general and robust similarity measure is Hoe↵ding’s D,288 which for two variables X and Y is a measure of
the agreement between F (x, y) and G(x)H(y), where G, H are marginal cu-

20

82

21

4 Multivariable Modeling Strategies

mulative distribution functions and F is the joint CDF. The D statistic will
detect a wide variety of dependencies between two variables.
See pp. 335 and 463 for examples of variable clustering.

4.7.3 Transformation and Scaling Variables Without
Using Y
Scaling techniques often allow the analyst to reduce the number of parameters
to fit by estimating transformations for each predictor using only information
about associations with other predictors. It may be advisable to cluster variables before scaling so that patterns are derived only from variables that are
related. For purely categorical predictors, methods such as correspondence
analysis (see, for example, [105,136,234,383,449]) can be useful for data reduction. Often one can use these techniques to scale multiple dummy variables
into a few dimensions. For mixtures of categorical and continuous predictors,
qualitative principal component analysis such as the maximum total variance
(MTV) method of Young et al.449, 675 is useful. For the special case of representing a series of variables with one PC, the MTV method is quite easy to
implement.
1. Compute P C1 , the first PC of the variables to reduce X1 , . . . , Xq using
the correlation matrix of Xs.
2. Use ordinary linear regression to predict P C1 on the basis of functions of
the Xs, such as restricted cubic spline functions for continuous Xs or a
series of dummy variables for polytomous Xs. The expansion of each Xj
is regressed separately on P C1 .
3. These separately fitted regressions specify the working transformations of
each X.
4. Recompute P C1 by doing a PC analysis on the transformed Xs (predicted
values from the fits).
5. Repeat steps 2 to 4 until the proportion of variation explained by P C1
reaches a plateau. This typically requires three to four iterations.
A transformation procedure that is similar to MTV is the maximum generalized variance (MGV) method due to Sarle [361, pp. 1267–1268]. MGV
involves predicting each variable from (the current transformations of) all
the other variables. When predicting variable i, that variable is represented
as a set of linear and nonlinear terms (e.g., spline components). Analysis of
canonical variates275 can be used to find the linear combination of terms for
Xi (i.e., find a new transformation for Xi ) and the linear combination of the
current transformations of all other variables (representing each variable as
a single, transformed, variable) such that these two linear combinations have
maximum correlation. (For example, if there are only two variables X1 and X2
represented as quadratic polynomials, solve for a, b, c, d such that aX1 + bX12

4.7 Data Reduction

83

has maximum correlation with cX2 + dX22 .) The process is repeated until the
transformations converge. The goal of MGV is to transform each variable so
that it is most similar to predictions from the other transformed variables.
MGV does not use PCs (so one need not precede the analysis by variable
clustering), but once all variables have been transformed, you may want to
summarize them with the first PC.
The SAS PRINQUAL procedure of Kuhfeld361 implements the MTV and
MGV methods, and allows for very flexible transformations of the predictors,
including monotonic splines and ordinary cubic splines.
A very flexible automatic procedure for transforming each predictor in
turn, based on all remaining predictors, is the ACE (alternating conditional expectation) procedure of Breiman and Friedman.67 Like SAS PROC
PRINQUAL, ACE handles monotonically restricted transformations and categorical variables. It fits transformations by maximizing R2 between one variable and a set of variables. It automatically transforms all variables, using
the “super smoother”203 for continuous variables. Unfortunately, ACE does
not handle missing values. See Chapter 16 for more about ACE.
It must be noted that at best these automatic transformation procedures
generally find only marginal transformations, not transformations of each predictor adjusted for the e↵ects of all other predictors. When adjusted transformations di↵er markedly from marginal transformations, only joint modeling
of all predictors (and the response) will find the correct transformations.
Once transformations are estimated using only predictor information, the
adequacy of each predictor’s transformation can be checked by graphical
methods, by nonparametric smooths of transformed Xj versus Y , or by expanding the transformed Xj using a spline function. This approach of checking that transformations are optimal with respect to Y uses the response data,
but it accepts the initial transformations unless they are significantly inadequate. If the sample size is low, or if P C1 for the group of variables used
in deriving the transformations is deemed an adequate summary of those
variables, that P C1 can be used in modeling. In that way, data reduction is
accomplished two ways: by not using Y to estimate multiple coefficients for
a single predictor, and by reducing related variables into a single score, after
transforming them. See Chapter 8 for a detailed example of these scaling
techniques.

4.7.4 Simultaneous Transformation and Imputation
As mentioned in Chapter 3 (p. 54) if transformations are complex or nonmonotonic, ordinary imputation models may not work. SAS PROC PRINQUAL
implemented a method for simultaneously imputing missing values while solving for transformations. Unfortunately, the imputation procedure frequently
converges to imputed values that are outside the allowable range of the data.

84

4 Multivariable Modeling Strategies

This problem is more likely when multiple variables are missing on the same
subjects, since the transformation algorithm may simply separate missings
and nonmissings into clusters.
A simple modification of the MGV algorithm of PRINQUAL that simultaneously imputes missing values without these problems is implemented in the
R function transcan. Imputed values are initialized to medians of continuous variables and the most frequent category of categorical variables. For
continuous variables, transformations are initialized to linear functions. For
categorical ones, transformations may be initialized to the identify function,
to dummy variables indicating whether the observation has the most prevalent categorical value, or to random numbers. Then when using canonical
variates to transform each variable in turn, observations that are missing on
the current “dependent” variable are excluded from consideration, although
missing values for the current set of “predictors” are imputed. Transformed
variables are normalized to have mean 0 and standard deviation 1. Although
categorical variables are scored using the first canonical variate, transcan
has an option to use recursive partitioning to obtain imputed values on the
original scale (Section 2.5) for these variables. It defaults to imputing categorical variables using the category whose predicted canonical score is closest
to the predicted score.
transcan uses restricted cubic splines to model continuous variables. It
does not implement monotonicity constraints. transcan automatically constrains imputed values (both on transformed and original scales) to be in the
same range as non-imputed ones. This adds much stability to the resulting
estimates although it can result in a boundary e↵ect. Also, imputed values
can optionally be shrunken using Eq. 4.3 to avoid overfitting when developing the imputation models. Optionally, missing values can be set to specified
constants rather than estimating them. These constants are ignored during
the transformation-estimation phasem . This technique has proved to be helpful when, for example, a laboratory test is not ordered because a physician
thinks the patient has returned to normal with respect to the lab parameter
measured by the test. In that case, it’s better to use a normal lab value for
missings.
The transformation and imputation information created by transcan may
be used to transform/impute variables in datasets not used to develop the
transformation and imputation formulas. There is also an R function to create
R functions that compute the final transformed values of each predictor given
input values on the original scale.
As an example of non-monotonic transformation and imputation, consider
a sample of 1000 hospitalized patients from the SUPPORTn study.345 Two
mean arterial blood pressure measurements were set to missing.
m

If one were to estimate transformations without removing observations that had
these constants inserted for the current Y -variable, the resulting transformations
would likely have a spike at Y = imputation constant.
n
Study to Understand Prognoses Preferences Outcomes and Risks of Treatments

4.7 Data Reduction

85

r e q u i r e ( Hmisc )
getHdata ( s u p p o r t )
# Get data frame from web site
heart.rate
support $ hrt
blood.pressure
s u p p o r t $meanbp
blood.pressure [400:401]
Mean Arterial Blood Pressure Day 3
[1] 151 136
blood.pressure [400:401]
NA # Create two missings
d
data.frame ( heart.rate , blood.pressure )
par ( pch =46)
# Figure 4.2
w
t r a n s c a n (⇠ h e a r t . r a t e + b l o o d . p r e s s u r e , t r a n s f o r m e d=TRUE,
imputed=TRUE, show.na=TRUE, data=d )
Convergence criterion:2.901 0.035
0.007
Convergence in 4 iterations
R2 achieved in predicting each variable:
heart.rate blood.pressure
0.259
0.259
Adjusted R2:
heart.rate blood.pressure
0.254
0.253
w$ imputed $ b l o o d . p r e s s u r e
400
401
132.4057 109.7741
t
spe

w$ t r a n s f o r m e d
round ( c ( spearman ( h e a r t . r a t e , b l o o d . p r e s s u r e ) ,
spearman ( t [ , ’ h e a r t . r a t e ’ ] ,
t [ , ’ b l o o d . p r e s s u r e ’ ] ) ) , 2)

plot ( heart.rate , blood.pressure )
# Figure 4.3
plot ( t [ , ’ heart.rate ’ ] , t [ , ’ blood.pressure ’ ] ,
x l a b= ’ Transformed hr ’ , y l a b= ’ Transformed bp ’ )

Spearman’s rank correlation ⇢ between pairs of heart rate and blood pressure was -0.02, because these variables each require U -shaped transformations. Using restricted cubic splines with five knots placed at default quantiles, transcan provided the transformations shown in Figure 4.2. Correlation
between transformed variables is ⇢ = 0.13. The fitted transformations are
similar to those obtained from relating these two variables to time until death.

86

4 Multivariable Modeling Strategies
8

Transformed blood.pressure

Transformed heart.rate

0
6

4

2

0

0

50

100

150

200

250

−2

−4

−6

−8

300

0

50

heart.rate

100

150

blood.pressure

Fig. 4.2 Transformations fitted using transcan. Tick marks indicate the two imputed values for blood pressure.

●
●
●
●●
●
●
●●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●● ●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
100
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
50 ●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●
●
●
●
●
●
●
●
●
●● ● ● ●
●●●
●
●
●
●●●●●
● ● ●
●
●
●

●

Transformed bp

blood.pressure

150

0

●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
● ● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ● ●
●
●●
●
●●
●
●
●
●
●● ●
●
●
●
●●
●
●
−2 ●
●
●
●●●●
●
●
●
●●
●
●
●● ●● ●
● ●
● ● ●
●
●●
−4 ● ● ●●
●
●
●
0

●

●

●

●

−6

●

−8

●
0

●

●
50

100

150

200

250

300

heart.rate
Fig. 4.3 The lower left plot contains raw data (Spearman ⇢ =
is a scatterplot of the corresponding transformed values (⇢ =
of the SUPPORT study345 .

0

2

4

Transformed hr
0.02); the lower right
0.13). Data courtesy

6

8

4.7 Data Reduction

87

4.7.5 Simple Scoring of Variable Clusters
If a subset of the predictors is a series of related dichotomous variables, a
simpler data reduction strategy is sometimes employed. First, construct two
new predictors representing whether any of the factors is positive and a count
of the number of positive factors. For the ordinal count of the number of
positive factors, score the summary variable to satisfy linearity assumptions
as discussed previously. For the more powerful predictor of the two summary
measures, test for adequacy of scoring by using all dichotomous variables as
candidate predictors after adjusting for the new summary variable. A residual
2
statistic can be used to test whether the summary variable adequately
captures the predictive information of the series of binary predictors.o This
statistic will have degrees of freedom equal to one less than the number of
binary predictors when testing for adequacy of the summary count (and hence
will have low power when there are many predictors). Stratification by the
summary score and examination of responses over cells can be used to suggest
a transformation on the score.
Another approach to scoring a series of related dichotomous predictors
is to have “experts” assign severity points to each condition and then to
either sum these points or use a hierarchical rule that scores according to the
condition with the highest points (see Section 14.3 for an example). The latter
has the advantage of being easy to implement for field use. The adequacy of
either type of scoring can be checked using tests of linearity in a regression
modelp .

4.7.6 Simplifying Cluster Scores
If a variable cluster contains many individual predictors, parsimony may
sometimes be achieved by predicting the cluster score from a subset of its
components (using linear regression or CART (Section 2.5), for example).
Then a new cluster score is created and the response model is rerun with the
new score in the place of the original one. If one constituent variable has a
very high R2 in predicting the original cluster score, the single variable may
sometimes be substituted for the cluster score in refitting the model without
loss of predictive discrimination.
Sometimes it may be desired to simplify a variable cluster by asking the
question “which variables in the cluster are really the predictive ones?,” even
though this approach will usually cause true predictive discrimination to
o

Whether this statistic should be used to change the model is problematic in view
of model uncertainty.
p
The R function score.binary in the Hmisc library (see Section 6.2) assists in
computing a summary variable from the series of binary conditions.

22

88

23

4 Multivariable Modeling Strategies

su↵er. For clusters that are retained after limited step-down modeling, the
entire list of variables can be used as candidate predictors and the step-down
process repeated. All variables contained in clusters that were not selected
initially are ignored. A fair way to validate such two-stage models is to use a
resampling method (Section 5.3) with scores for deleted clusters as candidate
variables for each resample, along with all the individual variables in the clusters the analyst really wants to retain. A method called battery reduction can
be used to delete variables from clusters by determining if a subset of the variables can explain most of the variance explained by P C1 (see [139, Chapter
12] and438 ). This approach does not require examination of associations with
Y . Battery reduction can also be used to find a set of individual variables
that capture much of the information in the first k principal components.

4.7.7 How Much Data Reduction Is Necessary?
In addition to using the sample size to degrees of freedom ratio as a rough
guide to how much data reduction to do before model fitting, the heuristic
shrinkage estimate in Equation 4.1 can also be informative. First, fit a full
model with all candidate variables, nonlinear terms, and hypothesized interactions. Let p denote the number of parameters in this model, aside from any
intercepts. Let LR denote the log likelihood ratio 2 for this full model. The
estimated shrinkage is (LR p)/LR. If this falls below 0.9, for example, we
may be concerned with the lack of calibration the model may experience on
new data. Either a shrunken estimator or data reduction is needed. A reduced
model may have acceptable calibration if associations with Y are not used to
reduce the predictors.
A simple method, with an assumption, can be used to estimate the target
number of total regression degrees of freedom q in the model. In a “best
case,” the variables removed to arrive at the reduced model would have no
association with Y . The expected value of the 2 statistic for testing those
variables would then be p q. The shrinkage for the reduced model is then
on average [LR (p q) q]/[LR (p q)]. Setting this ratio to be 0.9 and
solving for q gives q  (LR p)/9. Therefore, reduction of dimensionality
down to q degrees of freedom would be expected to achieve < 10% shrinkage.
With these assumptions, there is no hope that a reduced model would have
acceptable calibration unless LR > p + 9. If the information explained by the
omitted variables is less than one would expect by chance (e.g., their total
2
is extremely small), a reduced model could still be beneficial, as long as
the conservative bound (LR q)/LR 0.9 or q  LR/10 were achieved. This
conservative bound assumes that no 2 is lost by the reduction, that is that
the final model 2 ⇡ LR. This is unlikely in practice. Had the p q omitted
variables had a larger 2 of 2(p q) (the break-even point for AIC), q must
be  (LR 2p)/8.

variable removal has to be
based on prior knowledge not
on p-values or AIC

4.8 New Directions in Predictive Modeling

89

As an example, suppose that a binary logistic model is being developed
from a sample containing 45 events on 150 subjects. The 10:1 rule suggests
we can analyze 4.5 degrees of freedom. The analyst wishes to analyze age,
sex, and 10 other variables. It is not known whether interaction between age
and sex exists, and whether age is linear. A restricted cubic spline is fitted
with four knots, and a linear interaction is allowed between age and sex.
These two variables then need 3 + 1 + 1 = 5 degrees of freedom. The other
10 variables are assumed to be linear and to not interact with themselves
or age and sex. There is a total of 15 d.f. The full model with 15 d.f. has
LR = 50. Expected shrinkage from this model is (50 15)/50 = 0.7. Since
LR > 15 + 9 = 24, some reduction might yield a better validating model.
Reduction to q = (50 15)/9 ⇡ 4 d.f. would be necessary, assuming the
reduced LR is about 50 (15 4) = 39. In this case the 10:1 rule yields
about the same value for q. The analyst may be forced to assume that age is
linear, modeling 3 d.f. for age and sex. The other 10 variables would have to
be reduced to a single variable using principal components or another scaling
put the variables in as a PC
technique. The AIC-based calculation yields a maximum of 2.5 d.f.
If the goal of the analysis is to make a series of hypothesis tests (adjusting rather than as
P -values for multiple comparisons) instead of to predict future responses, the
alternatively put all 15 var full model would have to be used.
in the model and shrink
A summary of the various data reduction methods is given in Figure 4.4.
the model down (penalize
When principal component analysis or related methods are used for data
reduction, the model may be harder to describe since internal coefficients are
it back to the effective
degrees of freedom) - only “hidden.” R code on p. 142 shows how an ordinary linear model fit can be
used in conjunction with a logistic model fit based on principal components
real downside is the
to draw a nomogram with axes for all predictors.
24
CI is much narrower, and
we don't necessarily know
how to interpret it or if it
4.8 New Directions in Predictive Modeling
is valid.
The approaches recommended in this text are
• fitting fully pre-specified models without deletion of “insignificant” predictors
• using data reduction methods (masked to Y ) to reduce the dimensionality
of the predictors and then fitting the number of parameters the data’s
information content can support
• using shrinkage (penalized estimation) to fit a large model without worrying about the sample size.
Data reduction approaches covered in the last section can yield very interpretable, stable models, but there are many decisions to be made when using a
two-stage (reduction/model fitting) approach, Newer single stage approaches
are evolving, including the following. These new approach handle continuous
predictors well, unlike recursive partitioning. Some of the new approaches are

make xb^hat more like
xbar(b)
when you plot prediction
versus observed, the slope
will always be 1.

90

4 Multivariable Modeling Strategies

Fig. 4.4 Summary of Some Data Reduction Methods
Goals

Reasons

Methods
Variable clustering

• Subject matter knowledge
Group predictors so that
• Group predictors to
• # d.f. arising from muleach group represents a
maximize proportion of
tiple predictors
single dimension that can
variance explained by
• Make P C1 more reasonbe summarized with a sinP C1 of each group
able summary
gle score
• Hierarchical clustering
using a matrix of similarity measures between
predictors

Transform predictors

• # d.f. due to nonlinear and dummy variable
components
• Allows predictors to be • Maximum total variance on a group of reoptimally combined
lated predictors
• Make P C1 more reason•
Canonical variates on
able summary
the total set of predic• Use
in
customized
tors
model for imputing
missing values on each
predictor

Score a group of predictors

# d.f. for group to unity

• P C1
• Simple point scores

Multiple
dimensional
scoring of all predictors

# d.f. for all predictors
combined

Principal
components
1, 2, . . . , k, k < p computed from all transformed predictors

• lasso (shrinkage using L1 norm favoring zero regression coefficients)584, 601
• elastic net (combination of L1 and L2 norms that handles the p > n case
better than the lasso)685
• adaptive lasso642, 679
• more flexible lasso to di↵erentially penalize for variable selection and for
regression coefficient estimation515
• group lasso to force selection of all or none of a group of related variables
(e.g., dummy variables representing a polytomous predictor)
• group lasso-like procedures that also allow for variables within a group to
be removed645

4.9 Overly Influential Observations

91

• sparse-group lasso using L1 and L2 norms to achieve spareness on groups
and within groups of variables565
• adaptive group lasso (Wang & Leng)
• Breiman’s nonnegative garrote672
• “preconditioning”, i.e., model simplification after developing a “black box”
predictive model478
• sparse principal component analysis to achieve parsimony in data reduction388, 392, 666, 682
• bagging, boosting, and random forests.271
One problem prevents most of these methods from being ready for everyday
use: they require scaling predictors before fitting the model. When a predictor is represented by nonlinear basis functions, the scaling recommendations
in the literature are not sensible. There are also computational issues and
difficulties obtaining hypothesis tests and confidence intervals.
When data reduction is not required, generalized additive models273, 668
should also be considered.

4.9 Overly Influential Observations
Every observation should influence the fit of a regression model. It can be
disheartening, however, if a significant treatment e↵ect or the shape of a
regression e↵ect rests on one or two observations. Overly influential observations also lead to increased variance of predicted values, especially when
variances are estimated by bootstrapping after taking variable selection into
account. In some cases, overly influential observations can cause one to abandon a model, “change” the data, or get more data. Observations can be overly
influential for several major reasons.
1. The most common reason is having too few observations for the complexity of the model being fitted. Remedies for this have been discussed in
Sections 4.7 and 4.3.
2. Data transcription or data entry errors can ruin a model fit.
3. Extreme values of the predictor variables can have a great impact, even
when these values are validated for accuracy. Sometimes the analyst may
deem a subject so atypical of other subjects in the study that deletion
of the case is warranted. On other occasions, it is beneficial to truncate
measurements where the data density ends. In one dataset of 4000 patients
and 2000 deaths, white blood count (WBC) ranged from 500 to 100,000
with .05 and .95 quantiles of 2755 and 26,700, respectively. Predictions
from a linear spline function of WBC were sensitive to WBC > 60,000, for
which there were 16 patients. There were 46 patients with WBC > 40,000.
Predictions were found to be more stable when WBC was truncated at
40,000, that is, setting WBC to 40,000 if WBC > 40,000.

92

4 Multivariable Modeling Strategies

4. Observations containing disagreements between the predictors and the response can influence the fit. Such disagreements should not lead to discarding the observations unless the predictor or response values are erroneous
as in Reason 3, or the analysis is made conditional on observations being
unlike the influential ones. In one example a single extreme predictor value
in a sample of size 8000 that was not on a straight line relationship with
the other (X, Y ) pairs caused a 2 of 36 for testing nonlinearity of the predictor. Remember that an imperfectly fitting model is a fact of life, and
discarding the observations can inflate the model’s predictive accuracy. On
rare occasions, such lack of fit may lead the analyst to make changes in
the model’s structure, but ordinarily this is best done from the “ground
up” using formal tests of lack of fit (e.g., a test of linearity or interaction).
Influential observations of the second and third kinds can often be detected
by careful quality control of the data. Statistical measures can also be helpful.
The most common measures that apply to a variety of regression models are
leverage, DFBETAS, DFFIT, and DFFITS.
Leverage measures the capacity of an observation to be influential due
to having extreme predictor values. Such an observation is not necessarily
influential. To compute leverage in ordinary least squares, we define the hat
matrix H given by
H = X(X 0 X) 1 X 0 .
(4.4)
H is the matrix that when multiplied by the response vector gives the predicted values, so it measures how an observation estimates its own predicted
response. The diagonals hii of H are the leverage measures and they are not
influenced by Y . It has been suggested46 that hii > 2(p + 1)/n signal a high
leverage point, where p is the number of columns in the design matrix X
aside from the intercept and n is the number of observations. Some believe
that the distribution of hii should be examined for values that are higher
than typical.
DFBETAS is the change in the vector of regression coefficient estimates
upon deletion of each observation in turn, scaled by their standard errors.46
Since DFBETAS encompasses an e↵ect for each predictor’s coefficient, DFBETAS allows the analyst to isolate the problem better than some of the
other measures. DFFIT is the change in the predicted X when the observation is dropped, and DFFITS is DFFIT standardized by the standard error
of the estimate of X . In both cases, the standard error used for normalization is recomputed each time an observation is omitted.
p Some classify an
observation as overly influential when |DFFITS| > 2 (p + 1)/(n p 1),
while others prefer to examine the entire distribution of DFFITS to identify
“outliers”.46
Section 10.7 discusses influence measures for the logistic model, which
requires maximum likelihood estimation. These measures require the use of
special residuals and information matrices (in place of X 0 X).

4.10 Comparing Two Models

93

If truly influential observations are identified using these indexes, careful
thought is needed to decide how (or whether) to deal with them. Most important, there is no substitute for careful examination of the dataset before
doing any analyses.96 Spence and Garrison [577, p. 16] feel that
Although the identification of aberrations receives considerable attention in
most modern statistical courses, the emphasis sometimes seems to be on disposing of embarrassing data by searching for sources of technical error or minimizing the influence of inconvenient data by the application of resistant methods.
Working scientists often find the most interesting aspect of the analysis inheres
in the lack of fit rather than the fit itself.

4.10 Comparing Two Models
Frequently one wants to choose between two competing models on the basis
of a common set of observations. The methods that follow assume that the
performance of the models is evaluated on a sample not used to develop either
one. In this case, predicted values from the model can usually be considered as
a single new variable for comparison with responses in the new dataset. These
methods listed below will also work if the models are compared using the same
set of data used to fit each one, as long as both models have the same e↵ective
number of (candidate or actual) parameters. This requirement prevents us
from rewarding a model just because it overfits the training sample (see
Section 9.8.1 for a method comparing two models of di↵ering complexity).
The methods can also be enhanced using bootstrapping or cross-validation
on a single sample to get a fair comparison when the playing field is not level,
for example, when one model had more opportunity for fitting or overfitting
the responses.
Some of the criteria for choosing one model over the other are
1.
2.
3.
4.
5.

calibration,
discrimination,
face validity,
measurement errors in required predictors,
use of continuous predictors (which are usually better defined than categorical ones),
6. omission of “insignificant” variables that nonetheless make sense as risk
factors,
7. simplicity (although this is less important with the availability of computers), and
8. lack of fit for specific types of subjects.
Items 3 through 7 require subjective judgment, so we focus on the other aspects. If the purpose of the models is only to rank-order subjects, calibration
is not an issue. Otherwise, a model having poor calibration can be dismissed

cannot compare two AUROC
because AUROC is based on
a rank statistic.

94

25

4 Multivariable Modeling Strategies

outright. Given that the two models have similar calibration, discrimination
should be examined critically. Various statistical indexes can quantify discrimination ability (e.g., R2 , model 2 , Somers’ Dxy , Spearman’s ⇢, area under ROC curve—see Section 10.8). Rank measures (Dxy , ⇢, ROC area) only
measure how well predicted values can rank-order responses. For example,
predicted probabilities of 0.01 and 0.99 for a pair of subjects are no better
than probabilities of 0.2 and 0.8 using rank measures, if the first subject had
a lower response value than the second. Therefore, rank measures such as
ROC area (c index), although fine for describing a given model, may not be
very sensitive in choosing between two models115, 481, 486 . This is especially
true when the models are strong, as it is easier to move a rank correlation
from 0.6 to 0.7 than it is to move it from 0.9 to 1.0. Measures such as R2 and
the model 2 statistic (calculated from the predicted and observed responses)
are more sensitive. Still, one may not know how to interpret the added utility
of a model that boosts the R2 from 0.80 to 0.81.
Again given that both models are equally well calibrated, discrimination
can be studied more simply by examining the distribution of predicted values
Ŷ . Suppose that the predicted value is the probability that a subject dies.
Then high-resolution histograms of the predicted risk distributions for the
two models can be very revealing. If one model assigns 0.02 of the sample to
a risk of dying above 0.9 while the other model assigns 0.08 of the sample to
the high risk group, the second model is more discriminating. The worth of a
model can be judged by how far it goes out on a limb while still maintaining
good calibration.
Frequently, one model will have a similar discrimination index to another
model, but the likelihood ratio 2 statistic is meaningfully greater for one. Assuming corrections have been made for complexity, the model with the higher
2
usually has a better fit for some subjects, although not necessarily for the
average subject. A crude plot of predictions from the first model against
predictions from the second, possibly stratified by Y , can help describe the
di↵erences in the models. More specific analyses will determine the characteristics of subjects where the di↵erences are greatest. Large di↵erences may
be caused by an omitted, underweighted, or improperly transformed predictor, among other reasons. In one example, two models for predicting hospital
mortality in critically ill patients had the same discrimination index (to two
decimal places). For the relatively small subset of patients with extremely low
white blood counts or serum albumin, the model that treated these factors
as continuous variables provided predictions that were very much di↵erent
from a model that did not.
When comparing predictions for two models that may not be calibrated
(from overfitting, e.g.), the two sets of predictions may be shrunk so as to
not give credit for overfitting (see Equation 4.1).
Sometimes one wishes to compare two models that used the response variable di↵erently, a much more difficult problem. For example, an investigator
may want to choose between a survival model that used time as a continuous

Don't compare c-statistic,
compare the probability of
concordance - taking the
patient into account

if you just want to compare 2
models, just use chi-square
likelihood.

4.12 Summary: Possible Modeling Strategies

95

variable, and a binary logistic model for dead/alive at six months. Here, other
considerations are also important (see Section 17.1). A model that predicts
dead/alive at six months does not use the response variable e↵ectively, and
it provides no information on the chance of dying within three months.
When one or both of the models is fitted using least squares, it is useful
to compare them using an error measure that was not used as the optimization criterion, such as mean absolute error or median absolute error. Mean
and median absolute errors are excellent measures for judging the value of a
model developed without transforming the response to a model fitted after
transforming Y , then back-transforming to get predictions.

26

4.11 Improving the Practice of Multivariable Prediction
Standards for published predictive modeling and feature selection in highdimensional problems are not very high. There are several things that a good
analyst can do to improve the situation.
1. Insist on validation of predictive models and discoveries.
2. Show collaborators that split-sample validation is not appropriate unless
the number of subjects is huge
• This can be demonstrated by spliting the data more than once and
seeing volatile results, and by calculating a a confidence interval for the
predictive accuracy in the test dataset and showing that it is very wide.
3. Run a simulation study with no real associations and show that associations are easy to find if a dangerous data mining procedure is used.
Alternately, analyze the collaborator’s data after randomly permuting the
Y vector and show some “positive” findings.
4. Show that alternative explanations are easy to posit. For example:
• The importance of a risk factor may disappear if 5 “unimportant” risk
factors are added back to the model
• Omitted main e↵ects can explain away apparent interactions.
• Perform a uniqueness analysis: attempt to predict the predicted values from a model derived by data torture from all of the features not
used in the model. If one can obtain R2 = 0.85 in predicting the “winning” feature signature (predicted values) from the “losing” features,
the “winning” pattern is not unique and may be unreliable.

4.12 Summary: Possible Modeling Strategies
Some possible global modeling strategies are to

shouldn't try to pick
winners because we can't
achieve a unique result.

96

4 Multivariable Modeling Strategies

• Use a method known not to work well (e.g., stepwise variable selection
without penalization; recursive partitioning resulting in a single tree), document how poorly the model performs (e.g. using the bootstrap), and use
the model anyway
• Develop a black box model that performs poorly and is difficult to interpret
(e.g., does not incorporate penalization)
• Develop a black box model that performs well and is difficult to interpret
• Develop interpretable approximations to the black box
• Develop an interpretable model (e.g. give priority to additive e↵ects) that
performs well and is likely to perform equally well on future data from the
same stream.
As stated in the Preface, the strategy emphasized in this text, stemming
from the last philosophy, is to decide how many degrees of freedom can be
“spent,” where they should be spent, and then to spend them. If statistical
tests or confidence limits are required, later reconsideration of how d.f. are
spent is not usually recommended. In what follows some default strategies
are elaborated. These strategies are far from failsafe, but they should allow
the reader to develop a strategy that is tailored to a particular problem. At
the least these default strategies are concrete enough to be criticized so that
statisticians can devise better ones.

4.12.1 Developing Predictive Models
The following strategy is generic although it is aimed principally at the development of accurate predictive models.
1. Assemble as much accurate pertinent data as possible, with wide distributions for predictor values. For survival time data, follow-up must be
sufficient to capture enough events as well as the clinically meaningful
phases if dealing with a chronic process.
2. Formulate good hypotheses that lead to specification of relevant candidate
predictors and possible interactions. Don’t use Y (either informally using
graphs, descriptive statistics, or tables, or formally using hypothesis tests
or estimates of e↵ects such as odds ratios) in devising the list of candidate
predictors.
3. If there are missing Y values on a small fraction of the subjects but Y can
be reliably substituted by a surrogate response, use the surrogate to replace
the missing values. Characterize tendencies for Y to be missing using, for
example, recursive partitioning or binary logistic regression. Depending on
the model used, even the information on X for observations with missing
Y can be used to improve precision of ˆ, so multiple imputation of Y can
sometimes be e↵ective. Otherwise, discard observations having missing Y .

4.12 Summary: Possible Modeling Strategies

97

4. Impute missing Xs if the fraction of observations with any missing Xs is
not tiny. Characterize observations that had to be discarded. Special imputation models may be needed if a continuous X needs a non-monotonic
transformation (p. 54). These models can simultaneously impute missing
values while determining transformations. In most cases, multiply impute
missing Xs based on other Xs and Y , and other available information
about the missing data mechanism.
5. For each predictor specify the complexity or degree of nonlinearity that
should be allowed (see Section 4.1). When prior knowledge does not indicate that a predictor has a linear e↵ect on the property C(Y |X) (the
property of the response that can be linearly related to X), specify the
number of degrees of freedom that should be devoted to the predictor. The
d.f. (or number of knots) can be larger when the predictor is thought to
be more important in predicting Y or when the sample size is large.
6. If the number of terms fitted or tested in the modeling process (counting
nonlinear and cross-product terms) is too large in comparison with the
number of outcomes in the sample, use data reduction (ignoring Y ) until
the number of remaining free variables needing regression coefficients is
tolerable. Use the m/10 or m/15 rule or an estimate of likely shrinkage
or overfitting (Section 4.7) as a guide. Transformations determined from
the previous step may be used to reduce each predictor into 1 d.f., or the
transformed variables may be clustered into highly correlated groups if
more data reduction is required. Alternatively, use penalized estimation
with the entire set of variables. This will also e↵ectively reduce the total
degrees of freedom.267
7. Use the entire sample in the model development as data are too precious
to waste. If steps listed below are too difficult to repeat for each bootstrap
or cross-validation sample, hold out test data from all model development
steps that follow.
8. When you can test for model complexity in a very structured way, you may
be able to simplify the model without a great need to penalize the final
model for having made this initial look. For example, it can be advisable
to test an entire group of variables (e.g., those more expensive to collect)
and to either delete or retain the entire group for further modeling, based
on a single P -value (especially if the P value is not between 0.05 and 0.2).
Another example of structured testing to simplify the “initial” model is
making all continuous predictors have the same number of knots k, varying
k from 0 (linear), 3, 4, 5, . . . , and choosing the value of k that optimizes
AIC. A composite test of all nonlinear e↵ects in a model can also be
used, and statistical inferences are not invalidated if the global test of
nonlinearity yields P > 0.2 or so and the analyst deletes all nonlinear
terms.
9. Make tests of linearity of e↵ects in the model only to demonstrate to others
that such e↵ects are often statistically significant. Don’t remove insignificant e↵ects from the model when tested separately by predictor. Any

98

10.

11.
12.
13.

14.
15.

16.

17.
18.

19.
27

20.

4 Multivariable Modeling Strategies

examination of the response that might result in simplifying the model
needs to be accounted for in computing confidence limits and other statistics. It is preferable to retain the complexity that was prespecified in Step 5
regardless of the results of assessments of nonlinearity.
Check additivity assumptions by testing prespecified interaction terms. If
the global test for additivity is significant or equivocal, all prespecified
interactions should be retained in the model. If the test is decisive (e.g.,
P > 0.3), all interaction terms can be omitted, and in all likelihood there
is no need to repeat this pooled test for each resample during model validation. In other words, one can assume that had the global interaction
test been carried out for each bootstrap resample it would have been insignificant at the 0.05 level more than, say, 0.9 of the time. In this large
P -value case the pooled interaction test did not induce an uncertainty in
model selection that needed accounting.
Check to see if there are overly influential observations.
Check distributional assumptions and choose a di↵erent model if needed.
Do limited backwards step-down variable selection if parsimony is more
important than accuracy.578 The cost of doing any aggressive variable
selection is that the variable selection algorithm must also be included
in a resampling procedure to properly validate the model or to compute
confidence limits and the like.
This is the “final” model.
Interpret the model graphically (Section 5.1) and by examining predicted
values and using appropriate significance tests without trying to interpret
some of the individual model parameters. For collinear predictors obtain
pooled tests of association so that competition among variables will not
give misleading impressions of their total significance.
Validate the final model for calibration and discrimination ability, preferably using bootstrapping (see Section 5.3). Steps 9 to 13 must be repeated
for each bootstrap sample, at least approximately. For example, if age was
transformed when building the final model, and the transformation was
suggested by the data using a fit involving age and age2 , each bootstrap
repetition should include both age variables with a possible step-down
from the quadratic to the linear model based on automatic significance
testing at each step.
Shrink parameter estimates if there is overfitting but no further data reduction is desired, if shrinkage was not built into the estimation process.
When missing values were imputed, adjust final variance–covariance matrix for imputation wherever possible (e.g., using bootstrap or multiple
imputation). This may a↵ect some of the other results.
When all steps of the modeling strategy can be automated, consider using Faraway’s method182 to penalize for the randomness inherent in the
multiple steps.
Develop simplifications to the full model by approximating it to any desired
degrees of accuracy (Section 5.5).

4.12 Summary: Possible Modeling Strategies

99

4.12.2 Developing Models for E↵ect Estimation
By e↵ect estimation is meant point and interval estimation of di↵erences in
properties of the responses between two or more settings of some predictors,
or estimating some function of these di↵erences such as the antilog. In ordinary multiple regression with no transformation of Y such di↵erences are
absolute estimates. In regression involving log(Y ) or in logistic or proportional hazards models, e↵ect estimation is, at least initially, concerned with
estimation of relative e↵ects. As discussed on pp. 3 and 226, estimation of
absolute e↵ects for these models must involve accurate prediction of overall
response values, so the strategy in the previous section applies.
When estimating di↵erences or relative e↵ects, the bias in the e↵ect estimate, besides being influenced by the study design, is related to how well
subject heterogeneity and confounding are taken into account. The variance
of the e↵ect estimate is related to the distribution of the variable whose levels
are being compared, and, in least squares estimates, to the amount of variation “explained” by the entire set of predictors. Variance of the estimated
di↵erence can increase if there is overfitting. So for estimation, the previous
strategy largely applies.
The following are di↵erences in the modeling strategy when e↵ect estimation is the goal.
1. There is even less gain from having a parsimonious model than when
developing overall predictive models, as estimation is usually done at the
time of analysis. Leaving insignificant predictors in the model increases
the likelihood that the confidence interval for the e↵ect of interest has the
stated coverage. By contrast, overall predictions are conditional on the
values of all predictors in the model. The variance of such predictions is
increased by the presence of unimportant variables, as predictions are still
conditional on the particular values of these variables (Section 5.5.1) and
cancellation of terms (which occurs when di↵erences are of interest) does
not occur.
2. Careful consideration of inclusion of interactions is still a major consideration for estimation. If a predictor whose e↵ects are of major interest
is allowed to interact with one or more other predictors, e↵ect estimates
must be conditional on the values of the other predictors and hence have
higher variance.
3. A major goal of imputation is to avoid lowering the sample size because
of missing values in adjustment variables. If the predictor of interest is the
only variable having a substantial number of missing values, multiple imputation is less worthwhile, unless it corrects for a substantial bias caused
by deletion of nonrandomly missing data.
4. The analyst need not be very concerned about conserving degrees of freedom devoted to the predictor of interest. The complexity allowed for this

100

4 Multivariable Modeling Strategies

variable is usually determined by prior beliefs, with compromises that consider the bias-variance trade-o↵.
5. If penalized estimation is used, the analyst may wish to not shrink parameter estimates for the predictor of interest.
6. Model validation is not necessary unless the analyst wishes to use it to
quantify the degree of overfitting.

4.12.3 Developing Models for Hypothesis Testing
A default strategy for developing a multivariable model that is to be used
as a basis for hypothesis testing is almost the same as the strategy used for
estimation.
1. There is little concern for parsimony. A full model fit, including insignificant variables, will result in more accurate P -values for tests for the variables of interest.
2. Careful consideration of inclusion of interactions is still a major consideration for hypothesis testing. If one or more predictors interacts with a
variable of interest, either separate hypothesis tests are carried out over
the levels of the interacting factors, or a combined “main e↵ect + interaction” test is performed. For example, a very well–defined test is whether
treatment is e↵ective for any race group.
3. If the predictor of interest is the only variable having a substantial number
of missing values, multiple imputation is less worthwhile. In some cases,
multiple imputation may increase power (e.g., in ordinary multiple regression one can obtain larger degrees of freedom for error) but in others there
will be little net gain. However, the test can be biased due to exclusion of
nonrandomly missing observations if imputation is not done.
4. As before, the analyst need not be very concerned about conserving degrees
of freedom devoted to the predictor of interest. The degrees of freedom
allowed for this variable is usually determined by prior beliefs, with careful
consideration of the trade-o↵ between bias and power.
5. If penalized estimation is used, the analyst should not shrink parameter
estimates for the predictors being tested.
6. Model validation is not necessary unless the analyst wishes to use it to
quantify the degree of overfitting. This may shed light on whether there is
overadjustment for confounders.

4.13 Further Reading
1

Some good general references that address modeling strategies are [212, 264, 469,
585].

Inc. PC - main effects +IPC(interactions)
that would be a reduced rank regression to concentrate the interactions

4.13 Further Reading
2

3

4

5

6

7
8

9
10

11
12

13
14

15

101

Even though they used a generalized correlation index for screening variables
and not for transforming them, Hall and Miller244 present a related idea, computing the ordinary R2 against a cubic spline transformation of each potential
predictor.
Simulation studies are needed to determine the e↵ects of modifying the model
based on assessments of “predictor promise.” Although it is unlikely that this
strategy will result in regression coefficients that are biased high in absolute
value, it may on some occasions result in somewhat optimistic standard errors
and a slight elevation in type I error probability. Some simulation results may
be found on the Web site. Initial promising findings for least squares models
for two uncorrelated predictors indicate that the procedure is conservative in
its estimation of 2 and in preserving type I error.
Verweij and van Houwelingen632 and Shao560 describe how cross-validation can
be used in formulating a stopping rule. Luo et al.423 developed an approach to
tuning forward selection by adding noise to Y .
Roecker523 compared forward variable selection (FS) and all possible subsets
selection (APS) with full model fits in ordinary least squares. APS had a greater
tendency to select smaller, less accurate models than FS. Neither selection technique was as accurate as the full model fit unless more than half of the candidate
variables was redundant or unnecessary.
Wiegand662 showed that it is not very fruitful to try di↵erent stepwise algorithms and then to be comforted by agreements in some of the variables selected.
It is easy for di↵erent stepwise methods to agree on the wrong set of variables.
Other results on how variable selection a↵ects inference may be found in Hurvich
and Tsai309 and Breiman [65, Section 8.1].
Goring et al.222 presented an interesting analysis of the huge bias caused by
conditioning analyses on statistical significance in a high-dimensional genetics
context.
Steyerberg et al.584 have comparisons of smoothly penalized estimators with
the lasso and with several stepwise variable selection algorithms.
See Weiss,650 Faraway,182 and Chatfield97 for more discussions of the e↵ect of
not prespecifying models, for example, dependence of point estimates of e↵ects
on the variables used for adjustment.
Greenland236 provides an example in which overfitting a logistic model resulted
in far too many predictors with P < 0.05.
See Peduzzi et al.479, 480 for studies of the relationship between “events per
variable” and types I and II error, accuracy of variance estimates, and accuracy
of normal approximations for regression coefficient estimators. Their findings
are consistent with those given in the text (but636 has a slightly di↵erent take)
Copas [119, Eq. 8.5] adds 2 to the numerator of Equation 4.1 (see also [497,623]).
An excellent discussion about such indexes may be found in http://r.
789695.n4.nabble.com/Adjusted-R-squared-formula-in-lm-td4656857.
html where J. Lucke points out that R2 tends to n p 1 when the population R2
2
is zero, but Radj
converges to zero.
Efron [170, Eq. 4.23] and van Houwelingen and le Cessie625 showed that the
average expected optimism in a mean logarithmic quality score for a p-predictor
binary logistic model is p/n. Taylor et al.594 showed that the ratio of variances
for certain quantities is proportional to the ratio of the number of parameters
in two models. Copas stated that “Shrinkage can be particularly marked when
stepwise fitting is used: the shrinkage is then closer to that expected of the
full regression rather than of the subset regression actually fitted.”119, 497, 623
Spiegelhalter,578 in arguing against variable selection, states that better prediction will often be obtained by fitting all candidate variables in the final model,
shrinking the vector of regression coefficient estimates towards zero.

102
16
17

18

19
20
21

22

23

24

25

26
27

4 Multivariable Modeling Strategies
See Belsley [45, pp. 28–30] for some reservations about using VIF.
Friedman and Wall204 discuss and provide graphical devices for explaining suppression by a predictor not correlated with the response but that is correlated
with another predictor. Adjusting for a suppressor variable will increase the
predictive discrimination of the model. Meinshausen446 developed a novel hierarchical approach to gauging the importance of collinear predictors.
For incomplete principal component regression see [98, 116, 117, 139, 141, 313, 318].
See388, 682 for sparse principal component analysis methods in which constraints
are applied to loadings so that some of them are set to zero. The latter reference provides a principal component method for binary data. See241 for a
type of sparse principal component analysis that also encourages loadings to
be similar for a group of highly correlated variables and allows for a type
of variable clustering.See [382] for principal surfaces. Sliced inverse regression
is described in [101, 116, 117, 185, 396, 397]. For material on variable clustering
see [139, 141, 263, 434, 534]. A good general reference on cluster analysis is [626,
Chapter 11]. de Leeuw and Mair in their R homals package [150] have one of
the most general approaches to data reduction related to optimal scaling. Their
approach includes nonlinear principal component analysis among several other
multivariate analyses.
The redudancy analysis described here is related to principal variables 441 but
is faster.
Meinshausen446 developed a method of testing the importance of competing
(collinear) variables using an interesting automatic clustering procedure.
The R ClustOfVar package by Marie Chavent, Vanessa Kuentz, Benoit Liquet, and Jerome Saracco generalizes variable clustering and explicitly handles
a mixture of quantitative and categorical predictors. It also implements bootstrap cluster stability analysis.
Principal components are commonly used to summarize a cluster of variables.
Vines635 developed a method to constrain the principal component coefficients
to be integers without much loss of explained variability.
Jolli↵e317 presented a way to discard some of the variables making up principal
components. Wang and Gehan641 presented a new method for finding subsets of
predictors that approximate a set of principal components, and surveyed other
methods for simplifying principal components.
See D’Agostino et al.141 for excellent examples of variable clustering (including
a two-stage approach) and other data reduction techniques using both statistical
methods and subject-matter expertise.
Cook115 and Pencina et al.483, 485, 486 present a good approach for judging the
added value of new variables that is based on evaluating the extent to which
the new information moves predicted probabilities higher for subjects having
events and lower for subjects not having events.
The Hmisc abs.error.pred function computes a variety of accuracy measures
based on absolute errors.
Shen et al.562 developed an “optimal approximation” method to make correct
inferences after model selection.

4.14 Problems
Analyze the SUPPORT dataset (getHdata(support)) as directed below to
relate selected variables to total cost of the hospitalization. Make sure this

4.14 Problems

103

response variable is utilized in a way that approximately satisfies the assumptions of normality-based multiple regression so that statistical inferences will
be accurate. See problems at the end of Chapters 3 and 7 of the text for more
information. Consider as predictors mean arterial blood pressure, heart rate,
age, disease group, and coma score.
1. Do an analysis to understand interrelationships among predictors, and find
optimal scaling (transformations) that make the predictors better relate
to each other (e.g., optimize the variation explained by the first principal
component).
2. Do a redundancy analysis of the predictors, using both a less stringent and
a more stringent approach to assessing the redundancy of the multiple-level
variable disease group.
3. Do an analysis that helps one determine how many d.f. to devote to each
predictor.
4. Fit a model, assuming the above predictors act additively, but do not assume linearity for the age and blood pressure e↵ects. Use the truncated
power basis for fitting restricted cubic spline functions with 5 knots. Estimate the shrinkage coefficient ˆ .
5. Make appropriate graphical diagnostics for this model.
6. Test linearity in age, linearity in blood pressure, and linearity in heart rate,
and also do a joint test of linearity simultaneously in all three predictors.
7. Expand the model to not assume additivity of age and blood pressure.
Use a tensor natural spline or an appropriate restricted tensor spline. If
you run into any numerical difficulties, use 4 knots instead of 5. Plot in an
interpretable fashion the estimated 3-D relationship between age, blood
pressure, and cost for a fixed disease group.
8. Test for additivity of age and blood pressure. Make a joint test for the
overall absence of complexity in the model (linearity and additivity simultaneously).

Chapter 5

Describing, Resampling, Validating, and
Simplifying the Model

5.1 Describing the Fitted Model
5.1.1 Interpreting E↵ects
Before addressing issues related to describing and interpreting the model
and its coefficients, one can never apply too much caution in attempting to
interpret results in a causal manner. Regression models are excellent tools
for estimating and inferring associations between an X and Y given that the
“right” variables are in the model. Any ability of a model to provide causal
inference rests entirely on the faith of the analyst in the experimental design,
completeness of the set of variables that are thought to measure confounding
and are used for adjustment when the experiment is not randomized, lack of
important measurement error, and lastly the goodness of fit of the model.
The first line of attack in interpreting the results of a multivariable analysis
is to interpret the model’s parameter estimates. For simple linear, additive
models, regression coefficients may be readily interpreted. If there are interactions or nonlinear terms in the model, however, simple interpretations
are usually impossible. Many programs ignore this problem, routinely printing such meaningless quantities as the e↵ect of increasing age2 by one day
while holding age constant. A meaningful age change needs to be chosen,
and connections between mathematically related variables must be taken
into account. These problems can be solved by relying on predicted values
and di↵erences between predicted values.
Even when the model contains no nonlinear e↵ects, it is difficult to compare
regression coefficients across predictors having varying scales. Some analysts
like to gauge the relative contributions of di↵erent predictors on a common
scale by multiplying regression coefficients by the standard deviations of the
predictors that pertain to them. This does not make sense for nonnormally
distributed predictors (and regression models should not need to make assumptions about the distributions of predictors). When a predictor is binary

105

106

1

2

3

4

5 Describing, Resampling, Validating, and Simplifying the Model

(e.g., sex), the standard deviation makes no sense as a scaling factor as the
scale would depend on the prevalence of the predictor.a
It is more sensible to estimate the change in Y when Xj is changed by
an amount that is subject-matter relevant. For binary predictors this is a
change from 0 to 1. For many continuous predictors the interquartile range
is a reasonable default choice. If the 0.25 and 0.75 quantiles of Xj are g and
h, linearity holds, and the estimated coefficient of Xj is b; b ⇥ (h g) is the
e↵ect of increasing Xj by h g units, which is a span that contains half of
the sample values of Xj .
For the more general case of continuous predictors that are monotonically
but not linearly related to Y , a useful point summary is the change in X
when the variable changes from its 0.25 quantile to its 0.75 quantile. For
models for which exp(X ) is meaningful, antilogging the predicted change in
X results in quantities such as interquartile-range odds and hazards ratios.
When the variable is involved in interactions, these ratios are estimated separately for various levels of the interacting factors. For categorical predictors,
ordinary e↵ects are computed by comparing each level of the predictor with
a reference level. See Section 10.10 and Chapter 11 for tabular and graphical
examples of this approach.
The model can be described using partial e↵ect plots by plotting each X
against X ˆ holding other predictors constant. Modified versions of such plots,
by nonlinearly rank-transforming the predictor axis, can show the relative
importance of a predictor329 .
For an X that interacts with other factors, separate curves are drawn on
the same graph, one for each level of the interacting factor.
Nomograms39, 249, 332, 420 provide excellent graphical depictions of all the
variables in the model, in addition to enabling the user to obtain predicted
values manually. Nomograms are especially good at helping the user envision
interactions. See Section 10.10 and Chapter 11 for examples.

5.1.2 Indexes of Model Performance
5.1.2.1 Error Measures
Care must be taken in the choice of accuracy scores to be used in validation.
Indexes can be broken down into three main areas.
Central tendency of prediction errors: These measures include mean absolute di↵erences, mean squared di↵erences, and logarithmic scores. An absolute measure is mean |Y Ŷ |. The mean squared error is a commonly
used and sensitive measure if there are no outliers. For the special case
a

The s.d. of a binary variable is approximately
of ones.

p
a(1

a), where a is the proportion

5.1 Describing the Fitted Model

107

where Y is binary, such a measure is the Brier score, which is a quadratic
proper scoring rule that combines calibration and discriminationb . The
logarithmic proper scoring rules (related to average log-likelihood) is even
more sensitive but can be harder to interpret and can be destroyed by a
single predicted probability of 0 or 1 that was incorrect.
Discrimination measures: A measure of pure discrimination is a rank correlation of Ŷ and Y ), including Spearman’s ⇢, Kendall’s ⌧ , and Somers’
Dxy . When Y is binary, Dxy = 2⇥(c 12 ) where c is the concordance probability or area under the receiver operating characteristic curve, a linear
translation of the Wilcoxon-Mann-Whitney statistic. R2 is mostly a mea2
sure of discrimination, and Radj
is is a good overfitting-corrected measure,
if the model is pre-specified. See Section 10.8 for more information about
rank-based measures.
Discrimination measures based on variation in Ŷ : These include the regression sum of squares and the g–Index (see below).
Calibration measures: These assess absolute prediction accuracy. Calibration–
in–the–large compares the average Ŷ with the average Y . A high-resolution
calibration curve or calibration–in–the–small assesses the absolute forecast
accuracy of predictions at individual levels of Ŷ . When the calibration
curve is linear, this can be summarized by the calibration slope and intercept. A more general approach uses the loess nonparametric smoother to
estimate the calibration curve36 . For any shape of calibration curve, errors
can be summarized by quantities such as the maximum absolute calibration error, mean absolute calibration error, and 0.9 quantile of calibration
error.
The g-index is a new measure of a model’s predictive discrimination based
only on X ˆ = Ŷ that applies quite generally. It is based on Gini’s mean
di↵erence for a variable Z, which is the mean over all possible i 6= j of |Zi
Zj |. The g-index is an interpretable, robust, and highly efficient measure of
variation. For example, when predicting systolic blood pressure, g = 11mmHg
represents a typical di↵erence in Ŷ . g is independent of censoring and other
complexities. For models in which the anti-log of a di↵erence in Ŷ represents
meaningful ratios (e.g., odds ratios, hazard ratios, ratio of medians), gr can
be defined as exp(g). For models in which Ŷ can be turned into a probability
estimate (e.g., logistic regression), gp is defined as Gini’s mean di↵erence of
P̂ . These g–indexes represent e.g. “typical” odds ratios, and “typical” risk
di↵erences. Partial g indexes can also be defined. More details may be found
in the documentation for the R rms package’s gIndex function.
b

There are decompositions of the Brier score into discrimination and calibration
components.

108

5 Describing, Resampling, Validating, and Simplifying the Model

5.2 The Bootstrap
When one assumes that a random variable Y has a certain population distribution, one can use simulation or analytic derivations to study how a statistical estimator computed from samples from this distribution behaves. For
example, when Y has a log-normal distribution, the variance of the sample
median for a sample of size n from that distribution can be derived analytically. Alternatively, one can simulate 500 samples of size n from the lognormal distribution, compute the sample median for each sample, and then
compute the sample variance of the 500 sample medians. Either case requires
knowledge of the population distribution function.
Efron’s bootstrap147, 174, 175 is a general-purpose technique for obtaining estimates of the properties of statistical estimators without making assumptions
about the distribution giving rise to the data. Suppose that a random variable
Y comes from a cumulative distribution function F (y) = Prob{Y  y} and
that we have a sample of size n from this unknown distribution, Y1 , Y2 , . . . , Yn .
The basic idea is to repeatedly simulate a sample of size n from F , computing
the statistic of interest, and assessing how the statistic behaves over B repetitions. Not having F at our disposal, we can estimate F by the empirical
cumulative distribution function
n

1X
Fn (y) =
[Yi  y].
n i=1

5

(5.1)

Fn corresponds to a density function that places probability 1/n at each
observed datapoint (k/n if that point were duplicated k times and its value
listed only once).
As an example, consider a random sample of size n = 30 from a normal
distribution with mean 100 and standard deviation 10. Figure 5.1 shows the
population and empirical cumululative distribution functions.
Now pretend that Fn (y) is the original population distribution F (y). Sampling from Fn is equivalent to sampling with replacement from the observed
data Y1 , . . . , Yn . For large n, the expected fraction of original datapoints that
are selected for each bootstrap sample is 1 e 1 = 0.632. Some points are
selected twice, some three times, a few four times, and so on. We take B samples of size n with replacement, with B chosen so that the summary measure
of the individual statistics is nearly as good as taking B = 1. The bootstrap
is based on the fact that the distribution of the observed di↵erences between a
resampled estimate of a parameter of interest and the original estimate of the
parameter from the whole sample tells us about the distribution of unobservable di↵erences between the original estimate and the unknown population
value of the parameter.
As an example, consider the data (1, 5, 6, 7, 8, 9) and suppose that we would
like to obtain a 0.80 confidence interval for the population median, as well as
an estimate of the population expected value of the sample median (the latter

5.2 The Bootstrap

109
1.0

●
●
●
●
●
●

0.8

●
●

Prob[X ≤ x]

●
●
●
●

0.6

●
●
●
●
●
●

0.4

●
●
●
●
●
●
●
●
●
●

0.2
●
●

0.0
60

80

100

120

140

x
Fig. 5.1 Empirical and population cumulative distribution function

is only used to estimate bias in the sample median). The first 20 bootstrap
samples (after sorting data values) and the corresponding sample medians
are shown in Table 5.1.
For a given number B of bootstrap samples, our estimates are simply
the sample 0.1 and 0.9 quantiles of the sample medians, and the mean of
the sample medians. Not knowing how large B should be, we could let B
range from, say, 50 to 1000, stopping when we are sure the estimates have
converged. In the left plot of Figure 5.2, B varies from 1 to 400 for the mean
(10 to 400 for the quantiles). It can be seen that the bootstrap estimate of the
population mean of the sample median can be estimated satisfactorily when
B > 50. For the lower and upper limits of the 0.8 confidence interval for the
population median Y , B must be at least 200. For more extreme confidence
limits, B must be higher still.
For the final set of 400 sample medians, a histogram (right plot in Figure
5.2) can be used to assess the form of the sampling distribution of the sample
median. Here, the distribution is almost normal, although there is a slightly
heavy left tail that comes from the data themselves having a heavy left tail.
For large samples, sample medians are normally distributed for a wide variety of population distributions. Therefore we could use bootstrapping to
estimate the variance of the sample median and then take ±1.28 standard
errors as a 0.80 confidence interval. In other cases (e.g., regression coefficient
estimates for certain models), estimates are asymmetrically distributed, and
the bootstrap quantiles are better estimates than confidence intervals that

110

5 Describing, Resampling, Validating, and Simplifying the Model

8

60

7

Frequency

Mean and 0.1, 0.9 Quantiles

9

6

5

40

20
4

3

0
0

100

200

300

400

2

Bootstrap Samples Used
Fig. 5.2 Estimating properties of sample median using the bootstrap
Table 5.1

Bootstrap Sample
166789
155568
578999
777889
157799
156678
788888
555799
155779
155778
115577
115578
155778
156788
156799
667789
157889
668999
115569
168999

Sample Median
6.5
5.0
8.5
7.5
7.0
6.0
8.0
6.0
6.0
6.0
5.0
5.0
6.0
6.5
6.5
7.0
7.5
8.5
5.0
8.5

4

6

8

5.3 Model Validation

111

are based on a normality assumption. Note that because sample quantiles are
more or less restricted to equal one of the values in the sample, the bootstrap
distribution is discrete and can be dependent on a small number of outliers.
For this reason, bootstrapping quantiles does not work particularly well for
small samples [147, pp. 41–43].
The method just presented for obtaining a nonparametric confidence interval for the population median is called the bootstrap percentile method. It
is the simplest but not necessarily the best performing bootstrap method.
In this text we use the bootstrap primarily for computing statistical estimates that are much di↵erent from standard errors and confidence intervals,
namely, estimates of model performance.

6

5.3 Model Validation
5.3.1 Introduction
The surest method to have a model fit the data at hand is to discard much
of the data. A p-variable fit to p + 1 observations will perfectly predict Y as
long as no two observations have the same Y . Such a model will, however,
yield predictions that appear almost random with respect to responses on
a di↵erent dataset. Therefore, unbiased estimates of predictive accuracy are
essential.
Model validation is done to ascertain whether predicted values from the
model are likely to accurately predict responses on future subjects or subjects not used to develop our model. Three major causes of failure of the
model to validate are overfitting, changes in measurement methods/changes
in definition of categorical variables, and major changes in subject inclusion
criteria.
There are two major modes of model validation, external and internal. The
most stringent external validation involves testing a final model developed in
one country on subjects in another country at another time. This validation
would test whether the data collection instrument was translated into another language properly, whether cultural di↵erences make earlier findings
nonapplicable, and whether secular trends have changed associations or base
rates. Testing a finished model on new subjects from the same geographic
area but from a di↵erent institution as subjects used to fit the model is a
less stringent form of external validation. The least stringent form of external
validation involves using the first m of n observations for model training and
using the remaining n m observations as a test sample. This is very similar to data-splitting (Section 5.3.3). For details about methods for external
validation see the R functions val.prob and val.surv functions in the rms
package.

7

8

112

5 Describing, Resampling, Validating, and Simplifying the Model

Even though external validation is frequently favored by non-statisticians,
it is often problematic. Holding back data from the model-fitting phase results in lower precision and power, and one can increase precision and learn
more about geographic or time di↵erences by fitting a unified model to the
entire subject series including, for example, country or calendar time as a
main e↵ect and/or as an interacting e↵ect. Indeed one could use the following working definition of external validation: validation of a prediction tool
using data that were not available when the tool needed to be completed. An
alternate definition could be taken as the validation of a prediction tool by
an independent research team.
One suggested hierarchy of the quality of various validation methods is as
follows, ordered from worst to best.
1. Attempting several validations (internal or external) and reporting only
the one that “worked”
2. Reporting apparent performance on the training dataset (no validation)
3. Reporting predictive accuracy on an undersized independent test sample
4. Internal validation using data-splitting where at least one of the training
and test samples is not huge and the investigator is not aware of the
arbitrariness of variable selection done on a single sample
5. Strong internal validation using 100 repeats of 10-fold cross-validation or
several hundred bootstrap resamples, repeating all analysis steps involving
Y afresh at each re-sample and the arbitrariness of selected “important
variables” is reported (if variable selection is used)
6. External validation on a large test sample, done by the original research
team
7. Re-analysis by an independent research team using strong internal validation of the original dataset
8. External validation using new test data, done by an independent research
team
9. External validation using new test data generated using di↵erent instruments/technology, done by an independent research team
Internal validation involves fitting and validating the model by carefully
using one series of subjects. One uses the combined dataset in this way to
estimate the likely performance of the final model on new subjects, which
after all is often of most interest. Most of the remainder of Section 5.3 deals
with internal validation.

5.3.2 Which Quantities Should Be Used in Validation?
For ordinary multiple regression models, the R2 index is a good measure
of the model’s predictive ability, especially for the purpose of quantifying
drop-o↵ in predictive ability when applying the model to other datasets.

5.3 Model Validation

113

R2 is biased, however. For example, if one used nine predictors to predict
outcomes of 10 subjects, R2 = 1.0 but the R2 that will be achieved on future
subjects will be close to zero. In this case, dramatic overfitting has occurred.
The adjusted R2 (Equation 4.2) solves this problem, at least when the model
has been completely prespecified and no variables or parameters have been
2
“screened” out of the final model fit. That is, Radj
is only valid when p in its
formula is honest— when it includes all parameters ever examined (formally
or informally, e.g., using graphs or tables) whether these parameters are in
the final model or not.
Quite often we need to validate indexes other than R2 for which adjustments for p have not been created.c We also need to validate models containing “phantom degrees of freedom” that were screened out earlier, formally
or informally. For these purposes, we obtain nearly unbiased estimates of R2
or other indexes using data splitting, cross-validation, or the bootstrap. The
bootstrap provides the most precise estimates.
The g–index is another discrimination measure to validate. But g and R2
measures only one aspect of predictive ability. In general, there are two major aspects of predictive accuracy that need to be assessed. As discussed in
Section 4.5, calibration or reliability is the ability of the model to make unbiased estimates of outcome. Discrimination is the model’s ability to separate
subjects’ outcomes. Validation of the model is recommended even when a
data reduction technique is used. This is a way to ensure that the model was
not overfitted or is otherwise inaccurate.

5.3.3 Data-Splitting
The simplest validation method is one-time data-splitting. Here a dataset is
split into training (model development) and test (model validation) samples
by a random process with or without balancing distributions of the response
and predictor variables in the two samples. In some cases, a chronological
split is used so that the validation is prospective. The model’s calibration
and discrimination are validated in the test set.
In ordinary least squares, calibration may be assessed by, for example,
plotting Y against Ŷ . Discrimination here is assessed by R2 and it is of
interest in comparing R2 in the training sample with that achieved in the
test sample. A drop in R2 indicates overfitting, and the absolute R2 in the
test sample is an unbiased estimate of predictive discrimination. Note that
in extremely overfitted models, R2 in the test set can be negative, since it is
computed on “frozen” intercept and regression coefficients using the formula
c

For example, in the binary logistic model, there is a generalization of R2 available,
but no adjusted version. For logistic models we often validate other indexes such
as the ROC area or rank correlation between predicted probabilities and observed
outcomes. We also validate the calibration accuracy of Ŷ in predicting Y .

114

9

5 Describing, Resampling, Validating, and Simplifying the Model

1 SSE/SST , where SSE is the error sum of squares, SST is the total sum
of squares, and SSE can be greater than SST (when predictions are worse
than the constant predictor Y ).
To be able to validate predictions from the model over an entire test sample (without validating it separately in particular subsets such as in males
and females), the test sample must be large enough to precisely fit a model
containing one predictor. For a study with a continuous uncensored response
variable, the test sample size should ordinarily be 100 at a bare minimum.
For survival time studies, the test sample should at least be large enough
to contain a minimum of 100 outcome events. For binary outcomes, the test
sample should contain a bare minimum of 100 subjects in the least frequent
outcome category. Once the size of the test sample is determined, the remaining portion of the original sample can be used as a training sample. Even with
these test sample sizes, validation of extreme predictions is difficult.
Data-splitting has the advantage of allowing hypothesis tests to be confirmed in the test sample. However, it has the following disadvantages.
1. Data-splitting greatly reduces the sample size for both model development
and model testing. Because of this, Roecker523 found this method “appears
to be a costly approach, both in terms of predictive accuracy of the fitted
model and the precision of our estimate of the accuracy.” Breiman [65,
Section 1.3] found that bootstrap validation on the original sample was as
efficient as having a separate test sample twice as large.
2. It requires a larger sample to be held out than cross-validation (see below) to be able to obtain the same precision of the estimate of predictive
accuracy.
3. The split may be fortuitous; if the process were repeated with a di↵erent
split, di↵erent assessments of predictive accuracy may be obtained.
4. Data-splitting does not validate the final model, but rather a model developed on only a subset of the data. The training and test sets are recombined for fitting the final model, which is not validated.
5. Data-splitting requires the split before the first analysis of the data. With
other methods, analyses can proceed in the usual way on the complete
dataset. Then, after a “final” model is specified, the modeling process is
rerun on multiple resamples from the original data to mimic the process
that produced the “final” model.

5.3.4 Improvements on Data-Splitting: Resampling
Bootstrapping, jackknifing, and other resampling plans can be used to obtain
nearly unbiased estimates of model performance without sacrificing sample
size. These methods work when either the model is completely specified except for the regression coefficients, or all important steps of the modeling
process, especially variable selection, are automated. Only then can each

Requires large sample
held out than crossvalidation

5.3 Model Validation

115

bootstrap replication be a reflection of all sources of variability in modeling.
Note that most analyses involve examination of graphs and testing for lack of
model fit, with many intermediate decisions by the analyst such as simplification of interactions. These processes are difficult to automate. But variable
selection alone is often the greatest source of variability because of multiple
comparison problems, so the analyst must go to great lengths to bootstrap
or jackknife variable selection.
The ability to study the arbitrariness of how a stepwise variable selection
algorithm selects “important” factors is a major benefit of bootstrapping. A
useful display is a matrix of blanks and asterisks, where an asterisk is placed
in column x of row i if variable x is selected in bootstrap sample i (see p.
266 for an example). If many variables appear to be selected at random,
the analyst may want to turn to a data reduction method rather than using
stepwise selection (see also [536]).
Cross-validation is a generalization of data-splitting that solves some of the
problems of data-splitting. Leave-out-one cross-validation,560, 625 the limit of <cross-validation, is similar to jackknifing.669 Here one observation is omitted
from the analytical process and the response for that observation is predicted
using a model derived from the remaining n 1 observations. The process
is repeated n times to obtain an average accuracy. Efron169 reports that
grouped cross-validation is more accurate; here groups of k observations are
omitted at a time. Suppose, for example, that 10 groups are used. The original dataset is divided into 10 equal subsets at random. The first 9 subsets
are used to develop a model (transformation selection, interaction testing,
stepwise variable selection, etc. are all done). The resulting model is assessed
for accuracy on the remaining 1/10th of the sample. This process is repeated
at least 10 times to get an average of 10 indexes such as R2 .
A drawback of cross-validation is the choice of the number of observations
to hold out from each fit. Another is that the number of repetitions needed to
achieve accurate estimates of accuracy often exceeds 200. For example, one
1
may have to omit 10
th of the sample 500 times to accurately estimate the
index of interest Thus the sample would need to be split into tenths 50 times.
Another possible problem is that cross-validation may not fully represent the
variability of variable selection. If 20 subjects are omitted each time from a
sample of size 1000, the lists of variables selected from each training sample
of size 980 are likely to be much more similar than lists obtained from fitting
independent samples of 1000 subjects. Finally, as with data-splitting, crossvalidation does not validate the full 1000-subject model.
An interesting way to study overfitting could be called the randomization
method. Here we ask the question “How well can the response be predicted
when we use our best procedure on random responses when the predictive
accuracy should be near zero?” The better the fit on random Y , the worse the
overfitting. The method takes a random permutation of the response variable
and develops a model with optional variable selection based on the original X
and permuted Y . Suppose this yields R2 = .2 for the fitted sample. Apply the

not great.

10

11

116

12

5 Describing, Resampling, Validating, and Simplifying the Model

fit to the original data to estimate optimism. If overfitting is not a problem,
R2 would be the same for both fits and it will ordinarily be very near zero.

5.3.5 Validation Using the Bootstrap

13

14

Efron,169, 170 Efron and Gong,172 Gong,219 Efron and Tibshirani,174, 175 Linnet,409 and Breiman65 describe several bootstrapping procedures for obtaining nearly unbiased estimates of future model performance without holding
back data when making the final estimates of model parameters. With the
“simple bootstrap” [175, p. 247], one repeatedly fits the model in a bootstrap
sample and evaluates the performance of the model on the original sample.
The estimate of the likely performance of the final model on future data
is estimated by the average of all of the indexes computed on the original
sample.
Efron showed that an enhanced bootstrap estimates future model performance more accurately than the simple bootstrap. Instead of estimating
an accuracy index directly from averaging indexes computed on the original
sample, the enhanced bootstrap uses a slightly more indirect approach by
estimating the bias due to overfitting or the “optimism” in the final model
fit. After the optimism is estimated, it can be subtracted from the index
of accuracy derived from the original sample to obtain a bias-corrected or
overfitting-corrected estimate of predictive accuracy. The bootstrap method
is as follows. From the original X and Y in the sample of size n, draw a
sample with replacement also of size n. Derive a model in the bootstrap sample and apply it without change to the original sample. The accuracy index
from the bootstrap sample minus the index computed on the original sample
is an estimate of optimism. This process is repeated for 100 or so bootstrap
replications to obtain an average optimism, which is subtracted from the final
model fit’s apparent accuracy to obtain the overfitting-corrected estimate.
Note that bootstrapping validates the process that was used to fit the original model (as does cross-validation). It provides an estimate of the expected
value of the optimism, which when subtracted from the original index, provides an estimate of the expected bias-corrected index. If stepwise variable
selection is part of the bootstrap process (as it must be if the final model
is developed that way), and not all resamples (samples with replacement or
training samples in cross-validation) resulted in the same model (which is
almost always the case), this internal validation process actually provides an
unbiased estimate of the future performance of the process used to identify
markers and scoring systems; it does not validate a single final model. But
resampling does tend to provide good estimates of the future performance of
the final model that was selected using the same procedure repeated in the
resamples.

What is the likely future
performance of this
model on similar data?

The way you
operationalize it:
1. Get a bootstrap
sample (will have around
63% of subjects with
doubles/triples due to
resampling)
2. Redevelop the model
3. Get the apparent
performance
R^2(X,B,Y)
4. freeze B and get
performance in the
original sample.
R^2(X_orig, B, Yorig)
For example: get .6 in
the original fit, get .62,
and get .58 in final step bias corrected estimate
= .56
boostrap=super overfit
(minus) regular
overfitting= regular
overfitting-no overfitting

Bootstrap is like the training data, and the
real data is like the validation

Bootstrap estimates the likely future performance of the final model by
estimating the performance of the modeling process.
Bootstrap has highest precision estimator of the future performance of the model!
Precision here means MOE and MSE

validation in separate group - when you are translating a survey (language) or if you have a new type of
technology (proteomics)
5.3 Model Validation

.632 is a mixture of that
and a regular bootstrap.
If you used the wrong
accuracy score you
need to compensate by
doing a more complex
boostrap - Efron's
findings were because
he chose the wrong
accuracy score - if you
don't use a
discontinuous accuracy
score, you don't need
a .632

117

Note that by drawing samples from X and Y , we are estimating aspects
of the unconditional distribution of statistical quantities. One could instead
draw samples from quantities such as residuals from the model to obtain a
distribution that is conditional on X. However, this approach requires that
the model be specified correctly, whereas the unconditional bootstrap does
not. Also, the unconditional estimators are similar to conditional estimators
except for very skewed or very small samples [182, p. 217].
Bootstrapping can be used to estimate the optimism in virtually any index.
Besides discrimination indexes such as R2 , slope and intercept calibration factors can be estimated. When one fits the model C(Y |X) = X , and then refits
the model C(Y |X) = 0 + 1 X ˆ on the same data, where ˆ is an estimate of
, ˆ0 and ˆ1 will necessarily be 0 and 1, respectively. However, when ˆ is used
to predict responses on another dataset, ˆ1 may be < 1 if there is overfitting,
and ˆ0 will be di↵erent from zero to compensate. Thus a bootstrap estimate
of 1 will not only quantify overfitting nicely, but can also be used to shrink
predicted values to make them more calibrated (similar to [578]). Efron’s optimism bootstrap is used to estimate the optimism in (0, 1) and then ( 0 , 1 )
are estimated by subtracting the optimism in the constant estimator (0, 1).
Note that in cross-validation one estimates with ˆ from the training sample
and fits C(Y |X) = X ˆ on the test sample directly. Then the estimates are
averaged over all test samples. This approach does not require the choice of a
parameter that determines the amount of shrinkage as does ridge regression
or penalized maximum likelihood estimation; instead one estimates how to
make the initial fit well calibrated.120, 625 However, this approach is not as
reliable as building shrinkage into the original estimation process. The latter
allows di↵erent parameters to be shrunk by di↵erent factors.
Ordinary bootstrapping can sometimes yield overly optimistic estimates
of optimism, that is, may underestimate the amount of overfitting. This is
especially true when the ratio of the number of observations to the number
of parameters estimated is not large.201 A variation on the bootstrap that
improves precision of the assessment is the “.632” method, which Efron found
to be optimal in several examples.169 This method provides a bias-corrected
estimate of predictive accuracy by substituting 0.632⇥ [apparent accuracy
✏ˆ0 ] for the estimate of optimism, where ✏ˆ0 is a weighted average of accuracies
evaluated on observations omitted from bootstrap samples [175, Eq. 17.25, p.
253].
For ordinary least squares, where the genuine per-observation .632 estimator can be used, several simulations revealed close agreement with the modified .632 estimator, even in small, highly overfitted samples. In these overfitted cases, the ordinary bootstrap bias-corrected accuracy estimates were
significantly higher than the .632 estimates. Simulations254, 586 have shown,
however, that for most types of indexes of accuracy of binary logistic regression models, Efron’s original bootstrap has lower mean squared error than
the .632 bootstrap when n = 200, p = 30. Bootstrap overfitting-corrected estimates of model performance can be biased in favor of the model. Although

15

16

17

118

5 Describing, Resampling, Validating, and Simplifying the Model

Table 5.2

Method

Apparent Rank Over- Bias-Corrected
Correlation of Optimism Correlation
Predicted vs.
Observed
Full Model
0.50
0.06
0.44
Stepwise Model
0.47
0.05
0.42

cross-validation is less biased than the bootstrap, Efron169 showed that it has
much higher variance in estimating overfitting-corrected predictive accuracy
than bootstrapping. In other words, cross-validation, like data-splitting, can
yield significantly di↵erent estimates when the entire validation process is
repeated.
It is frequently very informative to estimate a measure of predictive accuracy forcing all candidate factors into the fit and then to separately estimate
accuracy allowing stepwise variable selection, possibly with di↵erent stopping rules. Consistent with Spiegelhalter’s proposal to use all factors and
then to shrink the coefficients to adjust for overfitting,578 the full model fit
will outperform the stepwise model more often than not. Even though stepwise modeling has slightly less optimism in predictive discrimination, this
improvement is not enough to o↵set the loss of information from deleting
even marginally significant variables. Table 5.2 shows a typical scenario. In
this example, stepwise modeling lost a possible 0.50 0.47 = 0.03 predictive
discrimination. The full model fit will especially be an improvement when
1. the stepwise selection deletes several variables that are almost significant;
2. these marginal variables have some real predictive value, even if it’s slight;
and
3. there is no small set of extremely dominant variables that would be easily
found by stepwise selection.
18

19

Faraway182 has a fascinating study showing how resampling methods can
be used to estimate the distributions of predicted values and of e↵ects of a
predictor, adjusting for an automated multistep modeling process. Bootstrapping can be used, for example, to penalize the variance in predicted values for
choosing a transformation for Y and for outlier and influential observation
deletion, in addition to variable selection. Estimation of the transformation of
Y greatly increased the variance in Faraway’s examples. Brownstone [76, p.
74] states that “In spite of considerable e↵orts, theoretical statisticians have
been unable to analyze the sampling properties of [usual multistep modeling
strategies] under realistic conditions” and concludes that the modeling strategy must be completely specified and then bootstrapped to get consistent
estimates of variances and other sampling properties.

5.4 Bootstrapping Ranks of Predictors

119

5.4 Bootstrapping Ranks of Predictors
When the order of importance of predictors is not pre-specified but the researcher attempts to determine that order by assessing multiple associations
with Y , the process of selecting “winners” and “losers” is unreliable. The
bootstrap can be used to demonstrate the difficulty of this task, by estimating confidence intervals for the ranks of all the predictors. Even though the
bootstrap intervals are wide, they actually underestimate the true widths245 .
The following exampling uses simulated data with known ranks of importance of 12 predictors, using an ordinary linear model. The importance metric
is the partial 2 minus its degrees of freedom, while the true metric is the
partial , as all covariates have U (0, 1) distributions.
# Use the plot method for anova, with pl=FALSE to suppress
# actual plotting of chi-square - d.f. for each bootstrap
# repetition. Rank the negative of the adjusted chi-squares
# so that a rank of 1 is assigned to the highest. It is
# important to tell plot.anova.rms not to sort the results,
# or every bootstrap replication would have ranks of 1,2,3,
# ... for the partial test statistics.
r e q u i r e ( rms )
n
300
set.seed (1)
d
d a t a . f r a m e ( x1=r u n i f ( n ) , x2=r u n i f ( n ) , x3=r u n i f ( n ) ,
x4=r u n i f ( n ) , x5=r u n i f ( n ) , x6=r u n i f ( n ) , x7=r u n i f ( n ) ,
x8=r u n i f ( n ) , x9=r u n i f ( n ) , x10=r u n i f ( n ) , x11=r u n i f ( n ) ,
x12=r u n i f ( n ) )
d$y
with ( d , 1 ⇤ x1 + 2 ⇤ x2 + 3 ⇤ x3 + 4 ⇤ x4 + 5 ⇤ x5 + 6 ⇤ x6 +
7 ⇤ x7 + 8 ⇤ x8 + 9 ⇤ x9 + 10 ⇤ x10 + 11 ⇤ x11 +
12 ⇤ x12 + 9 ⇤ rnorm ( n ) )
f
o l s ( y ⇠ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12 , data=d )
B
1000
ranks
m a t r i x (NA, nrow=B, n c o l =12)
rankvars
function ( f i t )
rank ( p l o t ( anova ( f i t ) , s o r t= ’ none ’ , p l=FALSE ) )
Rank
rankvars ( f )
f o r ( i i n 1 :B) {
j
sample ( 1 : n , n , TRUE)
bootfit
update ( f , data=d , s u b s e t=j )
ranks [ i , ]
rankvars ( b o o t f i t )
}
lim
t ( a p p l y ( ranks , 2 , q u a n t i l e , p r o b s=c ( . 0 2 5 , . 9 7 5 ) ) )
predictor
f a c t o r ( names ( Rank ) , names ( Rank ) )
w
d a t a . f r a m e ( p r e d i c t o r , Rank , l o w e r=l i m [ , 1 ] , upper=l i m [ , 2 ] )
require ( ggplot2 )
g g p l o t (w, a e s ( x=p r e d i c t o r , y=Rank ) ) + g e o m p o i n t ( ) +
c o o r d f l i p ( ) + s c a l e y c o n t i n u o u s ( b r e a k s = 1: 12 ) +
g e o m e r r o r b a r ( a e s ( ymin=l i m [ , 1 ] , ymax=l i m [ , 2 ] ) , width =0)

With a sample size of n = 300 the observed ranks of predictor importance do
not coincide with population s, even when there are no collinearities among

5 Describing, Resampling, Validating, and Simplifying the Model

predictor

120

x12
x11
x10
x9
x8
x7
x6
x5
x4
x3
x2
x1

●
●
●
●
●

ranked partial chi-square
statistics.

●
●
●
●

This is mainly a process to
show that we really shouldn't
be picking the "best"

●
●
●

1

2

3

4

5

6

7

8

9

10 11 12

Rank
Fig. 5.3 Bootstrap percentile 0.95 confidence limits for ranks of predictors in an OLS
model. Ranking is on the basis of partial 2 minus d.f. Point estimates are original
ranks

the predictors. Confidence intervals are wide; for example the 0.95 confidence
interval for the rank of x7 (which has a true rank of 7) is [1, 8], so we are
only confident that x7 is not one of the 4 most influential predictors. The
confidence intervals do include the true ranks in each case.

5.5 Simplifying the Final Model by Approximating It
5.5.1 Difficulties Using Full Models
A model that contains all prespecified terms will usually be the one that predicts the most accurately on new data. It is also a model for which confidence
limits and statistical tests have the claimed properties. Often, however, this
model will not be very parsimonious. The full model may require more predictors than the researchers care to collect in future samples. It also requires
predicted values to be conditional on all of the predictors, which can increase
the variance of the predictions.
As an example suppose that least squares has been used to fit a model
containing several variables including race (with four categories). Race may
be an insignificant predictor and may explain a tiny fraction of the observed

1. create a full model
and pull predictions
for each observation
2. do variable
selection based on
the r^2

here we are working
with a mechanistic
model, so the
residual=0, so stepwise
works better

5.5 Simplifying the Final Model by Approximating It

121

variation in Y . Yet when predictions are requested, a value for race must be
inserted. If the subject is of the majority race, and this race has a majority of,
say 0.75, the variance of the predicted value will not be significantly greater
than the variance for a predicted value from a model that excluded race
for its list of predictors. If, however, the subject is of a minority race (say
“other” with a prevalence of 0.01), the predicted value will have much higher
variance. One approach to this problem, that does not require development
of a second model, is to ignore the subject’s race and to get a weighted
average prediction. That is, we obtain predictions for each of the four races
and weight these predictions by the relative frequencies of the four races.d
This weighted average estimates the expected value of Y unconditional on
race. It has the advantage of having exactly correct confidence limits when
model assumptions are satisfied, because the correct “error term” is being
used (one that deducts 3 d.f. for having ever estimated the race e↵ect). In
regression models having nonlinear link functions, this process does not yield
such a simple interpretation.
When predictors are collinear, their competition results in larger P -values
when predictors are (often inappropriately) tested individually. Likewise, confidence intervals for individual e↵ects will be wide and uninterpretable (can
other variables really be held constant when one is changed?).

5.5.2 Approximating the Full Model
When the full model contains several predictors that do not appreciably affect the predictions, the above process of “unconditioning” is unwieldy. In the
search for a simple solution, the most commonly used procedure for making
the model parsimonious is to remove variables on the basis of P -values, but
this results in a variety of problems as we have seen. Our approach instead
is to consider the full model fit as the “gold standard” model, especially the
model from which formal inferences are made. We then proceed to approximate this full model to any desired degree of accuracy. For any approximate
model we calculate the accuracy with which it approximates the best model.
One goal this process accomplishes is that it provides di↵erent degrees of
parsimony to di↵erent audiences, based on their needs. One investigator may
be able to collect only three variables, another one seven. Each investigator
will know how much she is giving up by using a subset of the predictors.
In approximating the gold standard model it is very important to note that
there is nothing gained in removing certain nonlinear terms; gains in parsimony come only from removing entire predictors. Another accomplishment
d

Using the rms package described in Chapter 6, such estimates and their confidence
limits can easily be obtained, using for example contrast(fit, list(age=50,
disease=’hypertension’, race=levels(race)), type=’average’,
weights=table(race)).

122

5 Describing, Resampling, Validating, and Simplifying the Model

of model approximation is that when the full model has been fitted using
shrinkage (penalized estimation, Section 9.10), the approximate models will
inherit the shrinkage (see Section 14.10 for an example).
Approximating complex models with simpler ones has been used to decode “black boxes” such as artificial neural networks. Recursive partitioning
trees (Section 2.5) may sometimes be used in this context. One develops a
regression tree to predict the predicted value X ˆ on the basis of the unique
variables in X, using R2 , the average absolute prediction error, or the maximum absolute prediction error as a stopping rule, for example180 . The user
desiring simplicity may use the tree to obtain predicted values, using the
first k nodes, with k just large enough to yield a low enough absolute error
in predicting the more comprehensive prediction. Overfitting is not a problem as it is when the tree procedure is used to predict the outcome, because
(1) given the predictor values the predictions are deterministic and (2) the
variable being predicted is a continuous, completely observed variable. Hence
the best cross-validating tree approximation will be one with one subject per
node. One advantage of the tree-approximation procedure is that data collection on an individual subject whose outcome is being predicted may be
abbreviated by measuring only those Xs that are used in the top nodes, until
the prediction is resolved to within a tolerable error.
When principal component regression is being used, trees can also be used
to approximate the components or to make them more interpretable.
Full models may also be approximated using least squares as long as the
linear predictor X ˆ is the target, and not some nonlinear transformation of it
such as a logistic model probability. When the original model was fitted using
unpenalized least squares, submodels fitted against Ŷ will have the same
coefficients as if least squares had been used to fit the subset of predictors
directly against Y . To see this, note that if X denotes the entire design matrix
and T denotes a subset of the columns of X, the coefficient estimates for the
full model are (X 0 X) 1 X 0 Y , Ŷ = X(X 0 X) 1 X 0 Y , estimates for a reduced
model fitted against Y are (T 0 T ) 1 T 0 Y , and coefficients fitted against Ŷ are
(T 0 T )T 0 X(X 0 X) 1 X 0 Y which can be shown to equal (T 0 T ) 1 T 0 Y .
When least squares is used for both the full and reduced models, the
variance–covariance matrix of the coefficient estimates of the reduced model
is (T 0 T ) 1 2 , where the residual variance 2 is estimated using the full model.
When 2 is estimated by the unbiased estimator using the d.f. from the
full model, which provides the only unbiased estimate of 2 , the estimated
variance–covariance matrix of the reduced model will be appropriate (unlike
that from stepwise variable selection) although the bootstrap may be needed
to fully take into account the source of variation due to how the approximate
model was selected.
So if in the least squares case the approximate model coefficients are identical to coefficients obtained upon fitting the reduced model against Y , how
is model approximation any di↵erent from stepwise variable selection? There
are several di↵erences, in addition to how 2 is estimated.

5.6 Further Reading

123

1. When the full model is approximated by a backward step-down procedure
against Ŷ , the stopping rule is less arbitrary. One stops deleting variables
when deleting any further variable would make the approximation inadequate (e.g., the R2 for predictions from the reduced model against the
original Ŷ drops below 0.95).
2. Because the stopping rule is di↵erent (i.e., is not based on P -values), the
approximate model will have a di↵erent number of predictors than an
ordinary stepwise model.
3. If the original model used penalization, approximate models will inherit
the amount of shrinkage used in the full fit.
Typically, though, if one performed ordinary backward step-down against Y
using a large cuto↵ for ↵ (e.g., 0.5), the approximate model would be very
similar to the step-down model. The main di↵erence would be the use of a
larger estimate of 2 and smaller error d.f. than are used for the ordinary
step-down approach (an estimate that pretended the final reduced model was
prespecified).
When the full model was not fitted using least squares, least squares can
still easily be used to approximate the full model. If the coefficient estimates
from the full model are ˆ, estimates from the approximate model are matrix contrasts of ˆ, namely, W ˆ, where W = (T 0 T ) 1 T 0 X. So the variance–
covariance matrix of the reduced coefficient estimates is given by
W V W 0,

(5.2)

where V is the variance matrix for ˆ. See Section 19.5 for an example. Ambler
et al.21 studied model simplification using simulation studies based on several
clinical datasets, and compared it with ordinary backward stepdown variable
selection and with shrinkage methods such as the lasso (see Section 4.3). They
found that ordinary backwards variable selection can be competitive when
there is a large fraction of truly irrelevant predictors (something that can be
difficult to know in advance). Paul et al.478 found advantages to modeling
the response with a complex but reliable approach, and then developing a
parsimoneous model using the lasso or stepwise variable selection against Ŷ .
See Section 11.7 for a case study in model approximation.

5.6 Further Reading
1

Gelman209 argues that continuous variables should be scaled by two standard
deviations to make them comparable to binary predictors. However his approach
assumes linearity in the predictor e↵ect and assumes the prevalence of the binary
predictor is near 0.5. John Fox [198, p. 95] points out that if two predictors are
on the same scale and have the same impact (e.g., years of employment and
years of education), standardizing the coefficients will make them appear to
have di↵erent impacts.

The 3 over n rule - when
you have 0/n, your upper
confidence bound is 3/n

124
2
3

4

5

6

7

8

9
10

11

5 Describing, Resampling, Validating, and Simplifying the Model
Levine et al.394 have a compelling argument for graphing e↵ect ratios on a
logarithmic scale.
Hankins249 is a definitive reference on nomograms and has multi-axis examples
of historical significance. According to Hankins, Maurice d’Ocagne could be
called the inventor of the nomogram, starting with alignment diagrams in 1884
and declaring a new science of “nomography” in 1899. d’Ocagne was at École
des Ponts et Chaussées, a French civil engineering school. Julien and Hanley321
have a nice example of adding axes to a nomogram to estimate the absolute
e↵ect of a treatment estimated using a Cox proportional hazards model. Kattan
and Marasco332 have several clinical examples and explain advantages to the
user of nomograms over “black box” computerized prediction.
Graham and Clavel226 discuss graphical and tabular ways of obtaining risk
estimates. van Gorp et al.622 have a nice example of a score chart for manually
obtaining estimates.
Booth and Sarkar60 have a nice analysis of the number of bootstrap resamples
needed to guarantee with 0.95 confidence that a variance estimate has a sufficiently small relative error. They concentrate on the Monte Carlo simulation
error, showing that small errors in variance estimates can lead to important
di↵erences in P -values. Canty et al.88 provide a number of diagnostics to check
the reliability of bootstrap calculations.
There are many variations on the basic bootstrap for computing confidence
limits.147, 175 See Booth and Sarkar60 for useful information about choosing
the number of resamples. They report the number of resamples necessary to
not appreciably change P -values, for example. Booth and Sarkar propose a
more conservative number of resamples than others use (e.g., 800 resamples)
for estimating variances. Carpenter and Bithell89 have an excellent overview of
bootstrap confidence intervals, with practical guidance. They also have a good
discussion of the unconditional nonparametric bootstrap versus the conditional
semiparametric bootstrap.
Altman and Royston18 have a good general discussion of what it means to
validate a predictive model, including issues related to study design and consideration of uses to which the model will be put.
An excellent paper on external validation and generalizability is Justice et al.322 .
Bleeker et al.57 provide an example where internal validation is misleading when
compared with a true external validation done using subjects from di↵erent centers in a di↵erent time period. Vergouwe et al.630 give good guidance about the
number of events needed in sample used for external validation of binary logistic
models.
See Picard and Berk498 for more about data-splitting.
In the context of variable selection where one attempts to select the set of variables with nonzero true regression coefficients in an ordinary regression model,
Shao560 demonstrated that leave-out-one cross-validation selects models that
are “too large.” Shao also showed that the number of observations held back for
validation should often be larger than the number used to train the model. This
is because in this case one is not interested in an accurate model (you fit the
whole sample to do that), but an accurate estimate of prediction error is mandatory so as to know which variables to allow into the final model. Shao suggests
using a cross-validation strategy in which approximately n3/4 observations are
used in each training sample and the remaining observations are used in the
test sample. A repeated balanced or Monte Carlo splitting approach is used,
and accuracy estimates are averaged over 2n (for the Monte Carlo method)
repeated splits.
Picard and Cook’s Monte Carlo cross-validation procedure499 is an improvement over ordinary cross-validation.

5.6 Further Reading
12

13

14
15

16

125

The randomization method is related to Kipnis’ “chaotization relevancy principle”341 in which one chooses between two models by measuring how far each is
from a nonsense model. Tibshirani and Knight also use a randomization method
for estimating the optimism in a model fit.604
This method used here is a slight change over that presented in [169], where
Efron wrote predictive accuracy as a sum of per-observation components (such
as 1 if the observation is classified correctly, 0 otherwise). Here we are writing
m ⇥ the unitless summary index of predictive accuracy in the place of Efron’s
sum of m per-observation accuracies [409, p. 613].
See [625] and [65, Section 4] for insight on the meaning of expected optimism.
See Copas,120 van Houwelingen and le Cessie [625, p. 1318], Verweij and van
Houwelingen,632 and others623 for other methods of estimating shrinkage coefficients.
Efron169 developed the “.632” estimator only for the case where the index being
bootstrapped is estimated on a per-observation basis. A natural generalization
of this method can be derived by assuming that the accuracy evaluated on
observation i that is omitted from a bootstrap sample has the same expectation
as the accuracy of any other observation that would be omitted from the sample.
The modified estimate of ✏0 is then given by
✏ˆ0 =

B
X

wi Ti ,

(5.3)

i=1

where Ti is the accuracy estimate derived from fitting a model on the ith bootstrap sample and evaluating it on the observations omitted from that bootstrap
sample, and wi are weights derived for the B bootstrap samples:
wi =

17

18

19

n
1 X [bootstrap sample i omits observation j]
.
n j=1 #bootstrap samples omitting observation j

(5.4)

Note that ✏ˆ0 is undefined if any observation is included in every bootstrap
sample. Increasing B will avoid this problem. This modified “.632” estimator
is easy to compute if one assembles the bootstrap sample assignments and
computes the wi before computing the accuracy indexes Ti . For large n, the wi
approach 1/B and so ✏ˆ0 becomes equivalent to the accuracy computed on the
observations not contained in the bootstrap sample and then averaged over the
B repetitions.
Efron and Tibshirani176 have reduced the bias of the “.632” estimator further
with only a modest increase in its variance. Simulation has, however, shown no
advantage of this “.632+” method over the basic optimism bootstrap for most
accuracy indexes used in logistic models.
van Houwelingen and le Cessie625 have several interesting developments in
model validation. See Breiman65 for a discussion of the choice of X for which
to validate predictions. Steyerberg et al.583 present simulations showing the
number of bootstrap samples needed to obtain stable estimates of optimism of
various accuracy measures. They demonstrate that bootstrap estimates of optimism are nearly unbiased when compared with simulated external estimates.
They also discuss problems with precision of estimates of accuracy, especially
when using external validation on small samples.
Blettner and Sauerbrei also demonstrate the variability caused by data-driven
analytic decisions.58 Chatfield97 has more results on the e↵ects of using the
data to select the model.

126

5 Describing, Resampling, Validating, and Simplifying the Model

5.7 Problem
Perform a simulation study to understand the performance of various internal
validation methods for binary logistic models. Modify the R code below in at
least two meaningful ways with regard to covariate distribution or number,
sample size, true regression coefficients, number of resamples, or number of
times certain strategies are averaged. Interpret your findings and give recommendations for best practice for the type of configuration you studied. The
R code from this assignment may be downloaded from the RMS course wiki
page.
For each of 200 simulations, the code below generates a training sample
of 200 observations with p predictors (p = 15 or 30) and a binary reponse.
The predictors are independently U ( 0.5, 0.5). The response is sampled so
as to follow a logistic model where the intercept is zero and all regression
coefficients equal 0.5. The “gold standard” is the predictive ability of the
fitted model on a test sample containing 50,000 observations generated from
the same population model. For each of the 200 simulations, several validation
methods are employed to estimate how the training sample model predicts
responses in the 50,000 observations. These validation methods involve fitting
40 or 200 models in resamples.
g-fold cross-validation is done using the command validate(f, method=’cross’,
B=4 or B=10) using the rms package. This was repeated and averaged using
an extra loop, shown below.
For bootstrap methods, validate(f, method=’boot’ or ’.632’, B=40
or B=200) was used. method=’.632’ does Efron’s “.632” method176 , labeled
632a in the output. An ad-hoc modification of the .632 method, 632b was
also done. Here a “bias-corrected” index of accuracy is simply the index evaluated in the observation omitted from the bootstrap resample. The “gold
standard” external validations were obtained from the val.prob function in
the rms package. The following indexes of predictive accuracy are used:
Dxy : Somers’ rank correlation between predicted probability that Y = 1
vs. the binary Y values. This equals 2(C 0.5) where C is the “ROC Area”
or concordance probability.
D: Discrimination index — likelihood ratio 2 divided by the sample size
U : Unreliability index — unitless index of how far the logit calibration
curve intercept and slope are from (0, 1)
Q: Logarithmic accuracy score — a scaled version of the log-likelihood
achieved by the predictive model
Intercept: Calibration intercept on logit scale
Slope: Calibration slope (slope of predicted log odds vs. true log odds)
Accuracy of the various resampling procedures may be estimated by computing the mean absolute errors and the root mean squared errors of estimates (e.g., of Dxy from the bootstrap on the 200 observations) against the

5.7 Problem

127

“gold standard” (e.g., Dxy for the fitted 200-observation model achieved in
the 50,000 observations).
r e q u i r e ( rms )
s e t . s e e d ( 1 ) # so can reproduce results
n
200
# Size of training sample
reps
200
# Simulations
npop
50000
# Size of validation gold standard sample
methods
c ( ’ Boot 40 ’ , ’ Boot 200 ’ , ’ 632 a 40 ’ , ’ 632 a 200 ’ ,
’ 632 b 40 ’ , ’ 632 b 200 ’ , ’ 10 f o l d x 4 ’ , ’ 4 f o l d x 10 ’ ,
’ 10 f o l d x 20 ’ , ’ 4 f o l d x 50 ’ )
R
e x p a n d . g r i d ( sim
= 1 : reps ,
p
= c (15 ,30) ,
method = methods )
R$Dxy
R$ I n t e r c e p t
R$ S l o p e
R$D
R$U
R$Q
R$ repmeth
R$B
NA
R$n
n
## Function to do r overall reps of B resamples, averaging to
## get estimates similar to as if r⇤B resamples were done
val
f u n c t i o n ( f i t , method , B, r ) {
contains
f u n c t i o n (m) l e n g t h ( g r e p (m, method ) ) > 0
meth
i f ( c o n t a i n s ( ’ Boot ’ ) ) ’ boot ’ e l s e
i f ( contains ( ’ fold ’ )) ’ crossvalidation ’ else
i f ( c o n t a i n s ( ’ 632 ’ ) ) ’ . 6 3 2 ’
z
0
for ( i in 1: r ) z
z + v a l i d a t e ( f i t , method=meth , B=B ) [
c ( ”Dxy” , ” I n t e r c e p t ” , ” S l o p e ” , ”D” , ”U” , ”Q” ) ,
’ index.corrected ’ ]
z/r
}
for (p in c (15 , 30)) {
## For each p create the true betas, the design matrix,
## and realizations of binary y in the gold standard
## large sample
Beta
r e p ( . 5 , p ) # True betas
X
m a t r i x ( r u n i f ( npop ⇤p ) , nrow=npop )
0 .5
LX
matxv (X, Beta )
Y
i f e l s e ( r u n i f ( npop )  p l o g i s (LX) , 1 , 0 )
## For each simulation create the data matrix and
## realizations of y
for ( j in 1: reps ) {
## Make training sample
x
m a t r i x ( r u n i f ( n⇤p ) , nrow=n )
0 .5
L
matxv ( x , Beta )
y
i f e l s e ( r u n i f ( n )  p l o g i s (L ) , 1 , 0 )
f
lrm ( y ⇠ x , x=TRUE, y=TRUE)
beta
f $ coef

128

5 Describing, Resampling, Validating, and Simplifying the Model
forecast
matxv (X, b e t a )
## Validate in population
v
v a l . p r o b ( l o g i t=f o r e c a s t , y=Y, p l=FALSE ) [
c ( ”Dxy” , ” I n t e r c e p t ” , ” S l o p e ” , ”D” , ”U” , ”Q” ) ]
f o r ( method i n methods ) {
repmeth
1
i f ( method %i n% c ( ’ Boot 40 ’ , ’ 632 a 40 ’ , ’ 632 b 40 ’ ) )
B
40
i f ( method %i n% c ( ’ Boot 200 ’ , ’ 632 a 200 ’ , ’ 632 b 200 ’ ) )
B
200
i f ( method == ’ 10 f o l d x 4 ’ ) {
B
10
repmeth
4
}
i f ( method == ’ 4 f o l d x 10 ’ ) {
B
4
repmeth
10
}
i f ( method == ’ 10 f o l d x 20 ’ ) {
B
10
repmeth
20
}
i f ( method == ’ 4 f o l d x 50 ’ ) {
B
4
repmeth
50
}

z
v a l ( f , method , B, repmeth )
k
which (R$ sim == j & R$p == p & R$ method == method )
i f ( l e n g t h ( k ) != 1 ) s t o p ( ’ program l o g i c e r r o r ’ )
R[ k , names ( z ) ]
z
v
R[ k , c ( ’B ’ , ’ repmeth ’ ) ]
c (B=B, repmeth=repmeth )
} # end over methods
} # end over reps
} # end over p

Results are best summarized in a multi-way dot chart. Bootstrap nonparametric percentile 0.95 confidence limits are included.
statnames
names (R ) [ 6 : 1 1 ]
w
r e s h a p e (R, d i r e c t i o n= ’ l o n g ’ , v a r y i n g= l i s t ( s t a t n a m e s ) ,
v.names= ’ x ’ , t i m e v a r= ’ s t a t ’ , t i m e s=s t a t n a m e s )
w$p
p a s t e ( ’ p ’ , w$p , s e p= ’= ’ )
require ( lattice )
s
with (w, summarize ( abs ( x ) , l l i s t ( p , method , s t a t ) ,
s m e a n . c l . b o o t , s t a t . n a m e= ’ mae ’ ) )
D o t p l o t ( method ⇠ Cbind ( mae , Lower , Upper ) | s t a t ⇤p , data=s ,
x l a b= ’ Mean | e r r o r | ’ )
s
with (w, summarize ( x^ 2 , l l i s t ( p , method , s t a t ) ,
s m e a n . c l . b o o t , s t a t . n a m e= ’ mse ’ ) )
D o t p l o t ( method ⇠ Cbind ( s q r t ( mse ) , s q r t ( Lower ) , s q r t ( Upper ) ) |
s t a t ⇤p , data=s ,
x l a b=e x p r e s s i o n ( s q r t (MSE) ) )

Chapter 6
R Software

The methods described in this book are useful in any regression model that
involves a linear combination of regression parameters. The software that is
described below is useful in the same situations. Functions in R514 allow interaction spline functions as well as a wide variety of predictor parameterizations
for any regression function, and facilitate model validation by resampling.
R is the most comprehensive tool for general regression models for the
following reasons.
1. It is very easy to write R functions for new models, so R has implemented
a wide variety of modern regression models.
2. Designs can be generated for any model. There is no need to find out
whether the particular modeling function handles what SAS calls “class”
variables—dummy variables are generated automatically when an R category,
factor, ordered, or character variable is analyzed.
3. A single R object can contain all information needed to test hypotheses
and to obtain predicted values for new data.
4. R has superior graphics.
5. Classes in R make possible the use of generic function names (e.g., predict,
summary, anova) to examine fits from a large set of specific model–fitting
functions.
S43, 513, 627 is a high-level object-oriented language for statistical analysis with over three thousand packages and tens of thousands of functions
available. The R system311, 514 is the basis for R software used in this text,
centered around the Regression Modeling Strategies (rms) package256 . See
the Appendix and the Web site for more information about software implementations.

129

1

6 R Software

130

6.1 The R Modeling Language

2

R has a battery of functions that make up a statistical modeling language.93
At the heart of the modeling functions is an R formula of the form
r e s p o n s e ⇠ terms

The terms represent additive components of a general linear model. Although
variables and functions of variables make up the terms, the formula refers to
additive combinations; for example, when terms is age + blood.pressure,
it refers to 1 ⇥ age + 2 ⇥ blood.pressure. Some examples of formulas are
below.
# age + sex main effects
# add second-order interaction
# second-order interaction +
# all main effects
y ⇠ ( age + s e x + p r e s s u r e ) ^ 2
# age+sex+pressure+age:sex+age:pressure...
y ⇠ ( age + s e x + p r e s s u r e ) ^ 2
sex : pressure
# all main effects and all 2nd order
# interactions except sex:pressure
y ⇠ ( age + r a c e ) ⇤ s e x
# age+race+sex+age:sex+race:sex
y ⇠ t r e a t m e n t ⇤ ( age ⇤ r a c e + age ⇤ s e x )
# no interact. with race,sex
s q r t ( y ) ⇠ s e x ⇤ s q r t ( age ) + r a c e
# functions, with dummy variables generated if
# race is an R factor (classification) variable
y ⇠ s e x + p o l y ( age , 2 ) # poly makes orthogonal polynomials
race.sex
i n t e r a c t i o n ( race , sex )
y ⇠ age + r a c e . s e x
# if desire dummy variables for all
# combinations of the factors
y ⇠ age + s e x
y ⇠ age + s e x + age : s e x
y ⇠ age ⇤ s e x

The formula for a regression model is given to a modeling function; for example,
lrm ( y ⇠ r c s ( x , 4 ) )

is read “use a logistic regression model to model y as a function of x, representing x by a restricted cubic spline with four default knots.”a You can use
the R function update to refit a model with changes to the model terms or
the data used to fit it:
f
f2
f3
f4
f5
a

lrm ( y ⇠
update ( f
update ( f
update ( f
update ( f

r c s ( x , 4 ) + x2 + x3 )
, s u b s e t=s e x==” male ” )
, .⇠. x2 )
# remove x2 from model
, .⇠. + r c s ( x5 , 5 ) ) # add rcs(x5,5) to model
, y2 ⇠ . )
# same terms, new response var.

lrm and rcs are in the rms package.

6.2 User-Contributed Functions

131

6.2 User-Contributed Functions
In addition to the many functions that are packaged with S, a wide variety of user-contributed functions is available on the Internet (see the Appendix or Web site for addresses). Two packages of functions used extensively
in this text are Hmisc20 and rms written by the author. The Hmisc package contains miscellaneous functions such as varclus, spearman2, transcan,
hoeffd, rcspline.eval, impute, cut2, describe, sas.get, latex, and several power and sample size calculation functions. The varclus function uses
the R hclust hierarchical clustering function to do variable clustering, and
the R plclust function to draw dendrograms depicting the clusters. varclus
o↵ers a choice of three similarity measures (Pearson r2 , Spearman ⇢2 , and
Hoe↵ding D) and uses pairwise deletion of missing values. varclus automatically generates a series of dummy variables for categorical factors. The Hmisc
hoeffd function computes a matrix of Hoe↵ding Ds for a series of variables.
The spearman2 function will do Wilcoxon, Spearman, and Kruskal–Wallis
tests and generalizes Spearman’s ⇢ to detect non-monotonic relationships.
Hmisc’s transcan function (see Section 4.7) performs a similar function
to PROC PRINQUAL in SAS—it uses restricted splines, dummy variables, and
canonical variates to transform each of a series of variables while imputing
missing values. An option to shrink regression coefficients for the imputation
models avoids overfitting for small samples or a large number of predictors.
transcan can also do multiple imputation and adjust variance–covariance
matrices for imputation. See Chapter 8 for an example of using these functions
for data reduction.
See the Web site for a list of R functions for correspondence analysis,
principal component analysis, and missing data imputation available from
other users. Venables and Ripley [627, Chapter 11] provide a nice description
of the multivariate methods that are available in R, and they provide several
new multivariate analysis functions.
A basic function in Hmisc is the rcspline.eval function, which creates a
design matrix for a restricted (natural) cubic spline using the truncated power
basis. Knot locations are optionally estimated using methods described in
Section 2.4.6, and two types of normalizations to reduce numerical problems
are supported. You can optionally obtain the design matrix for the antiderivative of the spline function. The rcspline.restate function computes
the coefficients (after un-normalizing if needed) that translate the restricted
cubic spline function to unrestricted form (Equation 2.27). rcspline.restate
also outputs LATEX and R representations of spline functions in simplified
form.

132

6 R Software

6.3 The rms Package
A package of R functions called rms contains several functions that extend R
to make the analyses described in this book easy to do. A central function in
rms is datadist, which computes statistical summaries of predictors to automate estimation and plotting of e↵ects. datadist exists as a separate function so that the candidate predictors may be summarized once, thus saving
time when fitting several models using subsets or di↵erent transformations
of predictors. If datadist is called before model fitting, the distributional
summaries are stored with the fit so that the fit is self-contained with respect
to later estimation. Alternatively, datadist may be called after the fit to create temporary summaries to use as plot ranges and e↵ect intervals, or these
ranges may be specified explicitly to Predict and summary.rms (see below),
without ever calling datadist. The input to datadist may be a data frame,
a list of individual predictors, a frame number, or a combination of the first
two.
The characteristics saved by datadist include the overall range and certain quantiles for continuous variables, and the distinct values for discrete
variables (i.e., R factor variables or variables with 10 or fewer unique
values). The quantiles and set of distinct values facilitate estimation and
plotting, as described later. When a function of a predictor is used (e.g.,
pol(pmin(x,50),2)), the limits saved apply to the innermost variable (here,
x). When a plot is requested for how x relates to the response, the plot will
have x on the x-axis, not pmin(x,50). The way that defaults are computed
can be controlled by the q.effect and q.display parameters to datadist.
By default, continuous variables are plotted with ranges determined by the
tenth smallest and tenth largest values occurring in the data (if n < 200, the
0.05 and 0.95 quantiles are used). The default range for estimating e↵ects
such as odds and hazard ratios is the lower and upper quartiles. When a
predictor is adjusted to a constant so that the e↵ects of changes in other predictors can be studied, the default constant used is the median for continuous
predictors and the most frequent category for factor variables. The R system
option datadist is used to point to the result returned by the datadist
function. See the help files for datadist for more information.
rms fitting functions save detailed information for later prediction, plotting, and testing. rms also allows for special restricted interactions and
sets the default method of generating contrasts for categorical variables to
"contr.treatment", the traditional dummy-variable approach.
rms has a special operator %ia% in the terms of a formula that allows for
restricted interactions. For example, one may specify a model that contains
sex and a five-knot linear spline for age, but restrict the age ⇥ sex interaction
to be linear in age. To be able to connect this incomplete interaction with the
main e↵ects for later hypothesis testing and estimation, the following formula
would be given:

6.3 The rms Package

133

Table 6.1 rms Fitting Functions

Function
ols
lrm
orm
psm
cph
bj
Glm
Gls
Rq

Purpose
Ordinary least squares linear model
Binary and ordinal logistic regression model
Has options for penalized MLE
Ordinal semi-parametric regression model for
continuous Y and several link functions
Accelerated failure time parametric survival
models
Cox proportional hazards regression
Buckley–James censored least squares model
General linear models
Generalized least squares
Quantile regression

Related R
Functions
lm
glm
polr,lrm
survreg
coxph
survreg,lm
glm
gls
rq

y ⇠ s e x + l s p ( age , c ( 2 0 , 3 0 , 4 0 , 5 0 , 6 0 ) ) +
s e x %i a% l s p ( age , c ( 2 0 , 3 0 , 4 0 , 5 0 , 6 0 ) )

The following expression would restrict the age ⇥ cholesterol interaction to
be of the form AF (B) + BG(A) by removing doubly nonlinear terms.
y ⇠ l s p ( age , 3 0 ) + r c s ( c h o l e s t e r o l , 4 ) +
l s p ( age , 3 0 ) %i a% r c s ( c h o l e s t e r o l , 4 )
rms has special fitting functions that facilitate many of the procedures de-

scribed in this book, shown in Table 6.1.
Glm is a slight modification of the built-in R glm function so that rms
methods can be run on the resulting fit object. glm fits general linear models under a wide variety of distributions of Y . Gls is a modification of the
gls function from the nlme package of Pinheiro and Bates502 , for repeated
measures (longitudinal) and spatially correlated data. The Rq function is a
modification of the quantreg package’s rq function349, 350 . Functions related
to survival analysis make heavy use of Therneau’s survival package475 .
You may want to specify to the fitting functions an option for how missing values (NAs) are handled. The method for handling missing data in R
is to specify an na.action function. Some possible na.actions are given in
Table 6.2. The default na.action is na.delete when you use rms’s fitting
functions. An easy way to specify a new default na.action is, for example,
o p t i o n s ( n a . a c t i o n=” n a . o m i t ” ) # don’t report frequency of NAs

before using a fitting function. If you use na.delete you can also use the
system option na.detail.response that makes model fits store information
about Y stratified by whether each X is missing. The default descriptive
statistics for Y are the sample size and mean. For a survival time response

6 R Software

134
Table 6.2 Some na.actions Used in rms

Function Name
na.fail
na.omit
na.delete

Method Used
Stop with error message if any missing
values present
Function to remove observations with
any predictors or responses missing
Modified version of na.omit to also
report on frequency of NAs for each
variable

object the sample size and proportion of events are used. Other summary
functions can be specified using the na.fun.response option.
o p t i o n s ( n a . a c t i o n=” n a . d e l e t e ” , n a . d e t a i l . r e s p o n s e=TRUE,
n a . f u n . r e s p o n s e=” m y s t a t s ” )
# Just use na.fun.response="quantile" if don’t care about n
mystats
function (y) {
z
q u a n t i l e ( y , na.rm=T)
n
sum ( ! i s . n a ( y ) )
c (N=n , z )
# elements named N, 0%, 25%, etc.
}

When R deletes missing values during the model–fitting procedure, residuals, fitted values, and other quantities stored with the fit will not correspond
row-for-row with observations in the original data frame (which retained NAs).
This is problematic when, for example, age in the dataset is plotted against
the residual from the fitted model. Fortunately, for many na.actions including na.delete and a modified version of na.omit, a class of R functions
called naresid written by Therneau works behind the scenes to put NAs back
into residuals, predicted values, and other quantities when the predict or
residuals functions (see below) are used. Thus for some of the na.actions,
predicted values and residuals will automatically be arranged to match the
original data.
Any R function can be used in the terms for formulas given to the fitting function, but if the function represents a transformation that has datadependent parameters (such as the standard R functions poly or ns), R will
not in general be able to compute predicted values correctly for new observations. For example, the function ns that automatically selects knots for a
B-spline fit will not be conducive to obtaining predicted values if the knots
are kept “secret.” For this reason, a set of functions that keep track of transformation parameters, exists in rms for use with the functions highlighted
in this book. These are shown in Table 6.3. Of these functions, asis, catg,
scored, and matrx are almost always called implicitly and are not mentioned
by the user. catg is usually called explicitly when the variable is a numeric
variable to be used as a polytomous factor, and it has not been converted to
an R categorical variable using the factor function.

6.3 The rms Package

135

Table 6.3 rms Transformation Functions

Function
asis
rcs
pol
lsp
catg
scored
matrx
strat

Purpose
No post-transformation (seldom used explicitly)
Restricted cubic spline
Polynomial using standard notation
Linear spline
Categorical predictor (seldom)
Ordinal categorical variables
Keep variables as group for anova and fastbw
Nonmodeled stratification factors
(used for cph only)

Related R
Functions
I
ns
poly
factor
ordered
matrix
strata

These functions can be used with any function of a predictor. For example,
to obtain a four-knot cubic spline expansion of the cube root of x, specify
rcs(x^ (1/3),4).
When the transformation functions are called, they are usually given one
or two arguments, such as rcs(x,5). The first argument is the predictor
variable or some function of it. The second argument is an optional vector of
parameters describing a transformation, for example location or number of
knots. Other arguments may be provided.
The Hmisc package’s cut2 function is sometimes used to create a categorical variable from a continuous variable x. You can specify the actual interval
endpoints (cuts), the number of observations to have in each interval on
the average (m), or the number of quantile groups (g). Use, for example,
cuts=c(0,1,2) to cut into the intervals [0, 1), [1, 2].
A key concept in fitting models in R is that the fitting function returns an
object that is an R list. This object contains basic information about the fit
(e.g., regression coefficient estimates and covariance matrix, model 2 ) as well
as information about how each parameter of the model relates to each factor
in the model. Components of the fit object are addressed by, for example,
fit$coef, fit$var, fit$loglik. rms causes the following information to
also be retained in the fit object: the limits for plotting and estimating e↵ects
for each factor (if options(datadist="name") was in e↵ect), the label for
each factor, and a vector of values indicating which parameters associated
with a factor are nonlinear (if any). Thus the “fit object” contains all the
information needed to get predicted values, plots, odds or hazard ratios, and
hypothesis tests, and to do “smart” variable selection that keeps parameters
together when they are all associated with the same predictor.
R uses the notion of the class of an object. The object-oriented class idea
allows one to write a few generic functions that decide which specific functions to call based on the class of the object passed to the generic function.
An example is the function for printing the main results of a logistic model.

136

6 R Software

The lrm function returns a fit object of class "lrm". If you specify the R command print(fit) (or just fit if using R interactively—this invokes print),
the print function invokes the print.lrm function to do the actual printing
specific to logistic models. To find out which particular methods are implemented for a given generic function, type methods(generic.name).
Generic functions that are used in this book include those in Table 6.4.
Table 6.4 rms Package and R Generic Functions
Function
print
coef
formula
specs
vcov
logLik
AIC
lrtest
univarLR
robcov
bootcov

Purpose
Related Functions
Print parameters and statistics of fit
Fitted regression coefficients
Formula used in the fit
Detailed specifications of fit
Fetch covariance matrix
Fetch maximized log-likelihood
Fetch AIC
Likelihood ratio test for two nested models
Compute all univariable LR 2
Robust covariance matrix estimates
Bootstrap covariance matrix estimates
and bootstrap distributions of estimates
pentrace
Find optimum penalty factors by tracing
e↵ective AIC for a grid of penalties
effective.df
Print e↵ective d.f. for each type of variable
in model, for penalized fit or pentrace result
summary
Summary of e↵ects of predictors
plot.summary
Plot continuously shaded confidence bars
for results of summary
anova
Wald tests of most meaningful hypotheses
plot.anova
Graphical depiction of anova
contrast
General contrasts, C.L., tests
Predict
Predicted values and confidence limits easily
varying a subset of predictors and leaving the
rest set at default values
plot.Predict
Plot the result of Predict using
ggplot.Predict Plot the result of Predict using
bplot
3-dimensional plot when Predict varied
two continuous predictors over a fine grid
gendata
Easily generate predictor combinations
predict
Obtain predicted values or design matrix
fastbw
Fast backward step-down variable selection
step
residuals
(or resid) Residuals, influence stats from fit
sensuc
Sensitivity analysis for unmeasured
confounder
which.influence Which observations are overly influential
residuals
latex
LATEX representation of fitted model
Function
Function
R function analytic representation of X ˆ
latex
continued on next page

6.3 The rms Package

137
continued from previous page

Function
Hazard
Survival
Quantile
Mean
nomogram
survest
survplot
validate
calibrate
vif
naresid
naprint
impute

Purpose
from a fitted regression model
R function analytic representation of a fitted
hazard function (for psm)
R function analytic representation of fitted
survival function (for psm, cph)
R function analytic representation of fitted
function for quantiles of survival time
(for psm, cph)
R function analytic representation of fitted
function for mean survival time or for ordinal logistic
Draws a nomogram for the fitted model
Estimate survival probabilities (psm, cph)
Plot survival curves (psm, cph)
Validate indexes of model fit using resampling
Estimate calibration curve using resampling
Variance inflation factors for fitted model
Bring elements corresponding to missing data
back into predictions and residuals
Print summary of missing values
Impute missing values

Related Functions

latex, plot
survfit
plot.survfit
val.prob

transcan

The first argument of the majority of functions is the object returned from the
model fitting function. When used with ols, lrm, orm, psm, cph, Glm, Gls, Rq,
bj, these functions do the following. specs prints the design specifications, for
example, number of parameters for each factor, levels of categorical factors,
knot locations in splines, and so on. vcov returns the variance-covariance
matrix for the model. logLik retrieves the maximized log-likelihood, whereas
AIC computes the Akaike Information Criterion for the model on the minus
twice log-likelihood scale (with an option to compute it on the 2 scale if
you specify type=’chisq’). lrtest, when given two fit objects from nested
models, computes the likelihood ratio test for the extra variables. univarLR
computes all univariable likelihood ratio 2 statistics, one predictor at a time.
The robcov function computes the Huber robust covariance matrix estimate. bootcov uses the bootstrap to estimate the covariance matrix of parameter estimates. Both robcov and bootcov assume that the design matrix
and response variable were stored with the fit. They have options to adjust
for cluster sampling. Both replace the original variance–covariance matrix
with robust estimates and return a new fit object that can be passed to any
of the other functions. In that way, robust Wald tests, variable selection, confidence limits, and many other quantities may be computed automatically.
The functions do save the old covariance estimates in component orig.var
of the new fit object. bootcov also optionally returns the matrix of parameter estimates over the bootstrap simulations. These estimates can be used
to derive bootstrap confidence intervals that don’t assume normality or symmetry. Associated with bootcov are plotting functions for drawing histogram
and smooth density estimates for bootstrap distributions. bootcov also has
a feature for deriving approximate nonparametric simultaneous confidence

138

6 R Software

sets. For example, the function can get a simultaneous 0.90 confidence region
for the regression e↵ect of age over its entire range.
The pentrace function assists in selection of penalty factors for fitting
regression models using penalized maximum likelihood estimation (see Section 9.10). Di↵erent types of model terms can be penalized by di↵erent
amounts. For example, one can penalize interaction terms more than main
e↵ects. The effective.df function prints details about the e↵ective degrees
of freedom devoted to each type of model term in a penalized fit.
summary prints a summary of the e↵ects of each factor. When summary is
used to estimate e↵ects (e.g., odds or hazard ratios) for continuous variables,
it allows the levels of interacting factors to be easily set, as well as allowing
the user to choose the interval for the e↵ect. This method of estimating e↵ects
allows for nonlinearity in the predictor. By default, interquartile range e↵ects
(di↵erences in X ˆ, odds ratios, hazards ratios, etc.) are printed for continuous
factors, and all comparisons with the reference level are made for categorical
factors. See the example at the end of the summary.rms documentation for
a method of quickly computing pairwise treatment e↵ects and confidence
intervals for a large series of values of factors that interact with the treatment
variable. Saying plot(summary(fit)) will depict the e↵ects graphically, with
bars for a list of confidence levels.
The anova function automatically tests most meaningful hypotheses in a
design. For example, suppose that age and cholesterol are predictors, and
that a general interaction is modeled using a restricted spline surface. anova
prints Wald statistics for testing linearity of age, linearity of cholesterol, age
e↵ect (age + age ⇥ cholesterol interaction), cholesterol e↵ect (cholesterol +
age ⇥ cholesterol interaction), linearity of the age ⇥ cholesterol interaction
(i.e., adequacy of the simple age ⇥ cholesterol 1 d.f. product), linearity of the
interaction in age alone, and linearity of the interaction in cholesterol alone.
Joint tests of all interaction terms in the model and all nonlinear terms in
the model are also performed. The plot.anova function draws a dot chart
showing the relative contribution ( 2 , 2 minus d.f., AIC, or P -value) of each
factor in the model.
The contrast function is used to obtain general contrasts and corresponding confidence limits and test statistics. This is most useful for testing e↵ects
in the presence of interactions (e.g., type II and type III contrasts). See the
help file for contrast.rms for several examples of how to obtain joint tests of
multiple contrasts (see Section 9.3.2) as well as double di↵erences (interaction
contrasts).
The predict function is used to obtain a variety of values or predicted values from either the data used to fit the model or a new dataset. The Predict
function is easier to use for most purposes, and has a special plot method.
The gendata function makes it easy to obtain a data frame containing predictor combinations for obtaining selected predicted values.
The fastbw function performs a slightly inefficient but numerically stable
version of fast backward elimination on factors, using a method based on

6.3 The rms Package

139

Lawless and Singhal.377 This method uses the fitted complete model and
computes approximate Wald statistics by computing conditional (restricted)
maximum likelihood estimates assuming multivariate normality of estimates.
It can be used in simulations since it returns indexes of factors retained and
dropped:
fit
o l s ( y ⇠ x1 ⇤ x2 ⇤ x3 )
# run, and print results:
fastbw ( f i t , optional arguments )
# typically used in simulations:
z
fastbw ( f i t , o p t i o n a l a r g s )
# least squares fit of reduced model:
l m . f i t (X[ , z $ p a r m s . k e p t ] , Y)
fastbw deletes factors, not columns of the design matrix. Factors requiring

multiple d.f. will be retained or dropped as a group. The function prints the
deletion statistics for each variable in turn, and prints approximate parameter
estimates for the model after deleting variables. The approximation is better
when the number of factors deleted is not large. For ols, the approximation
is exact.
The which.influence function creates a list with a component for each
factor in the model. The names of the components are the factor names. Each
component contains the observation identifiers of all observations that are
“overly influential” with respect to that factor, meaning that |dfbetas| > u
for at least one i associated with that factor, for a given u. The default
u is .2. You must have specified x=TRUE, y=TRUE in the fitting function to
use which.influence. The first argument is the fit object, and the second
argument is the cuto↵ u.
The following R program will print the set of predictor values that were
very influential for each factor. It assumes that the data frame containing the
data used in the fit is called df.
f
lrm ( y ⇠ x1 + x2 + . . . , data=df , x=TRUE, y=TRUE)
w
which.influence ( f , .4 )
nam
names (w)
f o r ( i i n 1 : l e n g t h (nam ) ) {
cat ( ” I n f l u e n t i a l observations for e f f e c t of ” ,
nam [ i ] , ” \n” )
p r i n t ( df [w [ [ i ] ] , ] )
}

The latex function is a generic function available in the Hmisc package. It
invokes a specific latex function for most of the fit objects created by rms to
create a LATEX algebraic representation of the fitted model for inclusion in a
report or viewing on the screen. This representation documents all parameters
in the model and the functional form being assumed for Y , and is especially
useful for getting a simplified version of restricted cubic spline functions. On
the other hand, the print method with optional argument latex=TRUE is
used to output LATEX code representing the model results in tabular form to
the console. This is intended for use with Sweave391 .

140

6 R Software

The Function function composes an R function that you can use to evaluate X ˆ analytically from a fitted regression model. The documentation for
Function also shows how to use a subsidiary function sascode that will (almost) translate such an R function into SAS code for evaluating predicted
values in new subjects. Neither Function nor latex handles third-order interactions.
The nomogram function draws a partial nomogram for obtaining predictions
from the fitted model manually. It constructs di↵erent scales when interactions (up to third-order) are present. The constructed nomogram is not complete, in that point scores are obtained for each predictor and the user must
add the point scores manually before reading predicted values on the final
axis of the nomogram. The constructed nomogram is useful for interpreting
the model fit, especially for non-monotonically transformed predictors (their
scales wrap around an axis automatically).
The vif function computes variance inflation factors from the covariance
matrix of a fitted model, using [144, 648].
The impute function is another generic function. It does simple imputation
by default. It can also work with the transcan function to multiply or singly
impute missing values using a flexible additive model.
As an example of using many of the functions, suppose that a categorical variable treat has values "a", "b", and "c", an ordinal variable
num.diseases has values 0,1,2,3,4, and that there are two continuous variables, age and cholesterol. age is fitted with a restricted cubic spline, while
cholesterol is transformed using the transformation log(cholesterol+10).
Cholesterol is missing on three subjects, and we impute these using the overall median cholesterol. We wish to allow for interaction between treat and
cholesterol. The following R program will fit a logistic model, test all e↵ects
in the design, estimate e↵ects, and plot estimated transformations. The fit
for num.diseases really considers the variable to be a five-level categorical
variable. The only di↵erence is that a 3 d.f. test of linearity is done to assess
whether the variable can be remodeled “asis”. Here we also show statements
to attach the rms package and store predictor characteristics from datadist.
r e q u i r e ( rms )
# make new functions available
ddist
d a t a d i s t ( c h o l e s t e r o l , t r e a t , n u m . d i s e a s e s , age )
# Could have used ddist
datadist(data.frame.name)
o p t i o n s ( d a t a d i s t=” d d i s t ” )
# defines data dist. to rms
cholesterol
impute ( c h o l e s t e r o l )
fit
lrm ( y ⇠ t r e a t + s c o r e d ( n u m . d i s e a s e s ) + r c s ( age ) +
l o g ( c h o l e s t e r o l +10) +
t r e a t : l o g ( c h o l e s t e r o l +10))
d e s c r i b e ( y ⇠ t r e a t + s c o r e d ( n u m . d i s e a s e s ) + r c s ( age ) )
# or use describe(formula(fit)) for all variables used in
# fit. describe function (in Hmisc) gets simple statistics
# on variables
# fit
robcov(fit)# Would make all statistics that follow
# use a robust covariance matrix
# would need x=TRUE, y=TRUE in lrm()

6.3 The rms Package

141

specs ( f i t )
# Describe the design characteristics
anova ( f i t )
anova ( f i t , t r e a t , c h o l e s t e r o l ) # Test these 2 by themselves
p l o t ( anova ( f i t ) )
# Summarize anova graphically
summary ( f i t )
# Est. effects; default ranges
p l o t ( summary ( f i t ) ) # Graphical display of effects with C.I.
# Specific reference cell and adjustment value:
summary ( f i t , t r e a t=”b” , age =60)
# Estimate effect of increasing age: 50->70
summary ( f i t , age=c ( 5 0 , 7 0 ) )
# Increase age 50->70, adjust to 60 when estimating
# effects of other factors:
summary ( f i t , age=c ( 5 0 , 6 0 , 7 0 ) )
# If had not defined datadist, would have to define
# ranges for all variables
# Estimate and test treatment (b-a) effect averaged
# over 3 cholesterols:
c o n t r a s t ( f i t , l i s t ( t r e a t= ’ b ’ , c h o l e s t e r o l=c ( 1 5 0 , 2 0 0 , 2 5 0 ) ) ,
l i s t ( t r e a t= ’ a ’ , c h o l e s t e r o l=c ( 1 5 0 , 2 0 0 , 2 5 0 ) ) ,
t y p e= ’ a v e r a g e ’ )
p

P r e d i c t ( f i t , age=s e q ( 2 0 , 8 0 , l e n g t h =100) , t r e a t ,
c o n f . i n t=FALSE)
plot (p)
# Plot relationship between age and
# or ggplot(p)
# log odds, separate curve for each
# treat, no C.I.
p l o t ( p , ⇠ age | t r e a t )
# Same but 2 panels
g g p l o t ( p , g r o u p s=FALSE)
b p l o t ( P r e d i c t ( f i t , age , c h o l e s t e r o l , np =50))
# 3-dimensional perspective plot for
# age, cholesterol, and log odds
# using default ranges for both
# Plot estimated probabilities instead of log odds:
plot ( Predict ( f i t , num.diseases ,
f u n=f u n c t i o n ( x ) 1 /(1+ exp ( x ) ) ,
c o n f . i n t=. 9 ) , y l a b=” Prob ” )
# Again, if no datadist were defined, would have to tell
# plot all limits
logit
p r e d i c t ( f i t , e x p a n d . g r i d ( t r e a t=”b” , n u m. di s = 1 : 3 ,
age=c ( 2 0 , 4 0 , 6 0 ) ,
c h o l e s t e r o l=s e q ( 1 0 0 , 3 0 0 , l e n g t h = 10 )) )
# Could obtain list of predictor settings interactively
logit
p r e d i c t ( f i t , g e n d a t a ( f i t , nobs =12))
# An easier approach is
# Predict(fit, treat=’b’,num.dis=1:3,...)
#
#
#
#

Since age doesn’t interact with anything, we can quickly
and interactively try various transformations of age,
taking the spline function of age as the gold standard.
We are seeking a linearizing transformation.

ag
10:80
logit
p r e d i c t ( f i t , e x p a n d . g r i d ( t r e a t=” a ” , nu m .d i s =0 ,

6 R Software

142

#
#
#
#
#
#
#
#

age=ag ,
c h o l e s t e r o l=median ( c h o l e s t e r o l ) ) ,
t y p e=” terms ” ) [ , ” age ” ]
Note: if age interacted with anything, this would be the
age ‘main effect’ ignoring interaction terms
Could also use logit
Predict(f, age=ag, ...)$yhat,
which allows evaluation of the shape for any level of
interacting factors. When age does not interact with
anything, the result from predict(f, ..., type="terms")
would equal the result from Predict if all other terms
were ignored

# Could also specify:
# logit
predict(fit,
#
gendata(fit, age=ag, cholesterol=...))
# Unmentioned variables are set to reference values
p l o t ( ag ^ . 5 , l o g i t )
p l o t ( ag ^ 1 . 5 , l o g i t )

# try square root vs. spline transform.
# try 1.5 power

# Pretty printing of table of estimates and
# summary statistics:
p r i n t ( f i t , l a t e x=TRUE) # print LATEX code to console
latex ( f i t )
# invokes latex.lrm, creates fit.tex
# Draw a nomogram for the model fit
p l o t ( nomogram ( f i t ) )
# Compose R function to evaluate linear predictors
# analytically
g
Function ( f i t )
g ( t r e a t= ’ b ’ , c h o l e s t e r o l =260 , age =50)
# Letting num.diseases default to reference value

To examine interactions in a simpler way, you may want to group age into
tertiles:
age.tertile
c u t 2 ( age , g=3)
# For auto ranges later, specify age.tertile to datadist
fit
lrm ( y ⇠ a g e . t e r t i l e ⇤ r c s ( c h o l e s t e r o l ) )

Example output from these functions is shown in Chapter 10 and later chapters.
Note that type="terms" in predict scores each factor in a model with
its fitted transformation. This may be used to compute, for example, rank
correlation between the response and each transformed factor, pretending it
has 1 d.f.
When regression is done on principal components, one may use an ordinary linear model to decode “internal” regression coefficients for helping to
understand the final model. Here is an example.
r e q u i r e ( rms )
dd
d a t a d i s t ( my.data )
o p t i o n s ( d a t a d i s t= ’ dd ’ )
pcfit
princomp (⇠ pain.symptom1 + pain.symptom2 + s i g n 1 +

6.4 Other Functions

143

s i g n 2 + s i g n 3 + smoking )
pc2
pcfit $ scores [ ,1:2]
# first 2 PCs as matrix
logistic.fit
lrm ( death ⇠ r c s ( age , 4 ) + pc2 )
predicted.logit
predict ( l o g i s t i c . f i t )
linear.mod
o l s ( p r e d i c t e d . l o g i t ⇠ r c s ( age , 4 ) +
pain.symptom1 + pain.symptom2 +
s i g n 1 + s i g n 2 + s i g n 3 + smoking )
# This model will have R-squared=1
nom
nomogram ( l i n e a r . m o d , f u n=f u n c t i o n ( x ) 1 /(1+ exp ( x ) ) ,
f u n l a b e l=” P r o b a b i l i t y o f Death ” )
# can use fun=plogis
p l o t (nom)
# 7 Axes showing effects of all predictors, plus a reading
# axis converting to predicted probability scale

In addition to many of the add-on functions described above, there are
several other R functions that validate models. The first, predab.resample,
is a general-purpose function that is used by functions for specific models
described later. predab.resample computes estimates of optimism and biascorrected estimates of a vector of indexes of predictive accuracy, for a model
with a specified design matrix, with or without fast backward step-down of
predictors. If bw=TRUE, predab.resample prints a matrix of asterisks showing which factors were selected at each repetition, along with a frequency
distribution of the number of factors retained across resamples. The function has an optional parameter that may be specified to force the bootstrap
algorithm to do sampling with replacement from clusters rather than from
original records, which is useful when each subject has multiple records in
the dataset. It also has a parameter that can be used to validate predictions
in a subset of the records even though models are refit using all records.
The generic function validate invokes predab.resample with modelspecific fits and measures of accuracy. The function calibrate invokes
predab.resample to estimate bias-corrected model calibration and to plot
the calibration curve. Model calibration is estimated at a sequence of predicted values.

6.4 Other Functions
For principal component analysis, R has the princomp and prcomp functions.
Canonical correlations and canonical variates can be easily computed using the cancor function. There are many other R functions for examining
associations and for fitting models. The supsmu function implements Friedman’s “super smoother.”203 The lowess function implements Cleveland’s
two-dimensional smoother.108 The glm function will fit general linear models
under a wide variety of distributions of Y . There are functions to fit Hastie
and Tibshirani’s270 generalized additive model for a variety of distributions.
More is said about parametric and nonparametric additive multiple regres-

6 R Software

144

sion functions in Chapter 16. The loess function fits a multidimensional
scatterplot smoother (the local regression model of Cleveland et al.93 ). loess
provides approximate test statistics for normal or symmetrically distributed
Y:
f
l o e s s ( y ⇠ age ⇤ p r e s s u r e )
plot ( f )
# cross-sectional plots
ages
s e q ( 2 0 , 7 0 , l e n g t h =40)
pressures
s e q ( 8 0 , 2 0 0 , l e n g t h =40)
pred
predict ( f ,
e x p a n d . g r i d ( age=age s , p r e s s u r e=p r e s s u r e s ) )
p e r s p ( a ges , p r e s s u r e s , pred )
# 3-D plot
loess has a large number of options allowing various restrictions to be placed

on the fitted surface.
Atkinson and Therneau’s rpart recursive partitioning package and related functions implement classification and regression trees68 algorithms for
binary, continuous, and right-censored response variables (assuming an exponential distribution for the latter). rpart deals e↵ectively with missing
predictor values using surrogate splits. The rms package has validate.rpart
for obtaining cross-validated mean squared errors and Somers’ Dxy rank correlations (Brier score and ROC areas for probability models).
For displaying which variables tend to be missing on the same subjects, the
Hmisc naclus function can be used (e.g., plot(naclus(dataframename)) or
naplot(naclus( dataframename))). For characterizing what type of subjects
have NA’s on a given predictor (or response) variable, a tree model whose
response variable is is.na(varname) can be quite useful.
require ( rpart )
f
r p a r t ( i s . n a ( c h o l e s t e r o l ) ⇠ age + s e x + t r i g + smoking )
plot ( f )
# plots the tree
text ( f )
# labels the tree

The Hmisc rcorr.cens function can compute Somers’ Dxy rank correlation coefficient and its standard error, for binary or continuous (and possibly
right-censored) responses. A simple transformation of Dxy yields the c index
(generalized ROC area). The Hmisc improveProb function is useful for comparing two probability models using the methods of Pencina etal483, 485, 486
in an external validation setting. See also the rcorrp.cens function in this
context.

6.5 Further Reading
1
2

Harrell and Goldstein258 list components of statistical languages or packages
and compare several popular packages for survival analysis capabilities.
Imai et al.312 have further generalized R as a statistical modeling language.

Chapter 7

Modeling Longitudinal Responses using
Generalized Least Squares

In this chapter we consider models for a multivariate response variable represented by serial measurements over time within subject. This setup induces
correlations between measurements on the same subject that must be taken
into account to have optimal model fits and honest inference. Full likelihood
model-based approaches have advantages including (1) optimal handling of
imbalanced data and (2) robustness to missing data (dropouts) that occur
not completely at random. The three most popular model-based full likelihood approaches are mixed e↵ects models, generalized least squares, and
Bayesian hierarchical models. For continuous Y , generalized least squares
has a certain elegance, and a case study will demonstrate its use after surveying competing approaches. As OLS is a special case of generalized least
squares, the case study is also helpful in developing and interpreting OLS
models.
Some good references on longitudinal data analysis include145, 156, 247, 407, 502, 627, 629 .

7.1 Notation and Data Setup
Suppose there are N independent subjects, with subject i (i = 1, 2, . . . , N )
having ni responses measured at times ti1 , ti2 , . . . , tini . The response at time t
for subject i is denoted by Yit . Suppose that subject i has baseline covariates
Xi . Generally the response measured at time ti1 = 0 is a covariate in Xi
instead of being the first measured response Yi0 .
For flexible analysis, longitudinal data are usually arranged in a “tall and
thin” layout. This allows measurement times to be irregular. In studies comparing two or more treatments, a response is often measured at baseline
(pre-randomization). The analyst has the option to use this measurement as
Yi0 or as part of Xi . There are many reasons to put initial measurements of
Y in X, i.e., to use baseline measurements as baseline.

145

1

146

7 Modeling Longitudinal Responses using Generalized Least Squares

7.2 Model Specification for E↵ects on E(Y )
Common representations for mean time-response profiles include
• k dummy variables for k + 1 unique times (assumes no functional form for
time but may spend many d.f.)
• k = 1 for linear time trend, g1 (t) = t
• k–order polynomial in t
• k + 1–knot restricted cubic spline (one linear term, k 1 nonlinear terms)
Suppose the time trend is modeled with k parameters so that the time effect has k d.f. Let the basis functions modeling the time e↵ect be g1 (t), g2 (t), . . . , gk (t)
to allow it to be nonlinear. A model for the time profile without interactions
between time and any X is given by
E[Yit |Xi ] = Xi +

1 g1 (t)

+

2 g2 (t)

+ ... +

k gk (t).

(7.1)

To allow the slope or shape of the time-response profile to depend on some
of the Xs we add product terms for desired interaction e↵ects. For example,
to allow the mean time trend for subjects in group 1 (reference group) to
be arbitrarily di↵erent from the time trend for subjects in group 2, have a
dummy variable for group 2, a time “main e↵ect” curve with k d.f. and all k
products of these time components with the dummy variable for group 2.
Once the right hand side of the model is formulated, predicted values,
contrasts, and ANOVAs are obtained just as with a univariate model. For
these purposes time is no di↵erent than any other covariate except for what
is described in the next section.

7.3 Modeling Within-Subject Dependence

2

The following table is an attempt to briefly survey available longitudinal
analysis methods.

7.3 Modeling Within-Subject Dependence

147

What Methods To Use for Repeated Measurements /
Serial Data? ab

Assumes normality
Assumes independence of
measurements within subject
Assumes a correlation structuref
Requires same measurement
times for all subjects
Does not allow smooth modeling
of time to save d.f.
Does not allow adjustment for
baseline covariates
Does not easily extend to
non-continuous Y
Loses information by not using
intermediate measurements
Does not allow widely varying #
of observations per subject
Does not allow for subjects
to have distinct trajectoriesk
Assumes subject-specific e↵ects
are Gaussian
Badly biased if non-random
dropouts
Biased in general
Hard to get tests & CLs
Requires large # subjects/clusters
SEs are wrong
Assumptions are not verifiable
in small samples
Does not extend to complex
settings such as time-dependent
covariates and dynamico models

LOCF is an imputation
method that doesn't
penalize properly (the
standard error is way
too small)

a

Repeated GEE Mixed GLS LOCF Summary
Measures
E↵ects
Statisticc
ANOVA
Model
⇥
⇥
⇥
⇥d
⇥e
⇥
⇥

⇥g

⇥

⇥

?

⇥
⇥
⇥

⇥

⇥

⇥i

⇥

⇥

⇥

⇥h

⇥

⇥

⇥j

⇥

⇥
?

⇥n
⇥
⇥

⇥

⇥
N/A

⇥
⇥
⇥m

⇥l
⇥

⇥

⇥
⇥

⇥

⇥

⇥

?

Thanks to Charles Berry, Brian Cade, Peter Flom, Bert Gunter, and Leena Choi
for valuable input.
b
GEE: generalized estimating equations; GLS: generalized least squares; LOCF: last
observation carried forward.
c
E.g., compute within-subject slope, mean, or area under the curve over time. Assumes that the summary measure is an adequate summary of the time profile and
assesses the relevant treatment e↵ect.
d
Unless one uses the Huynh-Feldt or Greenhouse-Geisser correction
e
For full efficiency, if using the working independence model
f
Or requires the user to specify one
g
For full efficiency of regression coefficient estimates
h
Unless the last observation is missing
i
The cluster sandwich variance estimator used to estimate SEs in GEE does not
perform well in this situation, and neither does the working independence model
because it does not weight subjects properly.
j
Unless one knows how to properly do a weighted analysis
k
Or uses population averages
l
Unline GLS, does not use standard maximum likelihood methods yielding simple
likelihood ratio 2 statistics. Requires high-dimensional integration to marginalize
random e↵ects, using complex approximations, and if using SAS, unintuitive d.f. for
the various tests.
m
Because there is no correct formula for SE of e↵ects; ordinary SEs are not penalized
for imputation and are too small
n
If correction not applied
o
E.g., a model with a predictor that is a lagged value of the response variable

148

7 Modeling Longitudinal Responses using Generalized Least Squares

The most prevalent full modeling approach is mixed e↵ects models in which
baseline predictors are fixed e↵ects, and random e↵ects are used to describe
subject di↵erences and to induce within-subject correlation. Some disadvantages of mixed e↵ects models are
• The induced correlation structure for Y may be unrealistic if care is not
taken in specifying the model.
• Mixed e↵ects models are numerically demanding.
• They require complex approximations for distributions of test statistics.
• It is commonly assumed that random e↵ects following a normal distribution. This assumption may be violated.
It could be argued that an extended linear model (with no random e↵ects)
is a logical extension of the univariate OLS modela . This model, called the
generalized least squares or growth curve model217, 502, 503 , was developed
long before mixed e↵ect models became popular.
We will assume that Yit |Xi has a multivariate normal distribution with
mean given above and with variance-covariance matrix Vi , an ni ⇥ ni matrix
that is a function of ti1 , . . . , tini . We further assume that the diagonals of Vi
are all equalb . This extended linear model has the following assumptions:
• all the assumptions of OLS at a single time point including correct modeling of predictor e↵ects and univariate normality of responses conditional
on X
• the distribution of two responses at two di↵erent times for the same subject, conditional on X, is bivariate normal with a specified correlation
coefficient
• the joint distribution of all ni responses for the ith subject is multivariate
normal with the given correlation pattern (which implies the previous two
distributional assumptions)
• responses from two di↵erent subjects are uncorrelated.

7.4 Parameter Estimation Procedure
Generalized least squares is like weighted least squares but uses a covariance
matrix that is not diagonal. Each subject can have her own shape of Vi due
to each subject being measured at a di↵erent set of times. This is a maximum
likelihood procedure. Newton-Raphson or other trial-and-error methods are
a

E.g., few statisticians use subject random e↵ects for univariate Y . Pinheiro and
Bates [502, Section 5.1.2] state that “in some applications, one may wish to avoid
incorporating random e↵ects in the model to account for dependence among observations, choosing to use the within-group component ⇤i to directly model variancecovariance structure of the response.”
b
This procedure can be generalized to allow for heteroscedasticity over time or with
respect to X, e.g., males may be allowed to have a di↵erent variance than females.

use the date that the
data were measured as
the time variable, not the
intended date that you
hoped they'd come in.

7.5 Common Correlation Structures

149

used for estimating parameters. For a small number of subjects, there are
advantages in using REML (restricted maximum likelihood) instead of ordinary MLE [156, Section 5.3] [502, Chapter 5]217 (especially to get a more
unbiased estimate of the covariance matrix).
When imbalances of measurement times are not severe, OLS fitted ignoring
subject identifiers may be efficient for estimating . But OLS standard errors
will be too small as they don’t take intra-cluster correlation into account.
This may be rectified by substituting a covariance matrix estimated using
the Huber-White cluster sandwich estimator or from the cluster bootstrap.
When imbalances are severe and intra-subject correlations are strong, OLS
(or GEE using a working independence model) is not expected to be efficient
because it gives equal weight to each observation; a subject contributing two
distant observations receives 15 the weight of a subject having 10 tightlyspaced observations.

7.5 Common Correlation Structures
We usually restrict ourselves to isotropic correlation structures which assume
the correlation between responses within subject at two times depends only
on a measure of the distance between the two times, not the individual times.
We simplify further and assume it depends on |t1 t2 |c . Assume that the
correlation coefficient for Yit1 vs. Yit2 conditional on baseline covariates Xi
for subject i is h(|t1 t2 |, ⇢), where ⇢ is a vector (usually a scalar) set of
fundamental correlation parameters. Some commonly used structures when
times are continuous and are not equally spaced [502, Section 5.3.3] are shown
below, along with the correlation function names from the R nlme package.

most popular>

Compound symmetry: h = ⇢ if t1 6= t2 , 1 if t1 = t2
(Essentially what two-way ANOVA assumes)
Autoregressive-moving average lag 1: h = ⇢|t1 t2 | = ⇢s
where s = |t1 t2 |
Exponential: h = exp( s/⇢)
Gaussian: h = exp[ (s/⇢)2 ]
Linear: h = (1 s/⇢)[s < ⇢]
Rational quadratic: h = 1 (s/⇢)2 /[1 + (s/⇢)2 ]
Spherical: h = [1 1.5(s/⇢) + 0.5(s/⇢)3 ][s < ⇢]
Linear exponent AR(1):

h=⇢

dmin +

s

dmin

dmax

dmin

nlme corCompSymm
corCAR1
corExp
corGaus
corLin
corRatio
corSpher

, 1 if t1 = t2 568

The structures 3–7 use ⇢ as a scaling parameter, not as something restricted to be in [0, 1]
c

We can speak interchangeably of correlations of residuals within subjects or correlations between responses measured at di↵erent times on the same subject, conditional
on covariates X.

150

7 Modeling Longitudinal Responses using Generalized Least Squares

7.6 Checking Model Fit
The constant variance assumption may be checked using typical residual
plots. The univariate normality assumption (but not multivariate normality) may be checked using typical Q-Q plots on residuals. For checking the
correlation pattern, a variogram is a very helpful device based on estimating
correlations of all possible pairs of residuals at di↵erent time points. Pairs
of estimates obtained at the same absolute time di↵erence s are pooled. The
variogram is a plot with y = 1 ĥ(s, ⇢) vs. s on the x-axis, and the theoretical
variogram of the correlation model currently being assumed is superimposed.

7.7 R Software
The nonlinear mixed e↵ects model package nlme of Pinheiro & Bates in
Rprovides many useful functions. For fitting linear models, fitting functions
are lme for mixed e↵ects models and gls for generalized least squares without
random e↵ects. The rms package has a front-end function Gls so that many
features of rms can be used:
anova: all partial Wald tests, test of linearity, pooled tests
summary: e↵ect estimates (di↵erences in Ŷ ) and confidence limits
Predict and plot: partial e↵ect plots
nomogram: nomogram
Function: generate R function code for the fitted model
latex: LATEX representation of the fitted model.

In addition, Gls has a cluster bootstrap option (hence you do not use rms’s
bootcov for Gls fits). When B is provided to Gls( ), bootstrapped regression
coefficients and correlation estimates are saved, the former setting up for
bootstrap percentile confidence limitsd The nlme package has many graphics
and fit-checking functions. Several functions will be demonstrated in the case
study.

7.8 Case Study
Consider the dataset in Table 6.9 of Davis [145, pp. 161-163] from a multicenter, randomized controlled trial of botulinum toxin type B (BotB) in patients with cervical dystonia from nine U.S. sites. Patients were randomized
to placebo (N = 36), 5000 units of BotB (N = 36), or 10,000 units of BotB
d

To access regular gls functions named anova (for likelihood ratio tests, AIC, etc.)
or summary use anova.gls or summary.gls.

7.8 Case Study

151

(N = 37). The response variable is the total score on the Toronto Western
Spasmodic Torticollis Rating Scale (TWSTRS), measuring severity, pain, and
disability of cervical dystonia (high scores mean more impairment). TWSTRS
is measured at baseline (week 0) and weeks 2, 4, 8, 12, 16 after treatment
began. The dataset name on the dataset wiki page is cdystonia.

7.8.1 Graphical Exploration of Data
Graphics which follow display raw data as well as quartiles of TWSTRS by
time, site, and treatment. A table shows the realized measurement schedule.
r e q u i r e ( rms )
getHdata ( c d y s t o n i a )
attach ( cdystonia )
# Construct unique subject ID
uid
with ( c d y s t o n i a , f a c t o r ( p a s t e ( s i t e , i d ) ) )
# Tabulate patterns of subjects’ time points
t a b l e ( t a p p l y ( week , uid ,
f u n c t i o n (w) p a s t e ( s o r t ( u n i q u e (w) ) , c o l l a p s e= ’ ’ ) ) )

0
0 2 4
1
1
0 2 4 8 12 0 2 4 8 12 16
1
94
0 4 8 12 16
0 4 8 16
4
1

0 2 4 12 16
3
0 2 4 8 16
1

0 2 4 8
1
0 2 8 12 16
2

# Plot raw data, superposing subjects
xl
x l a b ( ’ Week ’ ) ; y l
y l a b ( ’ TWSTRS total s c o r e ’ )
g g p l o t ( c d y s t o n i a , a e s ( x=week , y=t w s t r s , c o l o r=f a c t o r ( i d ) ) ) +
geom line () + xl + yl + f a c e t g r i d ( treat ⇠ s i t e ) +
g u i d e s ( c o l o r=FALSE) # Fig. 7.1
# Show quartiles
g g p l o t ( c d y s t o n i a , a e s ( x=week , y=t w s t r s ) ) + x l + y l +
y l i m ( 0 , 7 0 ) + stat summary ( f u n . d a t a=” m e d i a n h i l o w ” ,
c o n f . i n t =0. 5 , geom= ’ smooth ’ ) +
f a c e t w r a p (⇠ t r e a t , nrow=2) # Fig. 7.2

Next the data are rearranged so that Yi0 is a baseline covariate.
baseline
baseline
followup

s u b s e t ( d a t a . f r a m e ( c d y s t o n i a , u i d ) , week == 0 ,
week )
upData ( b a s e l i n e , rename=c ( t w s t r s= ’ t w s t r s 0 ’ ) ,
p r i n t=FALSE)
s u b s e t ( d a t a . f r a m e ( c d y s t o n i a , u i d ) , week > 0 ,

The SAP was written before the data was unblinded.

152

1

7 Modeling Longitudinal Responses using Generalized Least Squares

2

3

4

5

6

7

8

9

60
10000U

40

60
5000U

TWSTRS−total score

20

40
20

60
Placebo

40
20
0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15

Week
Fig. 7.1 Time profiles for individual subjects, stratified by study site and dose

c ( uid , week , t w s t r s ) )
rm ( u i d )
both

merge ( b a s e l i n e , f o l l o w u p , by= ’ u i d ’ )

dd
d a t a d i s t ( both )
o p t i o n s ( d a t a d i s t= ’ dd ’ )

you can always make things more complex than you originally thought, because that process is selfpenalizing. For example, if you had pre-specified time to be linear, but saw this graph, you could
change to modeling time as quadratic. You could not, or at least should not, do the opposite (say it
is quadratic, look at a graph and change to linear).

7.8 Case Study

153

10000U

5000U

60

TWSTRS−total score

40
20
0
Placebo
60
40
20
0
0

5

10

15

Week
Fig. 7.2 Quartiles of TWSTRS stratified by dose

7.8.2 Using Generalized Least Squares
We stay with baseline adjustment and use a variety of correlation structures,
with constant variance. Time is modeled as a restricted cubic spline with 3
knots, because there are only 3 unique interior values of week.
r e q u i r e ( nlme )
cp
l i s t ( corCAR1 , corExp , corCompSymm , c o r L i n , corGaus , c o r S p h e r )
z
v e c t o r ( ’ l i s t ’ , l e n g t h ( cp ) )
f o r ( k i n 1 : l e n g t h ( cp ) ) {
z [[k]]
g l s ( t w s t r s ⇠ t r e a t ⇤ r c s ( week , 3 ) +
r c s ( t w s t r s 0 , 3 ) + r c s ( age , 4 ) ⇤ sex , data=both ,
c o r r e l a t i o n=cp [ [ k ] ] ( form = ⇠week | u i d ) )
}
anova ( z [ [ 1 ] ] , z [ [ 2 ] ] , z [ [ 3 ] ] , z [ [ 4 ] ] , z [ [ 5 ] ] , z [ [ 6 ] ] )

z[[1]]

Model df
AIC
BIC
logLik
1 20 3553.906 3638.357 -1756.953

154

7 Modeling Longitudinal Responses using Generalized Least Squares

z[[2]]
z[[3]]
z[[4]]
z[[5]]
z[[6]]

3

2
3
4
5
6

20
20
20
20
20

3553.906
3587.974
3575.079
3621.081
3570.958

3638.357
3672.426
3659.531
3705.532
3655.409

-1756.953
-1773.987
-1767.540
-1790.540
-1765.479

AIC computed above is set up so that smaller values are best. From this
the continuous-time AR1 and exponential structures are tied for the best.
For the remainder of the analysis we use corCAR1, using Gls.
a

Gls ( t w s t r s ⇠ t r e a t ⇤ r c s ( week , 3 ) + r c s ( t w s t r s 0 , 3 ) +
r c s ( age , 4 ) ⇤ sex , data=both ,
c o r r e l a t i o n=corCAR1 ( form=⇠week | u i d ) )

if you don't have
the correlation
p r i n t ( a , l a t e x=TRUE)
structure narrowed
Generalized Least Squares Fit by REML
down to about 3,
the AIC does not do
Gls(model = twstrs ˜ treat * rcs(week, 3) + rcs(twstrs0, 3) +
rcs(age, 4) * sex, data = both, correlation = corCAR1(form = ˜week |
a good job detecting the
uid))
best correlation pattern.
Obs
522 Log-restricted-likelihood -1756.95
Clusters 108 Model d.f.
17
g
11.334
8.5917
d.f.
504

Intercept
treat=5000U
treat=Placebo
week
week’
twstrs0
twstrs0’
age
age’
age”
sex=M
treat=5000U * week
treat=Placebo * week
treat=5000U * week’
treat=Placebo * week’
age * sex=M
age’ * sex=M
age” * sex=M

Coef
S.E.
t Pr(> |t|)
-0.3093 11.8804 -0.03 0.9792
0.4344 2.5962 0.17 0.8672
7.1433 2.6133 2.73 0.0065
0.2879 0.2973 0.97 0.3334
0.7313 0.3078 2.38 0.0179
0.8071 0.1449 5.57 < 0.0001
0.2129 0.1795 1.19 0.2360
-0.1178 0.2346 -0.50 0.6158
0.6968 0.6484 1.07 0.2830
-3.4018 2.5599 -1.33 0.1845
24.2802 18.6208 1.30 0.1929
0.0745 0.4221 0.18 0.8599
-0.1256 0.4243 -0.30 0.7674
-0.4389 0.4363 -1.01 0.3149
-0.6459 0.4381 -1.47 0.1411
-0.5846 0.4447 -1.31 0.1892
1.4652 1.2388 1.18 0.2375
-4.0338 4.8123 -0.84 0.4023

7.8 Case Study

155

Correlation Structure: Continuous AR(1)
Formula: ˜week | uid
Parameter estimate(s):
Phi
0.8666689
⇢ˆ = 0.8672, the estimate of the correlation between two measurements
taken one week apart on the same subject. The estimated correlation for
measurements 10 weeks apart is 0.867210 = 0.24.
v
Variogram ( a , form=⇠ week | u i d )
p l o t ( v ) # Figure 7.3

●

Semivariogram

0.6

●
●

●

8

10

●

0.4

●
●

0.2

2

4

6

12

14

Distance
Fig. 7.3 Variogram, with assumed correlation pattern superimposed

Next check constant variance and normality assumptions.
both $ r e s i d
r e s i d ( a ) ; both $ f i t t e d
fitted (a)
yl
ylab ( ’ Residuals ’ )
p1
g g p l o t ( both , a e s ( x=f i t t e d , y=r e s i d ) ) + g e o m p o i n t ( ) +
f a c e t g r i d (⇠ t r e a t ) + y l
p2
g g p l o t ( both , a e s ( x=t w s t r s 0 , y=r e s i d ) ) + g e o m p o i n t ()+ y l
p3
g g p l o t ( both , a e s ( x=week , y=r e s i d ) ) + y l + y l i m ( 20 , 2 0 ) +
stat summary ( f u n . d a t a=” m e a n s d l ” , geom= ’ smooth ’ )
p4
g g p l o t ( both , a e s ( sample=r e s i d ) ) + s t a t q q ( ) + y l
g r i d E x t r a : : g r i d . a r r a n g e ( p1 , p2 , p3 , p4 , n c o l =2)
# Figure 7.4

Now get hypothesis tests, estimates, and graphically interpret the model.

7 Modeling Longitudinal Responses using Generalized Least Squares

10000U

Residuals

20

0

● ●
●● ● ●
●
●
● ●
●●● ●
●
●
● ●●
●
●● ● ●
●
●
●●
●●
●●
●● ●
●
●
● ●●●
●
●
●● ●●
●
●
●
●● ● ●
●
●
●
●●
●
●
●
●
●
● ●
●
●●●
●
●
●
●
●●●●
●
●
●●●
●
●
●●
●●
●
●
●
●●●
●
●
●●
●
●
●
● ●
● ●●
●●
●
●
●
● ●
●●
●
● ●●●●
●
●
● ● ●
●●
● ●●●●●
●●
● ● ●● ●
●
● ●● ● ●
●
●
●
● ● ●
●

−20

●
● ● ●
●
●
●

20

5000U

Placebo

●
●
●●●●
●
● ●●
● ●● ●
●●●
●
●● ●
●
●
●
●
●●
●
●
●●●
●●
●●
●
● ● ●
●
●
●
●
●● ●
● ●●
●●
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●● ●●●●
●●
●●
●●●
●●
●● ●
●
●
●●
● ●● ●
●
●
●
●●
●
●
● ●
●
●●
●
● ●
●
●
●●●●●
●
●● ●
●●●
●
●
●●
●
● ●●
●●●●
●●
●
●● ●●●
●●●●●●
●●
●
●●
●
●

●
●
●
●
● ●●
●●●
● ●● ●
●
●
●
●
●
●
●
●●
●
●
● ●● ●
●
●●
●●●
●
●
●
●
●
●
● ●
●●●●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●●●●
●
●
●●●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●●●●
●
●
●●●
●
●● ●●
●
● ●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
● ●●● ●
●● ● ●●●
●
●●
● ●
●
●
●

●

Residuals

156

0

−20

●

−40

●
●
●
●
● ● ● ● ● ●
●
●
●
●
●●● ●
●●●●
● ● ● ●●
●
●
●
●
●
●●
●
●
●
●
●
● ●
● ●●
●● ● ●●●
●●● ●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
● ●●●●●
●●
●
●●●
● ●
●
●●●●
●
●
●●●
● ●●
● ● ●
●●●●
●
●●●
●● ●
●●●
●
●
●
●
●●●●
●●●● ●●●
●●
● ● ●
●●
●●
●
● ●
●●
● ●●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●●
●
●● ●
●
●
●
●●●●
●●
●●
●●
●
●
●
● ●
●
●●
● ●●
●●●
●●●
● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●● ●
● ●
●●●
●●●
●●●●
●●
●●●
●●●●●
●●
●●
● ●
●●
●●●●●●●
● ●
●
●
● ● ●
● ●
●●
●● ● ●
●
●
●
●
●
●
●
●
●
● ● ●●
●
●● ●●
● ●
●●
●●
●●
●
●● ●●● ●
●
● ● ●●
●
●
●
●
● ●●
●
●
●●
●● ●●●
●●
● ●●
●
● ● ●
●
●
●●●
●
●
●
●
●
●
●
● ●
●
●
●

−40

●

●

203040506070203040506070203040506070

30

fitted

50

60

twstrs0
20

20

Residuals

10

Residuals

40

0

●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●

0

−20
●

−10
−40

−20
4

8

week

12

16

●

−2

0

theoretical

Fig. 7.4 Three residual plots to check for absence of trends in central tendency
and in variability. Upper right panel shows the baseline score on the x-axis. Bottom
left panel shows the mean ±2⇥SD. Bottom right panel is the QQ plot for checking
normality of residuals from the GLS fit.

2

7.8 Case Study

157
# Figure 7.5

p l o t ( anova ( a ) )

sex

●

age * sex

●

age

●

treat * week

●

treat

●

week

●

twstrs0

●

0

50

100

150

200

χ2 − df
Fig. 7.5 Results of anova.rms from generalized least squares fit with continuous
time AR1 correlation structure. As expected, the baseline version of Y dominates.

ylm
p1

ylim (25 , 60)
g g p l o t ( P r e d i c t ( a , week , t r e a t , c o n f . i n t=FALSE) ,
a d j . s u b t i t l e=FALSE, l e g e n d . p o s i t i o n= ’ top ’ ) + ylm
p2
g g p l o t ( P r e d i c t ( a , t w s t r s 0 ) , a d j . s u b t i t l e=FALSE) + ylm
p3
g g p l o t ( P r e d i c t ( a , age , s e x ) , a d j . s u b t i t l e=FALSE,
l e g e n d . p o s i t i o n= ’ top ’ ) + ylm
g r i d E x t r a : : g r i d . a r r a n g e ( p1 , p2 , p3 , n c o l =2)
# Figure 7.6
l a t e x ( summary ( a ) ,

f i l e = ’ ’ , t a b l e . e n v=FALSE) # Shows for week 8

week
twstrs0
age
treat — 5000U:10000U
treat — Placebo:10000U
sex — M:F

Low High
E↵ect
S.E. Lower 0.95 Upper 0.95
4 12 8 6.69100 1.10570
4.5238
8.8582
39 53 14 13.55100 0.88618
11.8140
15.2880
46 65 19 2.50270 2.05140
-1.5179
6.5234
1
2
0.59167 1.99830
-3.3249
4.5083
1
3
5.49300 2.00430
1.5647
9.4212
1
2
-1.08500 1.77860
-4.5711
2.4011

# To get results for week 8 for a different reference group
# for treatment, use e.g. summary(a, week=4, treat=’Placebo’)
# Compare low dose with placebo, separately at each time
k1
c o n t r a s t ( a , l i s t ( week=c ( 2 , 4 , 8 , 1 2 , 1 6 ) , t r e a t= ’ 5000U ’ ) ,
l i s t ( week=c ( 2 , 4 , 8 , 1 2 , 1 6 ) , t r e a t= ’ P l a c e b o ’ ) )
o p t i o n s ( width =80)
p r i n t ( k1 , d i g i t s =3)

158

7 Modeling Longitudinal Responses using Generalized Least Squares

70
Treatment

10000U

5000U

60

Placebo

50

^
Xβ

48

^
Xβ

44

40
30

40

20

36

10
4

8

12

30

16

Sex

F

40

50

60

TWSTRS−total score

Week

maybe the kind of treatment that patients just get used
to and therefore does not work after a period of time.

M

50

^
Xβ

45
40
35
30
25
40

50

60

70

80

Age, years
Fig. 7.6 Estimated e↵ects of time, baseline TWSTRS, age, and sex

1
2
3
4*
5*

week twstrs0 age sex Contrast S.E. Lower Upper
Z Pr(>|z|)
2
46 56
F
-6.31 2.10 -10.43 -2.186 -3.00
0.0027
4
46 56
F
-5.91 1.82 -9.47 -2.349 -3.25
0.0011
8
46 56
F
-4.90 2.01 -8.85 -0.953 -2.43
0.0150
12
46 56
F
-3.07 1.75 -6.49 0.361 -1.75
0.0795
16
46 56
F
-1.02 2.10 -5.14 3.092 -0.49
0.6260

Redundant contrasts are denoted by *
Confidence intervals are 0.95 individual intervals

# Compare high dose with placebo
k2
c o n t r a s t ( a , l i s t ( week=c ( 2 , 4 , 8 , 1 2 , 1 6 ) , t r e a t= ’ 10000U ’ ) ,
l i s t ( week=c ( 2 , 4 , 8 , 1 2 , 1 6 ) , t r e a t= ’ P l a c e b o ’ ) )
p r i n t ( k2 , d i g i t s =3)

the treatment effect
diminishes over time.

7.8 Case Study

159

week twstrs0 age sex Contrast S.E. Lower Upper
Z Pr(>|z|)
2
46 56
F
-6.89 2.07 -10.96 -2.83 -3.32
0.0009
4
46 56
F
-6.64 1.79 -10.15 -3.13 -3.70
0.0002
8
46 56
F
-5.49 2.00 -9.42 -1.56 -2.74
0.0061
12
46 56
F
-1.76 1.74 -5.17 1.65 -1.01
0.3109
16
46 56
F
2.62 2.09 -1.47 6.71 1.25
0.2099

1
2
3
4*
5*

Redundant contrasts are denoted by *
Confidence intervals are 0.95 individual intervals

fitting a time trend
allows you to
include all
participants,
even those who
dropped out.

0
●

●

−4
●
●

●

−8

4

8

12

16

High Dose − Placebo

a s . d a t a . f r a m e ( k1 [ c ( ’ week ’ , ’ C o n t r a s t ’ , ’ Lower ’ , ’ Upper ’ ) ] )
g g p l o t ( k1 , a e s ( x=week , y=C o n t r a s t ) ) + g e o m p o i n t ( ) +
g e o m l i n e ( ) + y l a b ( ’ Low Dose
Placebo ’ ) +
g e o m e r r o r b a r ( a e s ( ymin=Lower , ymax=Upper ) , width =0)
k2
a s . d a t a . f r a m e ( k2 [ c ( ’ week ’ , ’ C o n t r a s t ’ , ’ Lower ’ , ’ Upper ’ ) ] )
p2
g g p l o t ( k2 , a e s ( x=week , y=C o n t r a s t ) ) + g e o m p o i n t ( ) +
g e o m l i n e ( ) + y l a b ( ’ High Dose
Placebo ’ ) +
g e o m e r r o r b a r ( a e s ( ymin=Lower , ymax=Upper ) , width =0)
g r i d E x t r a : : g r i d . a r r a n g e ( p1 , p2 , n c o l =2)
# Figure 7.7

Low Dose − Placebo

k1
p1

4
●

0
●

−4
●
●

●

−8

4

8

12

16

week
week
these are of interest, but the last time point is the most interesting, where they end up.
Fig. 7.7 Contrasts and 0.95 confidence limits from GLS fit

Although multiple d.f. tests such as total treatment e↵ects or treatment ⇥
time interaction tests are comprehensive, their increased degrees of freedom
can dilute power. In a treatment comparison, treatment contrasts at the last
time point (single d.f. tests) are often of major interest. Such contrasts are
informed by all the measurements made by all subjects (up until dropout
times) when a smooth time trend is assumed. They use appropriate extrapo-

<key display of the
result. an estimate of
your contrast with
point-wise confidence
intervals. You can see
the treatment effect
wearing off.

160

7 Modeling Longitudinal Responses using Generalized Least Squares

lation past dropout times based on observed trajectories of subjects followed
the entire observation period.
A nomogram can be used to obtain predicted values, as well as to better
understand the model, just as with a univariate Y .
n
nomogram ( a , age=c ( s e q ( 2 0 , 8 0 , by =10) , 8 5 ) )
p l o t ( n , c e x . a x i s=. 5 5 , c e x . v a r=. 8 , lmgp=. 2 5 ) # Figure 7.8

0

10

20

30

40

50

60

70

80

90

100

Points

TWSTRS−total score
20

25

30

50

35

40

45

50

55

60

65

70

60

age (sex=F)
85

4070 20
60

70

age (sex=M)
50

40

85

30

20

5000U

treat (week=2)
10000U
5000U

Placebo

treat (week=4)
10000U
Placebo
5000U

treat (week=8)
10000U

Placebo
10000U

treat (week=12)
5000U
Placebo

treat (week=16)
5000U

Total Points
0

10

20

30

40

50

60

70

80

90

100

110

120

130

45

50

55

60

65

70

140

Linear Predictor
15

20

25

30

35

40

Fig. 7.8 Nomogram from GLS fit. Second axis is the baseline score.

chronic disease and an acute treatment (the treatment wears off) - this is...not ideal.

7.9 Further Reading

161

7.9 Further Reading
1

Jim Rochon (Rho, Inc., Chapel Hill NC) has the following comments about
using the baseline measurement of Y as the first longitudinal response.
For RCTs [randomized clinical trials], I draw a sharp line at the point
when the intervention begins. The LHS [left hand side of the model equation] is reserved for something that is a response to treatment. Anything
before this point can potentially be included as a covariate in the regression model. This includes the “baseline” value of the outcome variable.
Indeed, the best predictor of the outcome at the end of the study is typically where the patient began at the beginning. It drinks up a lot of
variability in the outcome; and, the e↵ect of other covariates is typically
mediated through this variable.
I treat anything after the intervention begins as an outcome. In the western scientific method, an “e↵ect” must follow the “cause” even if by a
split second.
Note that an RCT is di↵erent than a cohort study. In a cohort study,
“Time 0” is not terribly meaningful. If we want to model, say, the trend
over time, it would be legitimate, in my view, to include the “baseline”
value on the LHS of that regression model.
Now, even if the intervention, e.g., surgery, has an immediate e↵ect, I
would include still reserve the LHS for anything that might legitimately
be considered as the response to the intervention. So, if we cleared a
blocked artery and then measured the MABP, then that would still be
included on the LHS.
Now, it could well be that most of the therapeutic e↵ect occurred by
the time that the first repeated measure was taken, and then levels o↵.
Then, a plot of the means would essentially be two parallel lines and the
treatment e↵ect is the distance between the lines, i.e., the di↵erence in
the intercepts.
If the linear trend from baseline to Time 1 continues beyond Time 1, then
the lines will have a common intercept but the slopes will diverge. Then,
the treatment e↵ect will the di↵erence in slopes.
One point to remember is that the estimated intercept is the value at time
0 that we predict from the set of repeated measures post randomization.
In the first case above, the model will predict di↵erent intercepts even
though randomization would suggest that they would start from the same
place. This is because we were asleep at the switch and didn’t record the
“action” from baseline to time 1. In the second case, the model will predict
the same intercept values because the linear trend from baseline to time
1 was continued thereafter.
More importantly, there are considerable benefits to including it as a covariate on the RHS. The baseline value tends to be the best predictor of
the outcome post-randomization, and this maneuver increases the precision of the estimated treatment e↵ect. Additionally, any other prognostic
factors correlated with the outcome variable will also be correlated with
the baseline value of that outcome, and this has two important consequences. First, this greatly reduces the need to enter a large number of
prognostic factors as covariates in the linear models. Their e↵ect is already
mediated through the baseline value of the outcome variable. Secondly,
any imbalances across the treatment arms in important prognostic factors

162

7 Modeling Longitudinal Responses using Generalized Least Squares
will induce an imbalance across the treatment arms in the baseline value
of the outcome. Including the baseline value thereby reduces the need to
enter these variables as covariates in the linear models.
Stephen Senn558 states that temporally and logically, a “baseline cannot be
a response to treatment”, so baseline and response cannot be modeled in an
integrated framework.
. . . one should focus clearly on ‘outcomes’ as being the only values that
can be influenced by treatment and examine critically any schemes that
assume that these are linked in some rigid and deterministic view to
‘baseline’ values. An alternative tradition sees a baseline as being merely
one of a number of measurements capable of improving predictions of
outcomes and models it in this way.

2

3

e

The final reason that baseline cannot be modeled as the response at time zero is
that many studies have inclusion/exclusion criteria that include cuto↵s on the
baseline variable yielding a truncated distribution. In general it is not appropriate to model the baseline with the same distributional shape as the follow-up
measurements. Thus the approach recommended by Liang and Zeger398 and
Liu et al.416 are problematice .
Gardiner et al.207 compared several longitudinal data models, especially with regard to assumptions and how regression coefficients are estimated. Peters et al.493
have an empirical study confirming that the “use all available data” approach of
likelihood–based longitudinal models makes imputation of follow-up measurements unnecessary.
Keselman et al.340 did a simulation study to study the reliability of AIC for
selecting the correct covariance structure in repeated measurement models. In
choosing from among 11 structures, AIC selected the correct structure 47% of
the time. Gurka et al.242 demonstrated that fixed e↵ects in a mixed e↵ects
model can be biased, independent of sample size, when the specified covariate
matrix is more restricted than the true one.

In addition to this, one of the paper’s conclusions that analysis of covariance is not
appropriate if the population means of the baseline variable are not identical in the
treatment groups is arguable558 . See339 for a discussion of416 .

Chapter 8

Case Study in Data Reduction

The following case study illustrates these techniques:
1. redundancy analysis;
2. variable clustering;
3. data reduction using principal component analysis (PCA), sparse PCA,
and pretransformations;
4. restricted cubic spline fitting using ordinary least squares, in the context
of scaling; and
5. scaling/variable transformations using canonical variates and nonparametric additive regression.

8.1 Data
Consider the 506-patient prostate cancer dataset from Byar and Green.85
The data are listed in [28, Table 46] and are available in ASCII form from
StatLib (lib.stat.cmu.edu) in the Datasets area from this book’s Web
page. These data were from a randomized trial comparing four treatments
for stage 3 and 4 prostate cancer, with almost equal numbers of patients
on placebo and each of three doses of estrogen. Four patients had missing
values on all of the following variables: wt, pf, hx, sbp, dbp, ekg, hg,
bm; two of these patients were also missing sz. These patients are excluded
from consideration. The ultimate goal of an analysis of the dataset might be
to discover patterns in survival or to do an analysis of covariance to assess the
e↵ect of treatment while adjusting for patient heterogeneity. See Chapter 21
for such analyses. The data reductions developed here are general and can
be used for a variety of dependent variables.
The variable names, labels, and a summary of the data are printed below.
r e q u i r e ( Hmisc )

163

164

8 Case Study in Data Reduction

getHdata ( p r o s t a t e ) # Download and make prostate accessible
# Convert an old date format to R format
prostate $ sdate
as.Date ( prostate $ sdate )
d
describe ( prostate [ 2 : 1 7 ] )
latex (d , f i l e=’ ’ )

prostate[2:17]
16 Variables
502 Observations
stage : Stage
n missing unique Info Mean
502
0
2 0.73 3.424
3 (289, 58%), 4 (213, 42%)

rx

n missing unique
502
0
4
placebo (127, 25%), 0.2 mg estrogen (124, 25%)
1.0 mg estrogen (126, 25%), 5.0 mg estrogen (125, 25%)

dtime : Months of Follow-up
n missing unique Info Mean .05 .10
.25
.50
.75
.90
.95
502
0
76
1 36.13 1.05 5.00 14.25 34.00 57.75 67.00 71.00
lowest :

0

1

2

3

4, highest: 72 73 74 75 76

status

n missing unique
502
0
10

alive (148, 29%), dead - prostatic ca (130, 26%)
dead - heart or vascular (96, 19%), dead - cerebrovascular (31, 6%)
dead - pulmonary embolus (14, 3%), dead - other ca (25, 5%)
dead - respiratory disease (16, 3%)
dead - other specific non-ca (28, 6%), dead - unspecified non-ca (7, 1%)
dead - unknown cause (7, 1%)

age : Age in Years
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
501
1
41
1 71.46 56 60 70 73 76 78 80
lowest : 48 49 50 51 52, highest: 84 85 87 88 89

wt : Weight Index = wt(kg)-ht(cm)+200
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
500
2
67
1 99.03 77.95 82.90 90.00 98.00 107.00 116.00 123.00
lowest :

pf

69

71

72

73

74, highest: 136 142 145 150 152

n missing unique
502
0
4
normal activity (450, 90%), in bed < 50% daytime (37, 7%)
in bed > 50% daytime (13, 3%), confined to bed (2, 0%)

hx : History of Cardiovascular Disease
n missing unique Info Sum Mean
502
0
2 0.73 213 0.4243

8.1 Data

165

sbp : Systolic Blood Pressure/10
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
502
0
18 0.98 14.35 11 12 13 14 16 17 18
8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 30
Frequency 1 3 14 27 65 74 98 74 72 34 17 12 3 2 3 1 1 1
%
0 1 3 5 13 15 20 15 14 7 3 2 1 0 1 0 0 0

dbp : Diastolic Blood Pressure/10
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
502
0
12 0.95 8.149 6 6 7 8 9 10 10
4 5 6
7
8 9 10 11 12 13 14 18
Frequency 4 5 43 107 165 94 66 9 5 2 1 1
%
1 1 9 21 33 19 13 2 1 0 0 0

ekg

n missing unique
494
8
7

normal (168, 34%), benign (23, 5%)
rhythmic disturb & electrolyte ch (51, 10%)
heart block or conduction def (26, 5%), heart strain (150, 30%)
old MI (75, 15%), recent MI (1, 0%)

hg : Serum Hemoglobin (g/100ml)
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
502
0
91
1 13.45 10.2 10.7 12.3 13.7 14.7 15.8 16.4
lowest : 5.899 7.000 7.199 7.800 8.199
highest: 17.297 17.500 17.598 18.199 21.199

sz: Size of Primary Tumor (cm2 )
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
497
5
55
1 14.63 2.0 3.0 5.0 11.0 21.0 32.0 39.2
lowest :

0

1

2

3

4, highest: 54 55 61 62 69

sg : Combined Index of Stage and Hist. Grade
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
491
11
11 0.96 10.31 8 8 9 10 11 13 13
5 6 7 8
9 10 11 12 13 14 15
Frequency 3 8 7 67 137 33 114 26 75 5 16
%
1 2 1 14 28 7 23 5 15 1 3

ap : Serum Prostatic Acid Phosphatase
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
502
0
128
1 12.18 0.300 0.300 0.500 0.700 2.975 21.689 38.470
lowest :
0.09999
0.19998
0.29999
0.39996
0.50000
highest: 316.00000 353.50000 367.00000 596.00000 999.87500

bm : Bone Metastases

n missing unique Info Sum Mean
502
0
2 0.41 82 0.1633

stage is defined by ap as well as X-ray results. Of the patients in stage 3,
0.92 have ap  0.8. Of those in stage 4, 0.93 have ap > 0.8. Since stage can
be predicted almost certainly from ap, we do not consider stage in some of
the analyses.

166

8 Case Study in Data Reduction

8.2 How Many Parameters Can Be Estimated?
There are 354 deaths among the 502 patients. If predicting survival time were
of major interest, we could develop a reliable model if no more than about
354/15 = 24 parameters were examined against Y in unpenalized modeling.
Suppose that a full model with no interactions is fitted and that linearity is
not assumed for any continuous predictors. Assuming age is almost linear,
we could fit a restricted cubic spline function with three knots. For the other
continuous variables, let us use five knots. For categorical predictors, the
maximum number of degrees of freedom needed would be one fewer than
the number of categories. For pf we could lump the last two categories since
the last category has only 2 patients. Likewise, we could combine the last
two levels of ekg. Table 8.1 lists the candidate predictors with the maximum
number of parameters we consider for each.
Table 8.1 Degrees of freedom needed for predictors

Predictor:
rx age wt pf hx sbp dbp ekg hg sz sg ap bm
# Parameters: 3 2 4 2 1 4 4 5 4 4 4 4 1

8.3 Redundancy Analysis
As described in Section 4.7.1, it is occasionally useful to do a rigorous redundancy analysis on a set of potential predictors. Let us run the algorithm
discussed there, on the set of predictors we are considering. We will use a low
threshold (0.3) for R2 for demonstration purposes.
# Allow only 1 d.f. for three of the predictors
prostate
transform ( prostate ,
ekg.norm = 1 ⇤ ( ekg %i n% c ( ” normal ” , ” b e n i g n ” ) ) ,
rxn = a s . n u m e r i c ( r x ) ,
pfn = as.numeric ( pf ) )
# Force pfn, rxn to be linear because of difficulty of placing
# knots with so many ties in the data
# Note: all incomplete cases are deleted (inefficient)
redun (⇠ s t a g e + I ( rxn ) + age + wt + I ( p f n ) + hx +
sbp + dbp + ekg.norm + hg + s z + s g + ap + bm,
r 2=. 3 , t y p e= ’ a d j u s t e d ’ , data=p r o s t a t e )
Redundancy Analysis
redun(formula = ⇠stage + I(rxn) + age + wt + I(pfn) + hx + sbp +

8.4 Variable Clustering

167

dbp + ekg.norm + hg + sz + sg + ap + bm, data = prostate,
r2 = 0.3, type = "adjusted")
n: 483

p: 14

nk: 3

Number of NAs:
19
Frequencies of Missing Values Due to Each Variable
stage
I(rxn)
age
wt
I(pfn)
hx
dbp
0
0
1
2
0
0
0
ekg.norm
hg
sz
sg
ap
bm
0
0
5
11
0
0

sbp
0

Transformation of target variables forced to be linear
R2 cutoff: 0.3

Type: adjusted

R2 with which each variable can be predicted from all other variables:
stage

I(rxn)

age

wt

I(pfn)

hx

sbp

0.658
0.417
ekg.norm
0.055

0.000

0.073

0.111

0.156

0.062

0.452

hg
0.146

sz
0.192

sg
0.540

ap
0.147

bm
0.391

dbp

Rendundant variables:
stage sbp bm sg
Predicted from variables:
I(rxn) age wt I(pfn) hx dbp ekg.norm hg sz ap
1
2
3
4

Variable Deleted
stage
sbp
bm
sg

R2 R2 after later deletions
0.658
0.658 0.646 0.494
0.452
0.453 0.455
0.374
0.367
0.342

By any reasonable criterion on R2 , none of the predictors is redundant. stage
can be predicted with an R2 = 0.658 from the other 13 variables, but only
with R2 = 0.493 after deletion of 3 variables later declared to be “redundant.”

8.4 Variable Clustering
From Table 8.1, the total number of parameters is 42, so some data reduction
should be considered. We resist the temptation to take the “easy way out” us-

168

1

8 Case Study in Data Reduction

ing stepwise variable selection so that we can achieve a more stable modeling
process and obtain unbiased standard errors. Before using a variable clustering procedure, note that ap is extremely skewed. To handle skewness, we use
Spearman rank correlations for continuous variables (later we transform each
variable using transcan, which will allow ordinary correlation coefficients to
be used). After classifying ekg as “normal/benign” versus everything else,
the Spearman correlations are plotted below.
x

with ( p r o s t a t e ,
c b i n d ( s t a g e , rx , age , wt , pf , hx , sbp , dbp ,
ekg.norm , hg , sz , sg , ap , bm) )
# If no missing data, could use cor(apply(x, 2, rank))
r
r c o r r ( x , t y p e=” spearman ” ) $ r
# rcorr in Hmisc
maxabsr
max( abs ( r [ row ( r ) != c o l ( r ) ] ) )
p
nrow ( r )
p l o t ( c ( .35 , p+. 5 ) , c ( . 5 , p+. 2 5 ) , t y p e= ’ n ’ , a x e s=FALSE,
x l a b= ’ ’ , y l a b= ’ ’ )
# Figure 8.1
v
dimnames ( r ) [ [ 1 ] ]
t e x t ( r e p ( . 5 , p ) , 1 : p , v , a d j =1)
for ( i in 1:(p 1)) {
f o r ( j i n ( i +1): p ) {
l i n e s ( c ( i , i ) , c ( j , j+r [ i , j ] / maxabsr / 2 ) ,
lwd =3 , l e n d= ’ b u t t ’ )
l i n e s ( c ( i . 2 , i+. 2 ) , c ( j , j ) , lwd =1 , c o l=g r a y ( . 7 ) )
}
t e x t ( i , i , v [ i ] , s r t= 45 , a d j =0)
}

We perform a hierarchical cluster analysis based on a similarity matrix
that contains pairwise Hoe↵ding D statistics.288 D will detect nonmonotonic
associations.
vc

v a r c l u s (⇠ s t a g e + rxn + age + wt + p f n + hx +
sbp + dbp + ekg.norm + hg + s z + s g + ap + bm,
sim= ’ h o e f f d i n g ’ , data=p r o s t a t e )
p l o t ( vc )
# Figure 8.2

We combine sbp and dbp, and tentatively combine ap, sg, sz, and bm.

8.5 Transformation and Single Imputation Using
transcan
Now we turn to the scoring of the predictors to potentially reduce the number of regression parameters that are needed later by doing away with the
need for nonlinear terms and multiple dummy variables. The R Hmisc package transcan function defaults to using a maximum generalized variance
method361 that incorporates canonical variates to optimally transform both
sides of a multiple regression model. Each predictor is treated in turn as a

8.5 Transformation and Single Imputation Using transcan

ap
sg
m
or
sz
n
hg kg.
e
p
db bp
s
hx f
p
t
w ge
a
e
rx tag
s

bm
ap
sg
sz
hg
ekg.norm
dbp
sbp
hx
pf
wt
age
rx
stage

169

Fig. 8.1 Matrix of Spearman ⇢ rank correlation coefficients between predictors. Horizontal gray scale lines correspond to ⇢ = 0. The tallest bar corresponds to |⇢| = 0.78.

0.06
0.08

pfn
sz
bm

0.04

wt
hg

0.02

ekg.norm
age
hx
rxn

0.12

sg
ap

0.14

stage

0.10

sbp
dbp

30 * Hoeffding D

0.00

Fig. 8.2 Hierarchical clustering using Hoe↵ding’s D as a similarity measure. Dummy
variables were used for the categorical variable ekg. Some of the dummy variables
cluster together since they are by definition negatively correlated.

170

8 Case Study in Data Reduction

variable being predicted, and all variables are expanded into restricted cubic
splines (for continuous variables) or dummy variables (for categorical ones).
# Combine 2 levels of ekg (one had freq. 1)
l e v e l s ( p r o s t a t e $ ekg ) [ l e v e l s ( p r o s t a t e $ ekg ) %i n%
c ( ’ o l d MI ’ , ’ r e c e n t MI ’ ) ]

’MI ’

prostate $ pf.coded
a s . i n t e g e r ( prostate $ pf )
# make a numeric version; combine last 2 levels of original
l e v e l s ( prostate $ pf )
l e v e l s ( prostate $ pf ) [ c ( 1 , 2 , 3 , 3 ) ]
ptrans
# Figure 8.3
t r a n s c a n (⇠ s z + s g + ap + sbp + dbp +
age + wt + hg + ekg + p f + bm + hx ,
imputed=TRUE, t r a n s f o r m e d=TRUE, t r a n t a b=TRUE, p l=FALSE,
show.na=TRUE, data=p r o s t a t e , f r a c=. 1 , pr=FALSE)
summary ( p t r a n s , d i g i t s =4)
transcan(x = ⇠sz + sg + ap + sbp + dbp + age + wt + hg + ekg +
pf + bm + hx, imputed = TRUE, trantab = TRUE, transformed = TRUE,
pr = FALSE, pl = FALSE, show.na = TRUE, data = prostate,
frac = 0.1)
Iterations: 8
R2 achieved in predicting each variable:
sz
sg
ap
sbp
dbp
age
wt
hg
ekg
pf
bm
hx
0.207 0.556 0.573 0.498 0.485 0.095 0.122 0.158 0.092 0.113 0.349 0.108
Adjusted R2:
sz
sg
ap
sbp
dbp
age
wt
hg
ekg
pf
bm
hx
0.180 0.541 0.559 0.481 0.468 0.065 0.093 0.129 0.059 0.086 0.331 0.083
Coefficients of canonical variates for predicting each (row) variable
sz
sg
ap
sbp
dbp
sz
0.66 0.20 0.33 0.33
sg
0.23
0.84 0.08 0.07
ap
0.07 0.80
-0.11 -0.05
sbp 0.13 0.10 -0.14
-0.94
dbp 0.13 0.09 -0.06 -0.98
age -0.02 -0.06 0.18 0.58 0.57
wt -0.02 0.06 -0.08 -0.31 0.23
hg
0.13 -0.02 0.03 0.09 0.15
ekg 0.20 -0.38 0.10 0.42 0.12
pf
0.04 0.08 0.02 0.36 0.14
bm -0.02 -0.03 -0.13 0.00 0.00
hx
0.04 0.05 -0.01 -0.04 0.00

age
-0.01
-0.02
0.03
0.14
0.14
0.12
0.33
0.41
-0.03
0.03
-0.06

wt
hg
-0.01 0.11
0.01 -0.01
-0.02 0.01
-0.09 0.03
0.07 0.05
0.14 0.46
0.51
0.43
-0.04 -0.04
0.22 0.29
-0.04 -0.06
0.02 -0.01

ekg
pf
0.11 0.03
-0.07 0.02
0.01 0.00
0.10 0.10
0.03 0.04
0.43 -0.03
-0.06 0.21
-0.02 0.24
0.15
0.13
-0.01 -0.06
-0.09 -0.04

bm
-0.36
-0.20
-0.83
-0.03
0.03
1.05
-1.09
-1.53
-0.42
-1.75

hx
0.34
0.14
-0.03
-0.14
-0.01
-0.76
0.27
-0.12
-1.23
-0.46
-0.02

-0.05

Summary of imputed values
sz
n missing
5
0

unique
4

Info
0.95

Mean
12.86

6 (2, 40%), 7.416 (1, 20%), 20.18 (1, 20%), 24.69 (1, 20%)
sg
n missing unique
Info
Mean
.05
.10
.25
11
0
10
1
10.1
6.900
7.289
7.697
.90
.95
15.000 15.000

.50
10.270

.75
10.560

8.6 Data Reduction Using Principal Components

171

6.511 7.289 7.394 8 10.25 10.27 10.32 10.39 10.73 15
Frequency
1
1
1 1
1
1
1
1
1 2
%
9
9
9 9
9
9
9
9
9 18
age
n missing unique
Info
Mean
1
0
1
0
71.65
wt
n missing unique
Info
Mean
2
0
2
1
97.77
91.24 (1, 50%), 104.3 (1, 50%)
ekg
n missing unique
Info
8
0
4
0.9

Mean
2.625

1 (3, 38%), 3 (3, 38%), 4 (1, 12%), 5 (1, 12%)
Starting estimates for imputed values:
sz
sg
11.0 10.0

ap sbp
0.7 14.0

dbp age
wt
hg
8.0 73.0 98.0 13.7

ekg
1.0

pf
1.0

bm
0.0

hx
0.0

g g p l o t ( p t r a n s , s c a l e=TRUE)

The plotted output is shown in Figure 8.3. Note that at face value the transformation of ap was derived in a circular manner, since the combined index
of stage and histologic grade, sg, uses in its stage component a cuto↵ on ap.
However, if sg is omitted from consideration, the resulting transformation for
ap does not change appreciably. Note that bm and hx are represented as binary
variables, so their coefficients in the table of canonical variable coefficients
are on a di↵erent scale. For the variables that were actually transformed, the
coefficients are for standardized transformed variables (mean 0, variance 1).
From examining the R2 s, age, wt, ekg, pf, and hx are not strongly related
to other variables. Imputations for age, wt, ekg are thus relying more on
the median or modal values from the marginal distributions. From the coefficients of first (standardized) canonical variates, sbp is predicted almost
solely from dbp; bm is predicted mainly from ap, hg, and pf.

8.6 Data Reduction Using Principal Components
The first PC, PC1 , is the linear combination of standardized variables having
maximum variance. PC2 is the linear combination of predictors having the
second largest variance such that PC2 is orthogonal to (uncorrelated with)
PC1 . If there are p raw variables, the first k PCs, where k < p, will explain
only part of the variation in the whole system of p variables unless one or
more of the original variables is exactly a linear combination of the remaining
variables. Note that it is common to scale and center variables to have mean
zero and variance 1 before computing PCs.
The response variable (here, time until death due to any cause) is not
examined during data reduction, so that if PCs are selected by variance

2

172

8 Case Study in Data Reduction

sz

sg

ap

sbp

1.00
0.75
0.50
0.25

R2 = 0.5
R2 = 0.21 5 missing

0.00

Transformed

0

20

40
dbp

R2 = 0.56 11 missing

60

R2 = 0.57

5.0 7.5 10.0 12.5 15.00
age

250 500 750 1000 10 15 20 25 30
wt
hg

1.00
0.75
0.50
0.25
0.00
4

8

12
ekg

16

50

60

70
pf

R2 = 0.16

R2 = 0.12 2 missing

R2 = 0.1 1 missing

R2 = 0.49

80

90

80 100 120 140
bm

10

15
hx

20

1.00
0.75
0.50

R2 = 0.35

R2 = 0.11

R2 = 0.11

0.25

R2 = 0.09 8 missing

0.00
2

4

6 1.0 1.5 2.0 2.5 3.00.00 0.25 0.50 0.75 1.00
0.00 0.25 0.50 0.75 1.00

Fig. 8.3 Simultaneous transformation and single imputation of all candidate predictors using transcan. Imputed values are shown as red plus signs. Transformed
values are arbitrarily scaled to [0, 1].

explained in the X-space and not by variation explained in Y , one needn’t
correct for model uncertainty or multiple comparisons.
PCA results in data reduction when the analyst uses only a subset of the
p possible PCs in predicting Y . This is called incomplete principal component
regression . When one sequentially enters PCs into a predictive model in a
strict pre-specified order (i.e., by descending amounts of variance explained
for the system of p variables), model uncertainty requiring bootstrap adjustment is minimized. In contrast, model uncertainty associated with stepwise
regression (driven by associations with Y ) is massive.

concerned only with
overfitting in terms of
over-transforming
(maybe we should
come up with a
penalization)

8.6 Data Reduction Using Principal Components

173

For the prostate dataset, consider PCs on raw candidate predictors, expanding polytomous factors using dummy variables. The R function princomp
is used, after singly imputing missing raw values using transcan’s optimal
additive nonlinear models. In this series of analyses we ignore the treatment
variable, rx.
# Impute all missing values in all variables given to transcan
imputed
impute ( p t r a n s , data=p r o s t a t e , l i s t . o u t =TRUE)

Imputed missing values with the following frequencies
and stored them in variables with their original names:
sz
5

sg age
11
1

imputed

wt ekg
2
8

a s . d a t a . f r a m e ( imputed )

# Compute principal components on imputed data.
# Create a design matrix from ekg categories
Ekg
m o d e l . m a t r i x (⇠ ekg , data=imputed ) [ ,
1]
# Use correlation matrix
pfn
p r o s t a t e $ pfn
prin.raw
princomp (⇠ s z + s g + ap + sbp + dbp + age +
wt + hg + Ekg + p f n + bm + hx ,
c o r=TRUE, data=imputed )
p l o t ( p r i n . r a w , t y p e= ’ l i n e s ’ , main= ’ ’ , y l i m=c ( 0 , 3 ) ) #Figure 8.4
# Add cumulative fraction of variance explained
addscree
f u n c t i o n ( x , npcs=min ( 1 0 , l e n g t h ( x$ s d e v ) ) ,
p l o t v=FALSE,
c o l =1 , o f f s e t=. 8 , a d j =0 , pr=FALSE) {
vars
x$ s d e v ^ 2
cumv
cumsum ( v a r s ) /sum ( v a r s )
i f ( pr ) p r i n t ( cumv )
t e x t ( 1 : npcs , v a r s [ 1 : npcs ] + o f f s e t ⇤ par ( ’ cxy ’ ) [ 2 ] ,
a s . c h a r a c t e r ( round ( cumv [ 1 : npcs ] , 2 ) ) ,
s r t =45 , a d j=adj , c e x=. 6 5 , xpd=NA, c o l=c o l )
i f ( p l o t v ) l i n e s ( 1 : npcs , v a r s [ 1 : npcs ] , t y p e= ’ b ’ , c o l=c o l )
}
addscree ( prin.raw )
prin.trans
princomp ( p t r a n s $ t r a n s f o r m e d , c o r=TRUE)
a d d s c r e e ( p r i n . t r a n s , npcs =10 , p l o t v=TRUE, c o l= ’ r e d ’ ,
o f f s e t= .8 , a d j =1)

The resulting plot shown in Figure 8.4 is called a “scree” plot [318, pp. 96–99,
104, 106]. It shows the variation explained by the first k principal components
as k increases all the way to 16 parameters (no data reduction). It requires
10 of the 16 possible components to explain > 0.8 of the variance, and the
first 5 components explain 0.49 of the variance of the system. Two of the 16
dimensions are almost totally redundant.
After repeating this process when transforming all predictors via transcan,
we have only 12 degrees of freedom for the 12 predictors. The variance ex-

174

8 Case Study in Data Reduction

3.0

●

●

●

●

Comp.3

Comp.5

Comp.7

0.
95

0.0
Comp.1

●
●

0.
91

85
0.

79
0.

73
0.

0.

66

●
●

0.
8

0.
7

●

●

0.
75

0.

●

59
0.

0.5

63

56

●
●

0.

1.0

0.

0.
49

●
●

5

●

0.

42

0.

●

1.5

35

0.

2.0

0.
38

Variances

26

0.

23

0.

15

●

2.5

Comp.9

Fig. 8.4 Variance of the system of raw predictors (black) explained by individual
principal components (lines) along with cumulative proportion of variance explained
(text), and variance explained by components computed on transcan-transformed
variables (red)

plained is depicted in Figure 8.4 in red. It requires at least 9 of the 12 possible
components to explain 0.9 of the variance, and the first 5 components explain 0.66 of the variance as opposed to 0.49 for untransformed variables.
Let us see how the PCs “explain” the times until death using the Cox regression129 function from rms, cph, described in Chapter 20. In what follows
we vary the number of components used in the Cox models from 1 to all 16,
computing the AIC for each model. AIC is related to model log likelihood
penalized for number of parameters estimated, and lower is better. For reference, the AIC of the model using all of the original predictors, and the AIC
of a full additive spline model are shown as horizontal lines.
r e q u i r e ( rms )
S
with ( p r o s t a t e , Surv ( dtime , s t a t u s != ” a l i v e ” ) )
# two-column response var.
pcs
prin.raw $ scores
# pick off all PCs
aic
numeric ( 1 6 )
for ( i in 1:16) {
ps
pcs [ , 1 : i ]
aic [ i ]
AIC ( cph ( S ⇠ ps ) )
}
# Figure 8.5
p l o t ( 1 : 1 6 , a i c , x l a b= ’ Number o f Components Used ’ ,
y l a b= ’ AIC ’ , t y p e= ’ l ’ , y l i m=c ( 3 9 5 0 , 4 0 0 0 ) )

8.6 Data Reduction Using Principal Components

175

cph ( S ⇠ s z + s g + l o g ( ap ) + sbp + dbp + age + wt + hg +
ekg + p f + bm + hx , data=imputed )
a b l i n e ( h=AIC ( f ) , c o l= ’ b l u e ’ )
f
cph ( S ⇠ r c s ( sz , 5 ) + r c s ( sg , 5 ) + r c s ( l o g ( ap ) , 5 ) + r c s ( sbp , 5 ) +
r c s ( dbp , 5 ) + r c s ( age , 3 ) + r c s ( wt , 5 ) + r c s ( hg , 5 ) +
ekg + p f + bm + hx ,
t o l =1e 14 , data=imputed )
a b l i n e ( h=AIC ( f ) , c o l= ’ b l u e ’ , l t y =2)
f

4000

3990

3980

AIC

incomplete principle component
in the order of variation explained
-including some of the PCs

3970

3960

3950
5

10

15

Number of Components Used
Fig. 8.5 AIC of Cox models fitted with progressively more principal components.
The solid blue line depicts the AIC of the model with all original covariates. The
dotted blue line is positioned at the AIC of the full spline model.

For the money, the first 5 components adequately summarizes all variables,
if linearly transformed, and the full linear model is no better than this. The
model allowing all continuous predictors to be nonlinear is not worth its
added degrees of freedom.
Next check the performance of a model derived from cluster scores of
transformed variables.
# Compute PC1 on a subset of transcan-transformed predictors
pco
function (v) {
f
princomp ( p t r a n s $ t r a n s f o r m e d [ , v ] , c o r=TRUE)
vars
f $ s d e v^2
c a t ( ’ F r a c t i o n o f v a r i a n c e e x p l a i n e d by PC1 : ’ ,
round ( v a r s [ 1 ] /sum ( v a r s ) , 2 ) , ’ \n ’ )
f $ scores [ ,1]
}
tumor
pco ( c ( ’ s z ’ , ’ s g ’ , ’ ap ’ , ’bm ’ ) )

176

8 Case Study in Data Reduction

Fraction of variance explained by PC1: 0.59

bp

pco ( c ( ’ sbp ’ , ’ dbp ’ ) )

Fraction of variance explained by PC1: 0.84

cardiac

pco ( c ( ’ hx ’ , ’ ekg ’ ) )

Fraction of variance explained by PC1: 0.61

# Get transformed individual variables that are not clustered
other
p t r a n s $ t r a n s f o r m e d [ , c ( ’ hg ’ , ’ age ’ , ’ p f ’ , ’ wt ’ ) ]
f
cph ( S ⇠ tumor + bp + c a r d i a c + o t h e r ) # other is a matrix
AIC ( f )
[1] 3954.393

p r i n t ( f , l a t e x=TRUE, l o n g=FALSE,

t i t l e=’ ’ )

Model Tests
Obs
502 LR 2
81.11
Events 354 d.f.
7
Center 0 Pr(> 2 ) 0.0000
Score 2 86.81
Pr(> 2 ) 0.0000

tumor
bp
cardiac
hg
age
pf
wt

Coef
-0.1723
-0.0251
-0.2513
-0.1407
-0.1034
-0.0933
-0.0910

Discrimination
Indexes
R2
0.149
Dxy
0.286
g
0.562
gr
1.755

S.E. Wald Z Pr(> |Z|)
0.0367
-4.69 < 0.0001
0.0424
-0.59
0.5528
0.0516
-4.87 < 0.0001
0.0554
-2.54
0.0111
0.0579
-1.79
0.0739
0.0487
-1.92
0.0551
0.0555
-1.64
0.1012

The tumor and cardiac clusters seem to dominate prediction of mortality,
and the AIC of the model built from cluster scores of transformed variables
compares favorably with other models (Figure 8.5).

8.6.1 Sparse Principal Components
A disadvantage of principal components is that every predictor receives a
nonzero weight for every component, so many coefficients are involved even

8.6 Data Reduction Using Principal Components

177

through the e↵ective degrees of freedom with respect to the response model
are reduced. Sparse principal components 666 uses a penalty function to reduce
the magnitude of the loadings variables receive in the components. If an L1
penalty is used (as with the lasso), some loadings are shrunk to zero, resulting in some simplicity. Sparse principal components combines some elements
of variable clustering, scoring of variables within clusters, and redundancy
analysis.
Filzmoser, Fritz, and Kalcher187 have written a nice R package pcaPP for
doing sparse PC analysis. The following example uses the prostate data again.
To allow for nonlinear transformations and to score the ekg variable in the
prostate dataset down to a scalar, we use the transcan-transformed predictors as inputs.
r e q u i r e ( pcaPP )
s

sPCAgrid ( p t r a n s $ t r a n s f o r m e d , k =10 , method= ’ sd ’ ,
c e n t e r=mean , s c a l e=sd , s c o r e s=TRUE,
m a x i t e r =10)
p l o t ( s , t y p e= ’ l i n e s ’ , main= ’ ’ , y l i m=c ( 0 , 3 ) )
# Figure 8.6
addscree ( s )
s$ loadings
# These loadings are on the orig. transcan scale
Loadings:
Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Comp.10
sz
0.248
0.950
sg
0.620
0.522
ap
0.634
-0.305
sbp
-0.707
dbp
0.707
age
1.000
wt
1.000
hg
1.000
ekg
1.000
pf
1.000
bm -0.391
0.852
hx
1.000
Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9
1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000
0.083 0.083 0.083 0.083 0.083 0.083 0.083 0.083 0.083
0.083 0.167 0.250 0.333 0.417 0.500 0.583 0.667 0.750
Comp.10
SS loadings
1.000
Proportion Var
0.083
Cumulative Var
0.833
SS loadings
Proportion Var
Cumulative Var

Only nonzero loadings are shown. The first sparse PC is the tumor cluster
used above, and the second is the blood pressure cluster. Let us see how well
incomplete sparse principal component regression predicts time until death.
pcs
s$ scores
# pick off sparse PCs
aic
numeric ( 1 0 )
for ( i in 1:10) {
ps
pcs [ , 1 : i ]
aic [ i ]
AIC ( cph ( S ⇠ ps ) )
}
# Figure 8.7

because they are
sparse, they are no
longer orthogonal

<-these match the
variable clustering
results.
<- this has fewer
decisions as compared
to the variable clustering
option, so this is "better"
in the sense that it
doesn't allow for
spurious decisions.

178

8 Case Study in Data Reduction
3.0
2

<- figure out the variance
explained is different from
the output above.

0.

2.5

35

2.0

0.

●
0.
79

0.
88

●

●

●

●

●

●

1

0.
7

●

0.
95

0.
61

1.0

0.
53

1.5
0.
44

Variances

●

9

10

0.5

●

0.0
1

2

3

4

5

6

7

8

Fig. 8.6 Variance explained by individual sparse principal components (lines) along
with cumulative proportion of variance explained (text)

p l o t ( 1 : 1 0 , a i c , x l a b= ’ Number o f Components Used ’ ,
y l a b= ’ AIC ’ , t y p e= ’ l ’ , y l i m=c ( 3 9 5 0 , 4 0 0 0 ) )

More components are required to optimize AIC than were seen in Figure 8.5,
but a model built from 6–8 sparse PCs performed as well as the other models.

8.7 Transformation Using Nonparametric Smoothers
The ACE nonparametric additive regression method of Breiman and Friedman67 transforms both the left-hand-side variable and all the right-hand-side
variables so as to optimize R2 . ACE can be used to transform the predictors
using the R ace function in the acepack package, called by the transace
function in the Hmisc package. transace does not impute data but merely
does casewise deletion of missing values. Here transace is run after single
imputation by transcan. binary is used to tell transace which variables not
to try to predict (because they need no transformation). Several predictors
are restricted to be monotonically transformed.
x

with ( imputed ,
c b i n d ( sz , sg , ap , sbp , dbp , age , wt , hg , ekg , pf ,
bm, hx ) )
monotonic
c ( ” s z ” , ” s g ” , ” ap ” , ” sbp ” , ”dbp” , ” age ” , ” p f ” )
t r a n s a c e ( x , monotonic ,
# Figure 8.8

8.8 Further Reading

179

4000

3990

AIC

3980

3970

3960

3950
2

4

6

8

10

Number of Components Used
Fig. 8.7 Performance of sparse principal components in Cox models

c a t e g o r i c a l=” ekg ” , b i n a r y=c ( ”bm” , ”hx” ) )
R2 achieved in predicting each variable:
sz
sg
ap
sbp
dbp
age
wt
hg
0.2265824 0.5762743 0.5717747 0.4823852 0.4580924 0.1514527 0.1732244 0.2001008
ekg
pf
bm
hx
0.1110709 0.1778705
NA
NA

Except for ekg, age, and for arbitrary sign reversals, the transformations in
Figure 8.8 determined using transace were similar to those in Figure 8.3.
The transcan transformation for ekg makes more sense.

8.8 Further Reading
1
2

Sauerbrei and Schumacher536 used the bootstrap to demonstrate the variability
of a standard variable selection procedure for the prostate cancer dataset.
Schemper and Heinze546 used logistic models to impute dichotomizations of the
predictors for this dataset.

180

3
2
1
0
−1

8 Case Study in Data Reduction

●
● ●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0

●

1.5
1.0
0.5
0.0
−0.5
−1.0
−1.5

●

●

10 20 30 40 50 60 70

●●

●●

●●

10

●●

●

15

●●

●

● ●
●
●●●

6

8

●●

20

●

25

8
6
4
2
0
−2

10

100

30

120

●●
● ●●
●
●

140

● ●
● ●

4

6

●

8

1
0
−1
−2
−3
−4
−5
−6

0

200

400

●

●

●

●

●

600

800 1000

ap
●

1
0
−1
−2
−3

●

10 12 14 16 18

●●
●●●●
●●●●
●●
●●
●
●
●●
●●●
●●●
●●●
●
●
●
●●●
●●●
●●●

50

60

●

15

70

80

●●●

90

age

●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●

10

wt
4
3
2
1
0

14

●
●

●
●
●● ● ●●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

dbp

●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●

80

12

3
2
1
0
−1

●

sbp
2
1
0
−1
−2
−3

●

sg
●

●
●●

●

●
●
●
●
●
●

sz
8
6
4
2
0
−2

●

1.5
1.0
0.5
0.0
−0.5
−1.0

20

hg

●
●
●
●
●
●

1

2

3

4

5

6

ekg

●

●

●

1.0

1.5

2.0

2.5

3.0

pf
Fig. 8.8 Simultaneous transformation of all variables using ACE.

8.9 Problems
The Mayo Clinic conducted a randomized trial in primary biliary cirrhosis
(PBC) of the liver between January 1974 and May 1984, to compare Dpenicillamine with placebo. The drug was found to be ine↵ective [193, p. 2],
and the trial was done before liver transplantation was common, so this trial
constitutes a natural history study for PBC. Followup continued through
July, 1986. For the 19 patients that did undergo transplant, followup time
was censored (status=0) at the day of transplant. 312 patients were randomized, and another 106 patients were entered into a registry. The nonrandomized patients have most of their laboratory values missing, except for
bilirubin, albumin, and prothrombin time. 28 randomized patients had both
serum cholesterol and triglycerides missing. The data, which consist of clinical, biochemical, serologic, and histologic information, are listed in [193, pp.

If you want to allow 8 variables to interact, you can collapse those 8 interaction terms in PCs.
Take products involving the principle components and not the raw variables.

8.9 Problems

181

359–375]. The PBC data are discussed and analyzed in [193, pp. 2–7, 102–
104, 153–162], [155], [7] (a tree-based analysis which on its p. 480 mentions
some possible lack of fit of the earlier analyses), and [354]. The data are stored
in the datasets web site so may be accessed using the Hmisc getHdata function with argument pbc. Use only the data on randomized patients for all
analyses. For Problems 1–6, ignore followup time, status, and drug.
1. Do an initial variable clustering based on ranks, using pairwise deletion of
missing data. Comment on the potential for one-dimensional summaries of
subsets of variables being adequate summaries of prognostic information.
2. cholesterol, triglycerides, platelets, and copper are missing on
some patients. Impute them using a method you recommend. Use some
or all of the remaining predictors and possibly the outcome. Provide a
correlation coefficient describing the usefulness of each imputation model.
Provide the actual imputed values, specifying observation numbers. For
all later analyses, use imputed values for missing values.
3. Perform a scaling/transformation analysis to better measure how the
predictors interrelate and to possibly pretransform some of them. Use
transcan or ACE. Repeat the variable clustering using the transformed
scores and Pearson correlation or using an oblique rotation principal component analysis. Determine if the correlation structure (or variance explained by the first principal component) indicates whether it is possible
to summarize multiple variables into single scores.
4. Do a principal component analysis of all transformed variables simultaneously. Make a graph of the number of components versus the cumulative proportion of explained variation. Repeat this for laboratory variables
alone.
5. Repeat the overall PCA using sparse principal components. Pay attention to how best to solve for sparse components, e.g., consider the lambda
parameter in sPCAgrid.
6. How well can variables (lab and otherwise) that are routinely collected
(on nonrandomized patients) capture the information (variation) of the
variables that are often missing? It would be helpful to explore the strength
of interrelationships by
a. correlating two PC1 s obtained from untransformed variables,
b. correlating two PC1 s obtained from transformed variables,
c. correlating the best linear combination of one set of variables with the
best linear combination of the other set, and
d. doing the same on transformed variables.
For this problem consider only complete cases, and transform the 5 nonnumeric categorical predictors to binary 0–1 variables.
7. Consider the patients having complete data who were randomized to
placebo. Consider only models that are linear in all the covariates.

182

8 Case Study in Data Reduction

a. Fit a survival model to predict time of death using the following
covariates: bili, albumin, stage, protime, age, alk.phos, sgot,
chol, trig, platelet, copper.
b. Perform an ordinary principal component analysis. Fit the survival
model using only the first 3 PCs. Compare the likelihood ratio 2 and
AIC with that of the model using the original variables.
c. Considering the PCs are fixed, use the bootstrap to estimate the 0.95
confidence interval of the inter-quartile-range age e↵ect on the original
scale, and the same type of confidence interval for the coefficient of PC1 .
d. Now accounting for uncertainty in the PCs, compute the same two
confidence intervals. Compare and interpret the two sets. Take into
account the fact that PCs are not unique to within a sign change.
R programming hints for this exercise are found on the course web site.

Chapter 9

Overview of Maximum Likelihood
Estimation

9.1 General Notions—Simple Cases
In ordinary least squares multiple regression, the objective in fitting a model
is to find the values of the unknown parameters that minimize the sum of
squared errors of prediction. When the response variable is non-normal, polytomous, or not observed completely, one needs a more general objective function to optimize.
Maximum likelihood (ML) estimation is a general technique for estimating parameters and drawing statistical inferences in a variety of situations,
especially nonstandard ones. Before laying out the method in general, ML
estimation is illustrated with a standard situation, the one-sample binomial
problem. Here, independent binary responses are observed and one wishes to
draw inferences about an unknown parameter, the probability of an event in
a population.
Suppose that in a population of individuals, each individual has the same
probability P that an event occurs. We could also say that the event has
already been observed, so that P is the prevalence of some condition in the
population. For each individual, let Y = 1 denote the occurrence of the
event and Y = 0 denote nonoccurrence. Then Prob{Y = 1} = P for each
individual. Suppose that a random sample of size 3 from the population is
drawn and that the first individual had Y = 1, the second had Y = 0, and the
third had Y = 1. The respective probabilities of these outcomes are P , 1 P ,
and P . The joint probability of observing the independent events Y = 1, 0, 1
is P (1 P )P = P 2 (1 P ). Now the value of P is unknown, but we can solve
for the value of P that makes the observed data (Y = 1, 0, 1) most likely
to have occurred. In this case, the value of P that maximizes P 2 (1 P ) is
P = 2/3. This value for P is the maximum likelihood estimate (MLE) of the
population probability.
Let us now study the situation of independent binary trials in general. Let
the sample size be n and the observed responses be Y1 , Y2 , . . . , Yn . The joint

183

184

9 Overview of Maximum Likelihood Estimation

probability of observing the data is given by
L=

n
Y

P Yi (1

P )1

Yi

.

(9.1)

i=1

Now let s denote the sum of the Y s or the number of times that the event
occurred (Yi = 1), that is the number of “successes.” The number of nonoccurrences (“failures”) is n s. The likelihood of the data can be simplified
to
L = P s (1 P )n s .
(9.2)
It is easier to work with the log likelihood function, which also has desirable
statistical properties. For the one-sample binary response problem, the log
likelihood is
log L = s log(P ) + (n s) log(1 P ).
(9.3)
The MLE of P is that value of P that maximizes L or log L. Since log L
is a smooth function of P , its maximum value can be found by finding the
point at which log L has a slope of 0. The slope or first derivative of log L,
with respect to P , is
U (P ) = @ log L/@P = s/P

(n

s)/(1

P ).

(9.4)

The first derivative of the log likelihood function with respect to the parameter(s), here U (P ), is called the score function. Equating this function to zero
requires that s/P = (n s)/(1 P ). Multiplying both sides of the equation
by P (1 P ) yields s(1 P ) = (n s)P or that s = (n s)P + sP = nP .
Thus the MLE of P is p = s/n.
Another important function is called the Fisher information about the
unknown parameters. The information function is the expected value of the
negative of the curvature in log L, which is the negative of the slope of the
slope as a function of the parameter, or the negative of the second derivative
of log L. Motivation for consideration of the Fisher information is as follows.
If the log likelihood function has a distinct peak, the sample provides information that allows one to readily discriminate between a good parameter
estimate (the location of the obvious peak) and a bad one. In such a case the
MLE will have good precision or small variance. If on the other hand the likelihood function is relatively flat, almost any estimate will do and the chosen
estimate will have poor precision or large variance. The degree of peakedness
of a function at a given point is the speed with which the slope is changing at
that point, that is, the slope of the slope or second derivative of the function
at that point.
Here, the information is
I(P ) = E{ @ 2 log L/@P 2 }
= E{s/P 2 + (n

s)/(1

P )2 }

(9.5)

9.1 General Notions—Simple Cases

= nP/P 2 + n(1

185

P )/(1

P )2 = n/[P (1

P )].

We estimate the information by substituting the MLE of P into I(P ), yielding
I(p) = n/[p(1 p)].
Figures 9.1, 9.2, and 9.3 depict, respectively, log L, U (P ), and I(P ), all
as a function of P . Three combinations of n and s were used in each graph.
These combinations correspond to p = .5, .6, and .6, respectively.

Log Likelihood

−20
−40
−60
−80
−100
s=50 n=100
s=60 n=100
s=12 n=20

−120
−140

0.0

0.2

0.4

0.6

0.8

1.0

P
Fig. 9.1 log likelihood functions for three one-sample binomial problems

In each case it can be seen that the value of P that makes the data most
likely to have occurred (the value that maximizes L or log L) is p given
above. Also, the score function (slope of log L) is zero at P = p. Note that
the information function I(P ) is highest for P approaching 0 or 1 and is
lowest for P near .5, where there is maximum uncertainty about P . Note
also that while log L has the same shape for the s = 60 and s = 12 curves
in Figure 9.1, the range of log L is much greater for the larger sample size.
Figures 9.2 and 9.3 show that the larger sample size produces a sharper
likelihood. In other words, with larger n, one can zero in on the true value of
P with more precision.
In this binary response one-sample example let us now turn to inference
about the parameter P . First, we turn to the estimation of the variance of the
MLE, p. An estimate of this variance is given by the inverse of the information
at P = p:
V ar(p) = I(p) 1 = p(1 p)/n.
(9.6)

186

9 Overview of Maximum Likelihood Estimation
600
400

Score

200
0
−200
s=50 n=100
s=60 n=100
s=12 n=20

−400

0.0

0.2

0.4

0.6

0.8

1.0

0.8

1.0

P
Fig. 9.2 Score functions (@L/@P )

1200

Information

1000
n=100
n=20

800
600
400
200

0.0

0.2

0.4

0.6

P
Fig. 9.3 Information functions ( @ 2 log L/@P 2 )

9.2 Hypothesis Tests

187

Note that the variance is smallest when the information is greatest (p = 0 or
1).
The variance estimate forms a basis for confidence limits on the unknown
parameter. For large n, the MLE p is approximately normally distributed
with expected value (mean) P and variance P (1 P )/n. Since p(1 p) is a
consistent estimate of P (1 P )/n, it follows that p ± z[p(1 p)/n]1/2 is an
approximate 1 ↵ confidence interval for P if z is the 1 ↵/2 critical value
of the standard normal distribution.

9.2 Hypothesis Tests
Now let us turn to hypothesis tests about the unknown population parameter
P — H0 : P = P0 . There are three kinds of statistical tests that arise from
likelihood theory.

9.2.1 Likelihood Ratio Test
This test statistic is the ratio of the likelihood at the hypothesized parameter
values to the likelihood of the data at the maximum (i.e., at parameter values
= MLEs). It turns out that 2⇥ the log of this likelihood ratio has desirable
statistical properties. The likelihood ratio test statistic is given by
LR =
=

2 log(L at H0 /L at MLEs)
2(log L at H0 )

[ 2(log L at MLEs)].

(9.7)

The LR statistic, for large enough samples, has approximately a 2 distribution with degrees of freedom equal to the number of parameters estimated,
if the null hypothesis is “simple,” that is, doesn’t involve any unknown parameters. Here LR has 1 d.f.
The value of log L at H0 is
log L(H0 ) = s log(P0 ) + (n

s) log(1

P0 ).

(9.8)

p).

(9.9)

The maximum value of log L (at MLEs) is
log L(P = p) = s log(p) + (n

s) log(1

For the hypothesis H0 : P = P0 , the test statistic is
LR =

2{s log(P0 /p) + (n

s) log[(1

P0 )/(1

p)]}.

(9.10)

188

9 Overview of Maximum Likelihood Estimation

Note that when p happens to equal P0 , LR = 0. When p is far from P0 , LR will
be large. Suppose that P0 = 1/2, so that H0 is P = 1/2. For n = 100, s = 50,
LR = 0. For n = 100, s = 60,
LR =

2{60 log(.5/.6) + 40 log(.5/.4)} = 4.03.

(9.11)

2{12 log(.5/.6) + 8 log(.5/.4)} = .81 = 4.03/5.

(9.12)

For n = 20, s = 12,
LR =

Therefore, even though the best estimate of P is the same for these two cases,
the test statistic is more impressive when the sample size is five times larger.

9.2.2 Wald Test
The Wald test statistic is a generalization of a t- or z-statistic. It is a function
of the di↵erence in the MLE and its hypothesized value, normalized by an
estimate of the standard deviation of the MLE. Here the statistic is
W = [p

P0 ]2 /[p(1

p)/n].

(9.13)

For large enough n, W is distributed as 2 with 1 d.f. For n = 100, s = 50,
W = 0. For the other samples, W is, respectively, 4.17 and 0.83 (note 0.83 =
4.17/5).
p
Many statistical packages treat W as having a t distribution instead of
a normal distribution. As pointed out by Gould,223 there is no basis for this
outside of ordinary linear modelsa .

9.2.3 Score Test
If the MLE happens to equal the hypothesized value P0 , P0 maximizes the
likelihood and so U (P0 ) = 0. Rao’s score statistic measures how far from
zero the score function is when evaluated at the null hypothesis. The score
function (slope or first derivative of log L) is normalized by the information
(curvature or second derivative of log L). The test statistic for our example
is
S = U (P0 )2 /I(P0 ),
(9.14)
a

In linear regression, a t distribution is used to penalize for the fact that the variance
of Y |X is estimated. In models such as the logistic model, there is no separate variance parameter to estimate. Gould has done simulations that show that the normal
distribution provides more accurate P -values than the t for binary logistic regression.

9.2 Hypothesis Tests

189

which formally does not involve the MLE, p. The statistic can be simplified
as follows.
U (P0 ) = s/P0

(n

s)/(1

P0 )

s/P02

+ (n

s)/(1

P0 ) 2

I(P0 ) =

S = (s

2

nP0 ) /[nP0 (1

(9.15)
2

P0 )] = n(p

P0 ) /[P0 (1

P0 )].

Note that the numerator of S involves s nP0 , the di↵erence between the
observed number of successes and the number of successes expected under
H0 .
As with the other two test statistics, S = 0 for the first sample. For the
last two samples S is, respectively, 4 and .8 = 4/5.

9.2.4 Normal Distribution—One Sample
Suppose that a sample of size n is taken from a population for a random
variable Y that is known to be normally distributed with unknown mean
µ and variance 2 . Denote the observed values of the random variable by
Y1 , Y2 , . . . , Yn . Now unlike the binary response case (Y = 0 or 1), we cannot
use the notion of the probability that Y equals an observed value. This is
because Y is continuous and the probability that it will take on a given value
is zero. We substitute the density function for the probability. The density at
a point y is the limit as d approaches zero of
Prob{y < Y  y + d}/d = [F (y + d)

F (y)]/d,

(9.16)

where F (y) is the normal cumulative distribution function (for a mean of µ
and variance of 2 ). The limit of the right-hand side of the above equation as
d approaches zero is f (y), the density function of a normal distribution with
mean µ and variance 2 . This density function is
f (y) = (2⇡

2

)

1/2

exp{ (y

µ)2 /2

2

}.

(9.17)

The likelihood of observing the observed sample values is the joint density
of the Y s. The log likelihood function here is a function of two unknowns, µ
and 2 .
n
X
log L = .5n log(2⇡ 2 ) .5
(Yi µ)2 / 2 .
(9.18)
i=1

It can be shown that the value of µ that maximizes log L is the value that
minimizes the sum of squared deviations about µ, which is the sample mean
Y . The MLE of 2 is
n
X
s2 =
(Yi Y )2 /n.
(9.19)
i=1

1

190

9 Overview of Maximum Likelihood Estimation

Recall that the sample variance uses n 1 instead of n in the denominator. It
can be shown that the expected value of the MLE of 2 , s2 , is [(n 1)/n] 2 ;
in other words, s2 is too small by a factor of (n 1)/n on the average. The
sample variance is unbiased, but being unbiased does not necessarily make
it a better estimator. The MLE has greater precision (smaller mean squared
error) in many cases.

9.3 General Case
Suppose we need to estimate a vector of unknown parameters B = {B1 , B2 ,
. . . , Bp } from a sample of size n based on observations Y1 , . . . , Yn . Denote the
probability or density function of the random variable Y for the ith observation by fi (y; B). The likelihood for the ith observation is Li (B) = fi (Yi ; B).
In the one-sample binary response case, recall that Li (B) = Li (P ) =
P Yi [1 P ]1 Yi . The likelihood function, or joint likelihood of the sample,
is given by
n
Y
L(B) =
fi (Yi ; B).
(9.20)
i=1

The log likelihood function is

log L(B) =

n
X

log Li (B).

(9.21)

i=1

The MLE of B is that value of the vector B that maximizes log L(B) as
a function of B. In general, the solution for B requires iterative trial-anderror methods as outlined later. Denote the MLE of B as b = {b1 , . . . , bp }.
The score vector is the vector of first derivatives of log L(B) with respect to
B1 , . . . , B p :
U (B) = {@/@B1 log L(B), . . . , @/@Bp log L(B)}
= (@/@B) log L(B).

(9.22)

The Fisher information matrix is the p ⇥ p matrix whose elements are the
negative of the expectation of all second partial derivatives of log L(B):
I ⇤ (B) =
=

{E[(@ 2 log L(B)/@Bj @Bk )]}p⇥p

E{(@ 2 /@B@B 0 ) log L(B)}.

(9.23)

The observed information matrix I(B) is I ⇤ (B) without taking the expectation. In other words, observed values remain in the second derivatives:
I(B) =

(@ 2 /@B@B 0 ) log L(B).

(9.24)

9.3 General Case

191

This information matrix is often estimated from the sample using the estimated observed information I(b), by inserting b, the MLE of B, into the
formula for I(B).
Under suitable conditions, which are satisfied for most situations likely
to be encountered, the MLE b for large samples is an optimal estimator
(has as great a chance of being close to the true parameter as all other
types of estimators) and has an approximate multivariate normal distribution
with mean vector B and variance–covariance matrix I ⇤ 1 (B), where C 1
denotes the inverse of the matrix C. (C 1 is the matrix such that C 1 C is
the identity matrix, a matrix with ones on the diagonal and zeros elsewhere.
If C is a 1 ⇥ 1 matrix, C 1 = 1/C.) A consistent estimator of the variance–
covariance matrix is given by the matrix V , obtained by inserting b for B in
I(B) : V = I 1 (b) .

9.3.1 Global Test Statistics
Suppose we wish to test the null hypothesis H0 : B = B 0 . The likelihood
ratio test statistic is
LR =
=

2 log(L at H0 /L at MLEs)
2[log L(B 0 )

log L(b)].

(9.25)

The corresponding Wald test statistic, using the estimated observed information matrix, is
W = (b

B 0 )0 I(b)(b

B 0 ) = (b

B 0 )0 V

1

(b

B 0 ).

(9.26)

(A quadratic form a0 V a is a matrix generalization of a2 V .) Note that if the
number of estimated parameters is p = 1, W reduces to (b B 0 )2 /V , which
is the square of a z- or t-type statistic (estimate hypothesized value divided
by estimated standard deviation of estimate).
The score statistic for H0 is
S = U 0 (B 0 )I

1

(B 0 )U (B 0 ).

(9.27)

Note that as before, S does not require solving for the MLE. For large samples, LR, W , and S have a 2 distribution with p d.f. under suitable conditions.

192

9 Overview of Maximum Likelihood Estimation

9.3.2 Testing a Subset of the Parameters
Let B = {B1 , B2 } and suppose that we wish to test H0 : B1 = B10 . We
are treating B2 as a nuisance parameter. For example, we may want to test
whether blood pressure and cholesterol are risk factors after adjusting for
confounders age and sex. In that case B1 is the pair of regression coefficients
for blood pressure and cholesterol and B2 is the pair of coefficients for age
and sex. B2 must be estimated to allow adjustment for age and sex, although
B2 is a nuisance parameter and is not of primary interest.
Let the number of parameters of interest be k so that B1 is a vector of
length k. Let the number of “nuisance” or “adjustment” parameters be q, the
length of B2 (note k + q = p).
Let b⇤2 be the MLE of B2 under the restriction that B1 = B10 . Then the
likelihood ratio statistic is
LR =

2[log L at H0

log L at MLE].

(9.28)

Now log L at H0 is more complex than before because H0 involves an unknown nuisance parameter B2 that must be estimated. log L at H0 is the
maximum of the likelihood function for any value of B2 but subject to the
condition that B1 = B10 . Thus
LR =

2[log L(B10 , b⇤2 )

log L(b)],

(9.29)

where as before b is the overall MLE of B. Note that LR requires maximizing two log likelihood functions. The first component of LR is a restricted
maximum likelihood and the second component is the overall or unrestricted
maximum.
LR is often computed by examining successively more complex models in
a stepwise fashion and calculating the increment in likelihood ratio 2 in the
overall model. The LR 2 for testing H0 : B2 = 0 when B1 is not in the
model is
LR(H0 : B2 = 0|B1 = 0) =

2[log L(0, 0)

log L(0, b⇤2 )].

(9.30)

Here we are specifying that B1 is not in the model by setting B1 = B10 = 0,
and we are testing H0 : B2 = 0. (We are also ignoring nuisance parameters
such as an intercept term in the test for B2 = 0.)
The LR 2 for testing H0 : B1 = B2 = 0 is given by
LR(H0 : B1 = B2 = 0) =
Subtracting LR

2

log L(b)].

(9.31)

for the smaller model from that of the larger model yields

2[log L(0, 0)
=

2[log L(0, 0)

log L(b)]

2[log L(0, 0)

2[log L(0, b⇤2 )

log L(b)],

log L(0, b2⇤ )]
(9.32)

9.3 General Case

193

Table 9.1

Variables (Parameters)
in Model
Intercept, age
Intercept, age, age2
Intercept, age, age2 , sex

LR

2

Number of
Parameters
1000
2
1010
3
1013
4

which is the same as above (letting B10 = 0).
For example, suppose successively larger models yield the LR 2 s in Table 9.1. The LR 2 for testing for linearity in age (not adjusting for sex)
against quadratic alternatives is 1010 1000 = 10 with 1 d.f. The LR 2
for testing the added information provided by sex, adjusting for a quadratic
e↵ect of age, is 1013 1010 = 3 with 1 d.f. The LR 2 for testing the joint importance of sex and the nonlinear (quadratic) e↵ect of age is 1013 1000 = 13
with 2 d.f.
To derive the Wald statistic for testing H0 : B1 = B10 with B2 being a
nuisance parameter, let the MLE b be partitioned into b = {b1 , b2 }. We can
likewise partition the estimated variance–covariance matrix V into

V11 V12
V =
.
(9.33)
0
V12
V22
The Wald statistic is
W = (b1

B10 )0 V11 1 (b1

B10 ),

(9.34)

which when k = 1 reduces to (estimate
hypothesized value)2 / estimated
variance, with the estimates adjusted for the parameters in B2 .
The score statistic for testing H0 : B1 = B10 does not require solving for
the full set of unknown parameters. Only the MLEs of B2 must be computed,
under the restriction that B1 = B10 . This restricted MLE is b⇤2 from above.
Let U (B10 , b⇤2 ) denote the vector of first derivatives of log L with respect to
all parameters in B, evaluated at the hypothesized parameter values B10 for
the first k parameters and at the restricted MLE b⇤2 for the last q parameters.
(Since the last q estimates are MLEs, the last q elements of U are zero, so
the formulas that follow simplify.) Let I(B10 , b⇤2 ) be the observed information
matrix evaluated at the same values of B as is U . The score statistic for
testing H0 : B1 = B10 is
S = U 0 (B10 , b⇤2 )I

1

(B10 , b⇤2 )U (B10 , b⇤2 ).

(9.35)

Under suitable conditions, the distribution of LR, W , and S can be adequately approximated by a 2 distribution with k d.f.

2

194

9 Overview of Maximum Likelihood Estimation

9.3.3 Tests Based on Contrasts
Wald tests are also done by setting up a general linear contrast. H0 : CB = 0
is tested by a Wald statistic of the form
W = (Cb)0 (CV C 0 )

1

(Cb),

(9.36)

where C is a contrast matrix that “picks o↵” the proper elements of B. The
contrasts can be much more general by allowing elements of C to be other
than zero and one. For the normal linear model, W is converted to an F statistic by dividing by the rank r of C (normally the number of rows in
C), yielding a statistic with an F -distribution with r numerator degrees of
freedom.
Many interesting contrasts are tested by forming di↵erences in predicted
values. By forming more contrasts than are really needed, one can develop
a surprisingly flexible approach to hypothesis testing using predicted values.
This has the major advantage of not requiring the analyst to account for how
the predictors are coded. Suppose that one wanted to assess the di↵erence
in two vectors of predicted values, X1 b X2 b = (X1 X2 )b = b to test
H0 : B = 0, where = X1 X2 . The covariance matrix for b is given by
var( b) =

V

0

.

(9.37)

Let r be the rank of var( b), i.e., the number of non-linearly-dependent
(non-redundant) di↵erences of predicted values of . The value of r and the
rows of
that are not redundant may easily be determined using the QR
decomposition as done by the R function qrb . The 2 statistic with r degrees
of freedom (or F -statistic upon dividing the statistic by r) may be obtained by
computing ⇤ V ⇤ ⇤ 0 where ⇤ is the subset of elements of corresponding
to non-redundant contrasts and V ⇤ is the corresponding sub-matrix of V .
The “di↵erence in predictions” approach can be used to compare means
in a 30 year old male with a 40 year old femalec . But the true utility of
the approach is most obvious when the contrast involves multiple nonlinear
terms for a single predictor, e.g., a spline function. To test for a di↵erence
in two curves, one can compare predictions at one predictor value against
predictions at a series of values with at least one value that pertains to each
basis function. Points can be placed between every pair of knots and beyond
the outer knots, or just obtain predictions at 100 equally spaced X-values.
b

For example, in a 3-treatment comparison one could examine contrasts between
treatments A and B, A and C, and B and C by obtaining predicted values for those
treatments, even though only two di↵erences are required.
c
The rms command could be contrast(fit, list(sex=’male’,age=30),
list(sex=’female’,age=40)) where all other predictors are set to medians or
modes.

Interaction contrast is a
double difference.

9.3 General Case

195

Suppose that there are three treatment groups (A, B, C) interacting with
a cubic spline function of X. If one wants to test the multiple degree of
freedom hypothesis that the profile for X is the same for treatment A and
B vs. the alternative hypothesis that there is a di↵erence between A and B
for at least one value of X, one can compare predicted values at treatment
A and a vector of X values against predicted values at treatment B and the
same vector of X values. If the X relationship is linear, any two X values
will suffice, and if X is quadratic, any three points will suffice. It would be
difficult to test complex hypotheses involving only 2 of 3 treatments using
other methods.
The contrast.rms function in rms can estimate a wide variety of contrasts
and make joint tests involving them, automatically computing the number
of non-linearly-dependent contrasts as the test’s degrees of freedom. See its
help file for several examples.

9.3.4 Which Test Statistics to Use When
At this point, one may ask why three types of test statistics are needed. The
answer lies in the statistical properties of the three tests as well as in computational expense in di↵erent situations. From the standpoint of statistical
properties, LR is the best statistic, followed by S and W . The major statistical problem with W is that it is sensitive to problems in the estimated
variance–covariance matrix in the full model. For some models, most notably
the logistic regression model,274 the variance–covariance estimates can be too
large as the e↵ects in the model become very strong, resulting in values of
W that are too small (or significance levels that are too large). W is also
sensitive to the way the parameter appears in the model. For example, a test
of H0 : log odds ratio = 0 will yield a di↵erent value of W than will H0 :
odds ratio = 1.
Relative computational efficiency of the three types of tests is also an issue.
Computation of LR and W requires estimating all p unknown parameters,
and in addition LR requires re-estimating the last q parameters under that
restriction that the first k parameters = B10 . Therefore, when one is contemplating whether a set of parameters should be added to a model, the score
test is the easiest test to carry out. For example, if one were interested in
testing all two-way interactions among 4 predictors, the score test statistic
for H0 : “no interactions present” could be computed without estimating the
4 ⇥ 3/2 = 6 interaction e↵ects. S would also be appealing for testing linearity
of e↵ects in a model—the nonlinear spline terms could be tested for significance after adjusting for the linear e↵ects (with estimation of only the linear
e↵ects). Only parameters for linear e↵ects must be estimated to compute
S, resulting in fewer numerical problems such as lack of convergence of the
Newton–Raphson algorithm.

196

9 Overview of Maximum Likelihood Estimation

Table 9.2

Type of Test
Recommended Test Statistic
Global association
LR (S for large no. parameters)
Partial association
W (LR or S if problem with W)
Lack of fit, 1 d.f.
W or S
Lack of fit, > 1 d.f.
S
Inclusion of additional predictors S

The Wald tests are very easy to make after all the parameters in a model
have been estimated. Wald tests are thus appealing in a multiple regression
setup when one wants to test whether a given predictor or set of predictors is “significant.” A score test would require re-estimating the regression
coefficients under the restriction that the parameters of interest equal zero.
Likelihood ratio tests are used often for testing the global hypothesis that
no e↵ects are significant, as the log likelihood evaluated at the MLEs is already available from fitting the model and the log likelihood evaluated at
a “null model” (e.g., a model containing only an intercept) is often easy to
compute. Likelihood ratio tests should also be used when the validity of a
Wald test is in question as in the example cited above.
Table 9.2 summarizes recommendations for choice of test statistics for
various situations.

9.3.5 Example: Binomial—Comparing Two
Proportions
Suppose that a binary random variable Y1 represents responses for population
1 and Y2 represents responses for population 2. Let Pi = Prob{Yi = 1}
and assume that a random sample has been drawn from each population
with respective sample sizes n1 and n2 . The sample values are denoted by
Yi1 , . . . , Yini , i = 1 or 2. Let
s1 =

n1
X

Y1j

s2 =

j=1

n2
X

Y2j ,

(9.38)

j=1

the respective observed number of “successes” in the two samples. Let us test
the null hypothesis H0 : P1 = P2 based on the two samples.
The likelihood function is
L=

ni
2 Y
Y

i=1 j=1

Yij

Pi

(1

Pi ) 1

Yij

9.4 Iterative ML Estimation

=

2
Y

197

Pisi (1

Pi ) n i

si

(9.39)

i=1

log L =

2
X
i=1

{si log(Pi ) + (ni

si ) log(1

Pi )}.

(9.40)

s) log(1

P ),

(9.41)

Under H0 , P1 = P2 = P , so
log L(H0 ) = s log(P ) + (n

where s = s1 + s2 , n = n1 + n2 . The (restricted) MLE of this common P is
p = s/n and log L at this value is s log(p) + (n s) log(1 p).
Since the original unrestricted log likelihood function contains two terms
with separate parameters, the two parts may be maximized separately giving
MLEs
p1 = s1 /n1
and
p2 = s2 /n2 .
(9.42)
log L evaluated at these (unrestricted) MLEs is
log L = s1 log(p1 ) + (n1

s1 ) log(1

p1 )

+ s2 log(p2 ) + (n2

s2 ) log(1

p2 ).

(9.43)

The likelihood ratio statistic for testing H0 : P1 = P2 is then
LR =

2{s log(p) + (n

s) log(1

p)

[s1 log(p1 ) + (n1

s1 ) log(1

p1 )

+ s2 log(p2 ) + (n2

s2 ) log(1

p2 )]}.

(9.44)

This statistic for large enough n1 and n2 has a 2 distribution with 1 d.f.
since the null hypothesis involves the estimation of one fewer parameter than
does the unrestricted case. This LR statistic is the likelihood ratio 2 statistic
for a 2 ⇥ 2 contingency table. It can be shown that the corresponding score
statistic is equivalent to the Pearson 2 statistic. The better LR statistic can
be used routinely over the Pearson 2 for testing hypotheses in contingency
tables.

9.4 Iterative ML Estimation
In most cases, one cannot explicitly solve for MLEs but must use trial-anderror numerical methods to solve for parameter values B that maximize
log L(B) or yield a score vector U (B) = 0. One of the fastest and most applicable methods for maximizing a function is the Newton–Raphson method,
which is based on approximating U (B) by a linear function of B in a small region. A starting estimate b0 of the MLE b is made. The linear approximation

198

9 Overview of Maximum Likelihood Estimation

(a first-order Taylor series approximation)
U (b) = U (b0 )

I(b0 )(b

b0 )

(9.45)

is equated to 0 and solved by b yielding
b = b0 + I

1

(b0 )U (b0 ).

(9.46)

The process is continued in like fashion. At the ith step the next estimate is
obtained from the previous estimate using the formula
bi+1 = bi + I

3

1

(bi )U (bi ).

(9.47)

If the log likelihood actually worsened at bi+1 , “step halving” is used; bi+1
is replaced with (bi + bi+1 )/2. Further step halving is done if the log likelihood still is worse than the log likelihood at bi , after which the original
iterative strategy is resumed. The Newton–Raphson iterations continue until
the 2 log likelihood changes by only a small amount over the previous iteration (say .025). The reasoning behind this stopping rule is that estimates of
B that change the 2 log likelihood by less than this amount do not a↵ect
statistical inference since 2 log likelihood is on the 2 scale.

9.5 Robust Estimation of the Covariance Matrix
The estimator for the covariance matrix of b found in Section 9.3 assumes that
the model is correctly specified in terms of distribution, regression assumptions, and independence assumptions. The model may be incorrect in a variety of ways such as non-independence (e.g., repeated measurements within
subjects), lack of fit (e.g., omitted covariable, incorrect covariable transformation, omitted interaction), and distributional (e.g., Y has a distribution
instead of a normal distribution). Variances and covariances, and hence confidence intervals and Wald tests, will be incorrect when these assumptions
are violated.
For the case in which the observations are independent and identically
distributed but other assumptions are possibly violated, Huber305 provided
a covariance matrix estimator that is consistent. His “sandwich” estimator is
given by
n
X
H = I 1 (b)[
Ui Ui0 ]I 1 (b),
(9.48)
i=1

where I(b) is the observed information matrix (Equation 9.24) and Ui is the
vector of derivatives, with respect to all parameters, of the log likelihood
component for the ith observation (assuming the log likelihood can be partitioned into per-observation contributions). For the normal multiple linear

9.5 Robust Estimation of the Covariance Matrix

199

regression case, H was derived by White:653
(X 0 X)

1

[

n
X

(Yi

Xi b)2 Xi Xi0 ](X 0 X)

1

,

(9.49)

i=1

where X is the design matrix (including an intercept if appropriate) and Xi
is the vector of predictors (including an intercept) for the ith observation.
This covariance estimator allows for any pattern of variances of Y |X across
observations. Note that even though H improves the bias of the covariance
matrix of b, it may actually have larger mean squared error than the ordinary
estimate in some cases due to increased variance.161, 524
When observations are dependent within clusters, and the number of observations within clusters is very small in comparison to the total sample
size, a simple adjustment to Equation 9.48 can be used to derive appropriate covariance matrix estimates (see Lin [400, p. 2237], Rogers,524 and
Lee et al. [385, Eq. 5.1, p. 246]). One merely accumulates sums of elements
of U within clusters before computing cross-product terms:
Hc = I

1

(b)[

ni
ni
c
X
X
X
{(
Uij )(
Uij )0 }]I
i=1

j=1

1

(b),

4

(9.50)

j=1

where c is the number of clusters, ni is the number of observations in the ith
cluster, Uij is the contribution of the jth observation within the ith cluster to
the score vector, and I(b) is computed as before ignoring clusters. For a model
such as the Cox model which has no per-observation score contributions,
special score residuals385, 400, 403, 598 are used for U .
Bootstrapping can also be used to derive robust covariance matrix estimates174, 175 in many cases, especially if covariances of b that are not conditional on X are appropriate. One merely generates approximately 200 samples
with replacement from the original dataset, computes 200 sets of parameter
estimates, and computes the sample covariance matrix of these parameter estimates. Sampling with replacement from entire clusters can be used to derive
variance estimates in the presence of intracluster correlation.184 Bootstrap
estimates of the conditional variance–covariance matrix given X are harder
to obtain and depend on the model assumptions being satisfied. The simpler
unconditional estimates may be more appropriate for many non-experimental
studies where one may desire to “penalize” for the X being random variables.
It is interesting that these unconditional estimates may be very difficult to
obtain parametrically, since a multivariate distribution may need to be assumed for X.
The previous discussion addresses the use of a “working independence
model” with clustered data. Here one estimates regression coefficients assuming independence of all records (observations). Then a sandwich or bootstrap
method is used to increase standard errors to reflect some redundancy in the
correlated observations. The parameter estimates will often be consistent es-

5

200

6

9 Overview of Maximum Likelihood Estimation

timates of the true parameter values, but they may be inefficient for certain
cluster or correlation structures.
The rms package’s robcov function computes the Huber robust covariance
matrix estimator, and the bootcov function computes the bootstrap covariance estimator. Both of these functions allow for clustering.

9.6 Wald, Score, and Likelihood-Based Confidence
Intervals

7

A 1 ↵ confidence interval for a parameter i is the set of all values i0
that if hypothesized would be accepted in a test of H0 : i = i0 at the
↵ level. What test should form the basis for the confidence interval? The
Wald test is most frequently used because of its simplicity. A two-sided 1 ↵
confidence interval is bi ±z1 ↵/2 s, where z is the critical value from the normal
distribution and s is the estimated standard error of the parameter estimate
bi .d The problem with s discussed in Section 9.3.4 points out that Wald
statistics may not always be a good basis. Wald-based confidence intervals are
also symmetric even though the coverage probability may not be.157 Scoreand LR-based confidence limits have definite advantages. When Wald-type
confidence intervals are appropriate, the analyst may consider insertion of
robust covariance estimates (Section 9.5) into the confidence interval formulas
(note that adjustments for heterogeneity and correlated observations are not
available for score and LR statistics).
Wald– (asymptotic normality) based statistics are convenient for deriving
confidence intervals for linear or more complex combinations of the model’s
parameters. As in Equation 9.36, the variance–covariance matrix of Cb, where
C is an appropriate matrix and b is the vector of parameter estimates, is
CV C 0 , where V is the variance matrix of b. In regression models we commonly
substitute a vector of predictors (and optional intercept) for C to obtain the
variance of the linear predictor Xb as
var(Xb) = XV X 0 .

(9.51)

See Section 9.3.3 for related information.
d

This is the basis for confidence limits computed by the R rms package’s Predict,
summary.rms, and contrast.rms functions. When the robcov function has been
used to replace the information-matrix-based covariance matrix with a Huber robust
covariance estimate with an optional cluster sampling correction, the functions are
using a “robust” Wald statistic basis. When the bootcov function has been used to
replace the model fit’s covariance matrix with a bootstrap unconditional covariance
matrix estimate, the two functions are computing confidence limits based on a normal
distribution but using more nonparametric covariance estimates.

9.7 Bootstrap Confidence Regions

201

9.6.1 Simultaneous Wald Confidence Regions
The confidence intervals just discussed are pointwise confidence intervals.
For OLS regression there are methods for computing confidence intervals
with exact simultaneous confidence coverage for multiple estimates367 . There
are approximate methods for simultaneous confidence limits for all models
for which the vector of estimates b is approximately multivariately normally
distributed. The method of Hothorn et al.300 is quite general; in their R package multcomp’s glht function, the user can specify any contrast matrix over
which the individual confidence limits will be simultaneous. A special case
of a contrast matrix is the design matrix X itself, resulting in simultaneous
confidence bands for any number of predicted values. An example is shown
in Figure 9.5. See Section 9.3.3 for a good use for simultaneous contrasts.

9.7 Bootstrap Confidence Regions
A more nonparametric method for computing confidence intervals for functions of the vector of parameters B can be based on bootstrap percentile
confidence limits. For each sample with replacement from the original dataset,
one computes the MLE of B, b, and then the quantity of interest g(b). Then
the gs are sorted and the desired quantiles are computed. At least 1000 bootstrap samples will be needed for accurate assessment of outer confidence
limits. This method is suitable for obtaining pointwise confidence bands for
a nonlinear regression function, say, the relationship between age and the log
odds of disease. At each of 100 age values the predicted logits are computed
for each bootstrap sample. Then separately for each age point the 0.025 and
0.975 quantiles of 1000 estimates of the logit are computed to derive a 0.95
confidence band. Other more complex bootstrap schemes will achieve somewhat greater accuracy of confidence interval coverage,175 and as described in
Section 9.5 one can use variations on the basic bootstrap in which the predictors are considered fixed and/or cluster sampling is taken into account.
The R function bootcov in the rms package bootstraps model fits to obtain
unconditional (with respect to predictors) bootstrap distributions with or
without cluster sampling. bootcov stores the matrix of bootstrap regression
coefficients so that the bootstrapped quantities of interest can be computed
in one sweep of the coefficient matrix once bootstrapping is completed.
For many regression models. the rms package’s Predict, summary.rms,
and contrast.rms functions make it easy to compute pointwise bootstrap
confidence intervals in a variety of contexts. As an example, consider 200
simulated x values from a log-normal distribution and simulate binary y
from a true population binary logistic model given by

8

9

202

9 Overview of Maximum Likelihood Estimation

Prob(Y = 1|X = x) =

1
.
1 + exp[ (1 + x/2)]

(9.52)

Not knowing the true model, a quadratic logistic model is fitted. The R code
needed to generate the data and fit the model is given below.
r e q u i r e ( rms )
n
200
s e t .s e e d (15)
x1
rnorm ( n )
logit
x1 / 2
y
i f e l s e ( r u n i f (n)  p l o g i s ( l o g i t ) , 1 , 0)
dd
d a t a d i s t ( x1 ) ;
o p t i o n s ( d a t a d i s t= ’ dd ’ )
f
lrm ( y ⇠ p o l ( x1 , 2 ) , x=TRUE, y=TRUE)
p r i n t ( f , l a t e x=TRUE)

Logistic Regression Model
lrm(formula = y ˜ pol(x1, 2), x = TRUE, y = TRUE)
Model Likelihood
Ratio Test
Obs
200 LR 2
16.37
0
97 d.f.
2
1
103 Pr(> 2 ) 0.0003
L
max | @ log
| 3⇥10 9
@

Coef
Intercept -0.0842
x1
0.5902
x12
0.1557
l a t e x ( anova ( f ) ,

Discrimination
Indexes
R2
0.105
g
0.680
gr
1.973
gp
0.156
Brier
0.231

Rank Discrim.
Indexes
C
0.642
Dxy
0.285
0.286
⌧a
0.143

S.E. Wald Z Pr(> |Z|)
0.1823
-0.46
0.6441
0.1580
3.74
0.0002
0.1136
1.37
0.1708

f i l e = ’ ’ , t a b l e . e n v=FALSE)
2

x1
13.99
Nonlinear 1.88
TOTAL
13.99

d.f. P
2 0.0009
1 0.1708
2 0.0009

The bootcov function is used to draw 1000 resamples to obtain bootstrap
estimates of the covariance matrix of the regression coefficients as well as
to save the 1000 ⇥ 3 matrix of regression coefficients. Then, because individual regression coefficients for x do not tell us much, we summarize the
x-e↵ect by computing the e↵ect (on the logit scale) of increasing x from 1

9.7 Bootstrap Confidence Regions

203

to 5. We first compute bootstrap nonparametric percentile confidence intervals the long way. The 1000 bootstrap estimates of the log odds ratio are
computed easily using a single matrix multiplication with the di↵erence in
predictions approach, multiplying the di↵erence in two design matrices, and
we obtain the bootstrap estimate of the standard error of the log odds ratio
by computing the sample standard deviation of the 1000 valuese . Bootstrap
percentile confidence limits are just sample quantiles from the bootstrapped
log odds ratios.
# Get 2-row design matrix for obtaining predicted values
# for x = 1 and 5
X
c b i n d ( I n t e r c e p t =1 ,
p r e d i c t ( f , d a t a . f r a m e ( x1=c ( 1 , 5 ) ) , t y p e= ’ x ’ ) )
Xdif
X[ 2 , , drop=FALSE ]
X[ 1 , , drop=FALSE ]
Xdif

2

Intercept pol(x1, 2)x1 pol(x1, 2)x1^2
0
4
24

b
b o o t c o v ( f , B=1000)
boot.log.odds.ratio
b$ b o o t . C o e f %⇤% t ( X d i f )
sd ( b o o t . l o g . o d d s . r a t i o )
[1] 2.752103
# This is the same as from summary(b, x=c(1,5)) as summary
# uses the bootstrap covariance matrix
summary ( b , x1=c ( 1 , 5 ) ) [ 1 , ’ S . E . ’ ]
[1] 2.752103
# Compare this s.d. with one from information matrix
summary ( f , x1=c ( 1 , 5 ) ) [ 1 , ’ S . E . ’ ]
[1] 2.988373
# Compute percentiles of bootstrap odds ratio
exp ( q u a n t i l e ( b o o t . l o g . o d d s . r a t i o , c ( . 0 2 5 , . 9 7 5 ) ) )
2.5%
97.5%
2.795032e+00 2.067146e+05
# Automatic:
summary ( b , x1=c ( 1 , 5 ) ) [ ’ Odds R a t i o ’ , ]
e

As indicated below, this standard deviation can also be obtained by using the
summary function on the object returned by bootcov, as bootcov returns a fit
object like one from lrm except with the bootstrap covariance matrix substituted for
the information-based one.

204

9 Overview of Maximum Likelihood Estimation

Low
High
Diff.
Effect
1.000000e+00 5.000000e+00 4.000000e+00 4.443932e+02
Lower 0.95
Upper 0.95
Type
2.795032e+00 2.067146e+05 2.000000e+00
print ( contrast (b ,

11

S.E.
NA

l i s t ( x1 =5) , l i s t ( x1 =1) , f u n=exp ) )

Contrast
S.E.
Lower
Upper
Z Pr(>|z|)
6.09671 2.752103 1.027843 12.23909 2.22
0.0267

Confidence intervals are 0.95 bootstrap nonparametric percentile intervals
# Figure 9.4
h i s t ( b o o t . l o g . o d d s . r a t i o , n c l a s s =100 , x l a b= ’ l o g (OR) ’ , main= ’ ’ )

Frequency

40
30
20
10
0
0

5

10

15

log(OR)
Fig. 9.4 Distribution of 1000 bootstrap x=1:5 log odds ratios

Figure 9.4 shows the distribution of log odds ratios.
Now consider confidence bands for the true log odds that y = 1, across a
sequence of x values. The Predict function automatically calculates point-bypoint bootstrap percentiles, basic bootsrap, or BCa199 confidence limits when
the fit has passed through bootcov. Simultaneous Wald-based confidence
intervals300 and Wald intervals substituting the bootstrap covariance matrix
estimator are added to the plot when Predict calls the multcomp package.
x1s
s e q ( 0 , 5 , l e n g t h =100)
pwald
P r e d i c t ( f , x1=x 1 s )
psand
P r e d i c t ( r o b c o v ( f ) , x1=x 1 s )
pbootcov
P r e d i c t ( b , x1=x1s , u s e b o o t c o e f=FALSE)
pbootnp
P r e d i c t ( b , x1=x 1 s )
pbootbca
P r e d i c t ( b , x1=x1s , b o o t . t y p e= ’ bca ’ )

9.7 Bootstrap Confidence Regions
pbootbas
psimult
z

205

P r e d i c t ( b , x1=x1s , b o o t . t y p e= ’ b a s i c ’ )
P r e d i c t ( b , x1=x1s , c o n f . t y p e= ’ s i m u l t a n e o u s ’ )

r b i n d ( ’ Boot p e r c e n t i l e ’
=
’ Robust sandwich ’
=
’ Boot BCa ’
=
’ Boot c o v a r i a n c e+Wald ’=
Wald
=
’ Boot b a s i c ’
=
Simultaneous
=

pbootnp ,
psand ,
pbootbca ,
pbootcov ,
pwald ,
pbootbas ,
psimult )

z$ class

i f e l s e ( z $ . s e t . %i n% c ( ’ Boot p e r c e n t i l e ’ , ’ Boot bca ’ ,
’ Boot b a s i c ’ ) , ’ Other ’ , ’ Wald ’ )
g g p l o t ( z , g r o u p s=c ( ’ . s e t . ’ , ’ c l a s s ’ ) ,
c o n f= ’ l i n e ’ , y l i m=c ( 1 , 9 ) , l e g e n d . l a b e l=FALSE)

7.5

Boot percentile
Robust sandwich

log odds

Boot BCa
Boot covariance+Wald

5.0

Wald
Boot basic
Simultaneous

2.5

Other
Wald

0.0
0

1

2

3

4

5

x1
Fig. 9.5 Predicted log odds and confidence bands for seven types of confidence intervals. Seven categories are ordered top to bottom corresponding to order of lower
confidence bands at x1=5. Dotted lines are for Wald–type methods that yield symmetric confidence intervals and assume normality of point estimators.

206

9 Overview of Maximum Likelihood Estimation

See Problems at chapter’s end for a worrisome investigation of bootstrap confidence interval coverage using simulation. It appears that when the model’s
log odds distribution is not symmetric and includes very high or very low
probabilities, neither the bootstrap percentile nor the bootstrap BCa intervals have good coverage, while the basic bootstrap and ordinary Wald intervals are fairly accuratef . It is difficult in general to know when to trust
the bootstrap for logistic and perhaps other models when computing confidence intervals, and the simulation problem suggests that the basic bootstrap
should be used more frequently. Similarly, the distribution of bootstrap e↵ect
estimates can be suspect. Asymmetry in this distribution does not imply that
the true sampling distribution is asymmetric or that the percentile intervals
are preferred.

9.8 Further Use of the Log Likelihood
9.8.1 Rating Two Models, Penalizing for Complexity

10

11

Suppose that from a single sample two competing models were developed. Let
the respective 2 log likelihoods for these models be denoted by L1 and L2 ,
and let p1 and p2 denote the number of parameters estimated in each model.
Suppose that L1 < L2 . It may be tempting to rate model one as the “best”
fitting or “best” predicting model. That model may provide a better fit for
the data at hand, but if it required many more parameters to be estimated,
it may not be better “for the money.” If both models were applied to a new
sample, model one’s overfitting of the original dataset may actually result in
a worse fit on the new dataset.
Akaike’s information criterion (AIC33, 352, 625 ) provides a method for penalizing the log likelihood achieved by a given model for its complexity to
obtain a more unbiased assessment of the model’s worth. The penalty is
to subtract the number of parameters estimated from the log likelihood, or
equivalently to add twice the number of parameters to the 2 log likelihood.
The penalized log likelihood is analogous to Mallows’ Cp in ordinary multiple
regression. AIC would choose the model by comparing L1 + 2p1 to L2 + 2p2
and picking the model with the lower value. We often use AIC in “adjusted
2
” form:
AIC = LR 2 2p.
(9.53)
Breiman [65, Section 1.3] and Chatfield [97, Section 4] discuss the fallacy of
AIC and Cp for selecting from a series of non-prespecified models.
f

Limited simulations using the conditional bootstrap and Firth’s penalized likelihood277 did not show significant improvement in confidence interval coverage.

9.8 Further Use of the Log Likelihood

207

9.8.2 Testing Whether One Model Is Better than
Another
One way to test whether one model (A) is better than another (B) is to
embed both models in a more general model (A + B). Then a LR 2 test
can be done to test whether A is better than B by changing the hypothesis
to test whether A adds predictive information to B (H0 : A + B > B) and
whether B adds information to A (H0 : A + B > A). The approach of testing
A > B via testing A + B > B and A + B > A is especially useful for selecting
from competing predictors such as a multivariable model and a subjective
assessor.128, 259, 387, 663
Note that LR 2 for H0 : A + B > B minus LR 2 for H0 : A + B > A
equals LR 2 for H0 : A has no predictive information minus LR 2 for
H0 : B has no predictive information,659 the di↵erence in LR 2 for testing
each model (set of variables) separately. This gives further support to the
use of two separately computed Akaike’s information criteria for rating the
two sets of variables.
See Section 9.8.4 for an example.

12

9.8.3 Unitless Index of Predictive Ability
The global likelihood ratio test for regression is useful for determining
whether any predictor is associated with the response. If the sample is large
enough, even weak associations can be “statistically significant.” Even though
a likelihood ratio test does not shed light on a model’s predictive strength,
the log likelihood (L.L.) can still be useful here. Consider the following L.L.s:
Best (lowest) possible 2 L.L.:
L⇤ = 2 L.L. for a hypothetical model that perfectly predicts the outcome.
2 L.L. achieved:
L = 2 L.L. for the fitted model.
Worst 2 L.L.:
L0 = 2 L.L. for a model that has no predictive information.
The last 2 L.L., for a “no information” model, is the 2 L.L. under the null
hypothesis that all regression coefficients except for intercepts are zero. A “no
information” model often contains only an intercept and some distributional
parameters (a variance, for example).
The quantity L0 L is LR, the log likelihood ratio statistic for testing
the global null hypothesis that no predictors are related to the response. It
is also the 2 log likelihood “explained” by the model. The best (lowest) 2

13

208

9 Overview of Maximum Likelihood Estimation

L.L. is L⇤ , so the amount of L.L. that is capable of being explained by the
model is L0 L⇤ . The fraction of 2 L.L. explained that was capable of being
explained is
(L0 L)/(L0 L⇤ ) = LR/(L0 L⇤ ).
(9.54)
The fraction of log likelihood explained is analogous to R2 in an ordinary
linear model, although Korn and Simon358, 359 provide a much more precise
notion.
Akaike’s information criterion can be used to penalize this measure of
association for the number of parameters estimated (p, say) to transform
this unitless measure of association into a quantity that is analogous to the
adjusted R2 or Mallows’ Cp in ordinary linear regression. We let R denote
the square root of such a penalized fraction of log likelihood explained. R is
defined by
R2 = (LR 2p)/(L0 L⇤ ).
(9.55)
The R index can be used to assess how well the model compares with a
“perfect” model, as well as to judge whether a more complex model has
predictive strength that justifies its additional parameters. Had p been used
in Equation 9.55 rather than 2p, R2 is negative if the log likelihood explained
is less than what one would expect by chance. R will be the square root of
1 2p/(L0 L⇤ ) if the model perfectly predicts the response. This upper
limit will be near one if the sample size is large.
Partial R indexes can also be defined by substituting the 2 L.L. explained
for a given factor in place of that for the entire model, LR. The “penalty
factor” p becomes one. This index Rpartial is defined by
2
Rpartial
= (LRpartial

2)/(L0

L⇤ ),

(9.56)

which is the (penalized) fraction of 2 log likelihood explained by the predictor. Here LRpartial is the log likelihood ratio statistic for testing whether
the predictor is associated with the response, after adjustment for the other
predictors. Since such likelihood ratio statistics are tedious to compute, the
1 d.f. Wald 2 can be substituted for the LR statistic (keeping in mind that
difficulties with the Wald statistic can arise).
Liu and Dyer417 and Cox and Wermuth133 point out difficulties with the
2
R measure for binary logistic models. Cox and Snell132 and Magee425 used
other analogies to derive other R2 measures that may have better properties.
For a sample of size n and a Wald statistic for testing overall association,
they defined
W
n+W
= 1 exp( LR/n)

2
RW
=
2
RLR

=1

2/n

,

(9.57)

9.8 Further Use of the Log Likelihood

209

where is the null model likelihood divided by the fitted model likelihood. In
the case of ordinary least squares with normality both of the above indexes
2
are equal to the traditional R2 . RLR
is equivalent to Maddala’s index [424, Eq.
134
2
2.44]. Cragg and Uhler
and Nagelkerke464 suggested dividing RLR
by its
maximum attainable value
2
Rmax
=1

exp( L0 /n)

(9.58)

2
to derive RN
which ranges from 0 to 1. This is the form of the R2 index we
use throughout.
For penalizing for overfitting, see Verweij and van Houwelingen632 for an
overfitting-corrected R2 that uses a cross-validated likelihood.

14

9.8.4 Unitless Index of Adequacy of a Subset of
Predictors
Log likelihoods are also useful for quantifying the predictive information contained in a subset of the predictors compared with the information contained
in the entire set of predictors.259 Let LR again denote the 2 log likelihood
ratio statistic for testing the joint significance of the full set of predictors. Let
LRs denote the 2 log likelihood ratio statistic for testing the importance of
the subset of predictors of interest, excluding the other predictors from the
model. A measure of adequacy of the subset for predicting the response is
given by
A = LRs /LR.
(9.59)
A is then the proportion of log likelihood explained by the subset with reference to the log likelihood explained by the entire set. When A = 1, the subset
contains all the predictive information found in the whole set of predictors;
that is, the subset is adequate by itself and the additional predictors contain
no independent information. When A = 0, the subset contains no predictive
information by itself.
Cali↵ et al.87 used the A index to quantify the adequacy (with respect to
prognosis) of two competing sets of predictors that each describe the extent of
coronary artery disease. The response variable was time until cardiovascular
death and the statistical model used was the Cox129 proportional hazards
model. Some of their results are reproduced in Table 9.4. A chance-corrected
adequacy measure could be derived by squaring the ratio of the R-index for
the subset to the R-index for the whole set. A formal test of superiority of
X1 = maximum % stenosis over X2 = jeopardy score can be obtained by
testing whether X1 adds to X2 (LR 2 = 57.5 42.6 = 14.9) and whether
X2 adds to X1 (LR 2 = 57.5 51.8 = 5.7). X1 adds more to X2 (14.9) than
X2 adds to X1 (5.7). The di↵erence 14.9 5.7 = 9.2 equals the di↵erence in
single factor 2 (51.8 42.6).659

15

210

9 Overview of Maximum Likelihood Estimation

Table 9.4

Predictors Used
LR 2 Adequacy
Coronary jeopardy score
42.6
0.74
Maximum % stenosis in each artery 51.8
0.90
Combined
57.5
1.00

9.9 Weighted Maximum Likelihood Estimation
It is commonly the case that data elements represent combinations of values
that pertain to a set of individuals. This occurs, for example, when unique
combinations of X and Y are determined from a massive dataset, along with
the frequency of occurrence of each combination, for the purpose of reducing
the size of the dataset to analyze. For the ith combination we have a case
weight wi that is a positive integer representing a frequency. Assuming that
observations represented by combination i are independent, the likelihood
needed to represent all wi observations is computed simply by multiplying
all of the likelihood elements (each having value Li ), yielding a total likelii
hood contribution for combination i of Lw
i or a log likelihood contribution
of wi log Li . To obtain a likelihood for the entire dataset P
one computes the
product over all combinations. The total log likelihood is
wi log Li . As an
example, the weighted likelihood that would be used to fit a weighted logistic
regression model is given by
L=

n
Y

Piwi Yi (1

Pi )wi (1

Yi )

,

(9.60)

i=1

Pn
where there are n combinations, i=1 wi > n, and Pi is Prob[Yi = 1|Xi ] as
dictated by the model. Note that in general the correct likelihood function
cannot be obtained by weighting the data and using an unweighted likelihood.
By a small leap one can obtain weighted maximum likelihood estimates
from the above method even if the weights do not represent frequencies or
even integers, as long as the weights are non-negative. Non-frequency weights
are commonly used in sample surveys to adjust estimates back to better
represent a target population when some types of subjects have been oversampled from that population. Analysts should beware of possible losses in
efficiency when obtaining weighted estimates in sample surveys.356, 357 Making the regression estimates conditional on sampling strata by including strata
as covariables may be preferable to re-weighting the strata. If weighted estimates must be obtained, the weighted likelihood function is generally valid
for obtaining properly weighted parameter estimates. However, the variance–
covariance matrix obtained by inverting the information matrix from the
weighted likelihood will not be correct in general. For one thing, the sum of

9.10 Penalized Maximum Likelihood Estimation

211

the weights may be far from the number of subjects in the sample. A rough
approximation to the variance–covariance
matrix may be obtained by first
P
multiplying each weight by n/ wi and then computing the weighted information matrix, where n is the number of actual subjects in the sample.

16

9.10 Penalized Maximum Likelihood Estimation
Maximizing the log likelihood provides the best fit to the dataset at hand,
but this can also result in fitting noise in the data. For example, a categorical predictor with 20 levels can produce extreme estimates for some of the
19 regression parameters, especially for the small cells (see Section 4.5). A
shrinkage approach will often result in regression coefficient estimates that
while biased are lower in mean squared error and hence are more likely to be
close to the true unknown parameter values. Ridge regression is one approach
to shrinkage, but a more general and better developed approach is penalized
maximum likelihood estimation,232, 380, 631, 633 which is really a special case
of Bayesian modeling with a Gaussian prior. Letting L denote the usual likelihood function and be a penalty factor, we maximize the penalized log
likelihood given by
p
1 X
log L
(si i )2 ,
(9.61)
2 i=1
where s1 , s2 , . . . , sp are scale factors chosen to make si i unitless. Most authors standardize the data first and do not have scale factors in the equation,
but Equation 9.61 has the advantage of allowing estimation of on the original scale of the data. The usual methods (e.g., Newton–Raphson) are used
to maximize 9.61.
The choice of the scaling constants has received far too little attention in
the ridge regression and penalized MLE literature. It is common to use the
standard deviation of each column of the design matrix to scale the corresponding parameter. For models containing nothing but continuous variables
that enter the regression linearly, this is usually a reasonable approach. For
continuous variables represented with multiple terms (one of which is linear), it is not always reasonable to scale each nonlinear term with its own
standard
p deviation. For dummy variables, scaling using the standard deviation ( d(1 d), where d is the mean of the dummy variable, i.e., the fraction of observations in that cell) is problematic since this will result in high
prevalance cells getting more shrinkage than low prevalence ones because the
high prevalence cells will dominate the penalty function.
An advantage of the formulation in Equation 9.61 is that one can assign
scale constants of zero for parameters for which no shrinkage is desired.232, 631

17

18

212

19

9 Overview of Maximum Likelihood Estimation

For example, one may have prior beliefs that a linear additive model will fit
the data. In that case, nonlinear and non-additive terms may be penalized.
For a categorical predictor having c levels, users of ridge regression often do
not recognize that the amount of shrinkage and the predicted values from the
fitted model depend on how the design matrix is coded. For example, one will
get di↵erent predictions depending on which cell is chosen as the reference
cell when constructing dummy variables. The setup in Equation 9.61 has the
same problem. For example, if for a three-category factor we use category 1
as the reference cell and have parameters 2 and 3 , the unscaled penalty
function is 22 + 32 . If category 3 were used as the reference cell instead, the
2
penalty would be 32 + ( 2
Verweij and
3 ) . To get around this problem,
Pc
631
van Houwelingen
proposed using the penalty function i ( i
)2 , where
is the mean of all c s. This causes shrinkage of all parameters toward
the mean parameter value. Letting the first category be the reference cell,
we use c 1 dummy variables and define 1 ⌘ 0. For the case c = 3 the
is 22 /2. If no
sum of squares is 2[ 22 + 32
2 3 ]/3. For c = 2 the penalty
p
scale constant is used, this is the same as scaling 2 with 2 ⇥ the standard
deviation of a binary dummy variable with prevalance of 0.5.
The sum of squares can be written in matrix form as [ 2 , . . . , c ]0 (A
B)[ 2 , . . . , c ], where A is a c 1⇥c 1 identity matrix and B is a c 1⇥c 1
matrix all of whose elements are 1c .
For general penalty functions such as that just described, the penalized
log likelihood can be generalized to
log L

20

21

1
2

0

P .

(9.62)

For purposes of using the Newton–Raphson procedure, the first derivative
of the penalty function with respect to is
P , and the negative of the
second derivative is P .
Another problem in penalized estimation is how the choice of is made.
Many authors use cross-validation. A limited number of simulation studies in binary logistic regression modeling has shown that for each being
considered, at least 10-fold cross-validation must be done so as to obtain a
reasonable estimate of predictive accuracy. Even then, a smoother203 (“super smoother”) must be used on the ( , accuracy) pairs to allow location of
the optimum value unless one is careful in choosing the initial sub-samples
and uses these same splits throughout. Simulation studies have shown that a
modified AIC is not only much quicker to compute (since it requires no crossvalidation) but performs better at finding a good value of (see below).
For a given , the e↵ective number of parameters being estimated is reduced because of shrinkage. Gray [232, Eq. 2.9] and others estimate the effective degrees of freedom by computing the expected value of a global Wald
statistic for testing association, when the null hypothesis of no association is
true. The d.f. is equal to

choose lambda that give
the best AIC (let vary
from 0 to infinity)

9.10 Penalized Maximum Likelihood Estimation

trace[I( ˆP )V ( ˆP )],

213

(9.63)

where ˆP is the penalized MLE (the parameters that maximize Equation 9.61), I is the information matrix computed from ignoring the penalty
function, and V is the covariance matrix computed by inverting the information matrix that included the second derivatives with respect to in the
penalty function.
Gray [232, Eq. 2.6] states that a better estimate of the variance–covariance
matrix for ˆP than V ( ˆP ) is
V ⇤ = V ( ˆP )I( ˆP )V ( ˆP ).

(9.64)

Therneau (personal communication, 2000) has found in a limited number
of simulation studies that V ⇤ underestimates the true variances, and that a
better estimate of the variance–covariance matrix is simply V ( ˆP ), assuming
that the model is correctly specified. This is the covariance matrix used by
default in the rms package (the user can request that the sandwich estimator
be used instead) and is in fact the one Gray used for Wald tests.
Penalization will bias estimates of , so hypothesis tests and confidence
intervals using ˆP may not have a simple interpretation. The same problem
arises in score and likelihood ratio tests.
Equation 9.63 can be used to derive a modified AIC (see [631, Eq. 6]
and [633, Eq. 7]) on the model 2 scale:
LR

2

2 ⇥ e↵ective d.f.,

(9.65)

where LR 2 is the likelihood ratio 2 for the penalized model, but ignoring
the penalty function. If a variety of are tried and one plots the ( , AIC)
pairs, the that maximizes AIC will often be a good choice, that is, it is
likely to be near the value of
that maximizes predictive accuracy on a
future datasetg .
Note that if one does penalized maximum likelihood estimation where a set
of variables being penalized has a negative value for the unpenalized 2 2 ⇥
d.f., the value of that will optimize the overall model AIC will be 1.
As an example, consider some simulated data (n = 100) with one predictor
in which the true model is Y = X1 + ✏, where ✏ has a standard normal
distribution and so does X1 . We use a series of penalties (found by trial and
error) that give rise to sensible e↵ective d.f., and fit penalized restricted cubic
spline functions with five knots. We penalize two ways: all terms in the model
including the coefficient of X1 , which in reality needs no penalty; and only
the nonlinear terms. The following R program, in conjunction with the rms
package, does the job.
s e t . s e e d (191)
g

Several examples from simulated datasets have shown that using BIC to choose a
penalty results in far too much shrinkage.

22

214
x1
y
pens
all

9 Overview of Maximum Likelihood Estimation
rnorm ( 1 0 0 )
x1 + rnorm ( 1 0 0 )
df
aic
c (0 , .07 , .5 , 2 , 6 , 1 5 , 6 0 )
nl
l i s t ()

for ( penalize in 1:2) {
f o r ( i i n 1 : l e n g t h ( pens ) ) {
f
o l s ( y ⇠ r c s ( x1 , 5 ) , p e n a l t y=
l i s t ( s i m p l e= i f ( p e n a l i z e ==1)pens [ i ] e l s e 0 ,
n o n l i n e a r=pens [ i ] ) )
df [ i ]
f$ stats [ ’ d.f. ’ ]
aic [ i ]
AIC ( f )
nam
p a s t e ( i f ( p e n a l i z e == 1 ) ’ a l l ’ e l s e ’ n l ’ ,
’ p e n a l t y : ’ , pens [ i ] , s e p= ’ ’ )
nam
a s . c h a r a c t e r ( pens [ i ] )
p
P r e d i c t ( f , x1=s e q ( 2.5 , 2 . 5 , l e n g t h =100) ,
c o n f . i n t=FALSE)
i f ( p e n a l i z e == 1 ) a l l [ [ nam ] ]
p e l s e n l [ [ nam ] ]
p
}
p r i n t ( r b i n d ( d f=df , a i c=a i c ) )
}
[,1]
[,2]
[,3]
[,4]
[,5]
[,6]
df
4.0000
3.213591
2.706069
2.30273
2.029282
1.822758
aic 270.6653 269.154045 268.222855 267.56594 267.288988 267.552915
[,7]
df
1.513609
aic 270.805033
[,1]
[,2]
[,3]
[,4]
[,5]
[,6]
df
4.0000
3.219149
2.728126
2.344807
2.109741
1.960863
aic 270.6653 269.167108 268.287933 267.718681 267.441197 267.347475
[,7]
df
1.684421
aic 267.892073
all
d o . c a l l ( ’ rbind ’ , a l l ) ; a l l $ type
’ Penalize All ’
nl
d o . c a l l ( ’ rbind ’ , nl ) ; nl $ type
’ Penalize Nonlinear ’
both
as.data.frame ( rbind.data.frame ( all , nl ))
both $ P e n a l t y
both $ . s e t .
g g p l o t ( both , a e s ( x=x1 , y=yhat , c o l o r=P e n a l t y ) ) + g e o m l i n e ( ) +
g e o m a b l i n e ( c o l=g r a y ( . 7 ) ) + f a c e t g r i d (⇠ t y p e )
# Figure 9.6

23

The left panel in Figure 9.6 corresponds to penalty = list(simple=a,
nonlinear=a) in the R program, meaning that all parameters except the
intercept are shrunk by the same amount a (this would be more appropriate
had there been multiple predictors). As e↵ective d.f. get smaller (penalty
factor gets larger), the regression fits get flatter (too flat for the largest
penalties) and confidence bands get narrower. The right graph corresponds to
penalty=list(simple=0, nonlinear=a), causing only the cubic spline terms
that are nonlinear in X1 to be shrunk. As the amount of shrinkage increases
(d.f. lowered), the fits become more linear and closer to the true regression
line (longer dotted line). Again, confidence intervals become smaller.

9.11 Further Reading

4

215

Penalize All

Penalize Nonlinear
Penalty
0

2

0.07

yhat

0.5
2
0

6
15
60

−2
−2 −1

0

1

2

−2 −1

0

1

2

x1
Fig. 9.6 Penalized least squares estimates for an unnecessary five-knot restricted
cubic spline function. In the left graph all parameters (except the intercept) are
penalized. The e↵ective d.f. are 4, 3.21, 2.71, 2.30, 2.03, 1.82, and 1.51. In the right
graph, only parameters associated with nonlinear functions of X1 are penalized. The
e↵ective d.f. are 4, 3.22, 2.73, 2.34, 2.11, 1.96, and 1.68.

9.11 Further Reading
1

2
3
4

5

Boos59 has some nice generalizations of the score test. Morgan et al.457 show
how score test 2 statistics may negative unless the expected information matrix
is used.
See Marubini and Valsecchi [437, pp. 164–169] for an excellent description of
the relationship between the three types of test statistics.
References [112, 500] have good descriptions of methods used to maximize log L.
As Long and Ervin419 argue, for small sample sizes, the usual Huber–White covariance estimator should not be used because there the residuals do not have
constant variance even under homoscedasticity. They showed that a simple correction due to Efron and others can result in substantially better estimates.
Lin and Wei,403 Binder,54 and Lin400 have applied the Huber estimator to the
Cox129 survival model. Freedman202 questioned the use of sandwich estimators because they are often used to obtain the right variances on the wrong
parameters when the model doesn’t fit. He also has some excellent background
information.
Feng et al.184 showed that in the case of cluster correlations arising from repeated measurement data with Gaussian errors, the cluster bootstrap performs
excellently even when the number of observations per cluster is large and the
number of subjects is small. Xiao and Abrahamowicz670 compared the cluster
bootstrap with a two-stage cluster bootstrap in the context of the Cox model.

Lasso may be
parsimonious, but it
may not be selecting
the correct things.

1-sample binomial casescore CI is most
accurate (Wilson CI)

216
6
7

8
9

10

11

9 Overview of Maximum Likelihood Estimation
Graubard and Korn230 and Fitzmaurice191 describe the kinds of situations in
which the working independence model can be trusted.
Minkin,453 Alho,11 Doganaksoy and Schmee,157 and Meeker and Escobar445
discuss the need for LR and score-based confidence intervals. Alho found that
score-based intervals are usually more tedious to compute, and provided useful
algorithms for the computation of either type of interval (see also [445] and [437,
p. 167]). Score and LR intervals require iterative computations and have to deal
with the fact that when one parameter is changed (e.g., bi is restricted to be
zero), all other parameter estimates change. DiCiccio and Efron154 provide a
method for very accurate confidence intervals for exponential families that requires a modest amount of additional computation. Venzon and Moolgavkar
provide an efficient general method for computing LR-based intervals.628 Brazzale and Davison64 developed some promising and feasible ways to make unconditional likelihood-based inferences more accurate in small samples.
Carpenter and Bithell89 have an excellent overview of several variations on the
bootstrap for obtaining confidence limits.
Tibshirani and Knight603 developed an easy to program approach for deriving
simultaneous confidence sets that is likely to be useful for getting simultaneous confidence regions for the entire vector of model parameters, for population
values for an entire sequence of predictor values, and for a set of regression
e↵ects (e.g., interquartile-range odds ratios for age for both sexes). The basic
idea is that during the, say, 1000 bootstrap repetitions one stores the 2 log
likelihood for each model fit, being careful to compute the likelihood at the
current bootstrap parameter estimates but with respect to the original data
matrix, not the bootstrap sample of the data matrix. To obtain an approximate simultaneous 0.95 confidence set one computes the 0.95 quantile of the
2 log likelihood values and determines which vectors of parameter estimates
correspond to 2 log likelihoods that are at least as small as the 0.95 quantile
of all 2 log likelihoods. Once the qualifying parameter estimates are found,
the quantities of interest are computed from those parameter estimates and an
outer envelope of those quantities is found. Computations are facilitated with
the rms package confplot function.
van Houwelingen and le Cessie [625, Eq. 52] showed, consistent with AIC, that
the average optimism in a mean logarithmic (minus log likelihood) quality score
for logistic models is p/n.
Schwarz555 derived a di↵erent penalty using large-sample Bayesian properties
of competing models. His Bayesian Information Criterion (BIC) chooses the
model having the lowest value of L + 1/2p log n or the highest value of LR
2
p log n. Kass and Raftery have done several studies of BIC.330 Smith
and Spiegelhalter572 and Laud and Ibrahim369 discussed other useful generalizations of likelihood penalties. Zheng and Loh681 studied several penalty
measures, and found that AIC does not penalize enough for overfitting in the
ordinary regression case. Kass and Raftery [330, p. 790] provide a nice review
of this topic, stating that “AIC picks the correct model asymptotically if the
complexity of the true model grows with sample size” and that “AIC selects
models that are too big even when the sample size is large.” But they also cite
other papers that show the existence of cases where AIC can work better than
BIC. According to Buckland et al.,79 BIC “assumes that a true model exists
and is low-dimensional.”
Hurvich and Tsai307, 308 made an improvement in AIC that resulted in much
better model selection for small n. They defined the corrected AIC as
AICC = LR

2

2p[1 +

n

p+1
].
p 1

(9.66)

9.11 Further Reading

12

13
14

15

16
17

18

19

20

21
22

23

217

In [307] they contrast asymptotically efficient model selection with AIC when
the true model has infinitely many parameters with improvements using other
indexes such as AICC when the model is finite.
One difficulty in applying the Schwarz, AICC , and related criteria is that with
censored or binary responses it is not clear that the actual sample size n should
be used in the formula.
Goldstein,218 Willan et al.,663 and Royston and Thompson529 have nice discussions on comparing non-nested regression models. Schemper’s method544 is
useful for testing whether a set of variables provides significantly greater information (using an R2 measure) than another set of variables.
van Houwelingen and le Cessie [625, Eq. 22] recommended using L/2 (also called
the Kullback–Leibler error rate) as a quality index.
Schemper544 provides a bootstrap technique for testing for significant di↵erences between correlated R2 measures. Mittlböck and Schemper,454 Schemper
and Stare,549 Korn and Simon,358, 359 Menard,447 and Zheng and Agresti680
have excellent discussions about the pros and cons of various indexes of the
predictive value of a model.
Al-Radi et al.10 presented another analysis comparing competing predictors
using the adequacy index and a receiver operating characteristic curve area
approach based on a test for whether one predictor has a higher probability of
being “more concordant” than another.
[54, 94, 402] provide good variance–covariance estimators from a weighted maximum likelihood analysis.
Huang and Harrington303 developed penalized partial likelihood estimates for
Cox models and provided useful background information and theoretical results
about improvements in mean squared errors of regression estimates. They used
a bootstrap error estimate for selection of the penalty parameter.
Sardy533 proposes that the square roots of the diagonals of the inverse of the
covariance matrix for the predictors be used for scaling rather than the standard
deviations.
Park and Hastie476 and articles referenced therein describe how quadratic penalized logistic regression automatically sets coefficient estimates for empty cells
to zero and forces the sum of k coefficients for a k-level categorical predictor to
equal zero.
Greenland236 has a nice discussion of the relationship between penalized maximum likelihood estimation and mixed e↵ects models. He cautions against estimating the shrinkage parameter.
See303 for a bootstrap approach to selection of .
Verweij and van Houwelingen [631, Eq. 4] derived another expression for d.f., but
it requires more computation and did not perform any better than Equation 9.63
in choosing in several examples tested.
See van Houwelingen and Thorogood623 for an approximate empirical Bayes
approach to shrinkage. See Tibshirani601 for the use of a non-smooth penalty
function that results in variable selection as well as shrinkage (see Section 4.3).
Verweij and van Houwelingen632 used a “cross-validated likelihood” based on
leave-out-one estimates to penalize for overfitting. Wang and Taylor646 presented some methods for carrying out hypothesis tests and computing confidence limits under penalization. Moons et al.455 presented a case study of
penalized estimation and discussed the advantages of penalization.

218

9 Overview of Maximum Likelihood Estimation

Table 9.5

Variables in Model
age
sex
age, sex
age2
age, age2
age, age2 , sex

LR 2
100
108
111
60
102
115

9.12 Problems
1. A sample of size 100 from a normal distribution with unknown mean and
standard deviation (µ and ) yielded the following log likelihood values
when computed at two values of µ.
log L(µ = 10,

= 5) =

800

log L(µ = 20,

= 5) =

820.

What do you know about µ? What do you know about Y ?
2. Several regression models were considered for predicting a response. LR 2
(corrected for the intercept) for models containing various combinations of
variables are found in Table 9.5. Compute all possible meaningful LR 2 .
For each, state the d.f. and an approximate P -value. State which LR 2
involving only one variable is not very meaningful.
3. For each problem below, rank Wald, score, and LR statistics by overall
statistical properties and then by computational convenience.
a. A forward stepwise variable selection (to be later accounted for with
the bootstrap) is desired to determine a concise model that contains
most of the independent information in all potential predictors.
b. A test of independent association of each variable in a given model
(each variable adjusted for the e↵ects of all other variables in the given
model) is to be obtained.
c. A model that contains only additive e↵ects is fitted. A large number
of potential interaction terms are to be tested using a global (multiple
d.f.) test.
4. Consider a univariate saturated model in 3 treatments (A, B, C) that is
quadratic in age. Write out the model with all the s, and write in detail
the contrast for comparing treatment B with treatment C for 30 year
olds. Sketch out the same contrast using the “di↵erence in predictions”
approach without simplification.
5. Simulate a binary logistic model for n = 300 with an average fraction of
events somewhere between 0.15 and 0.3. Use 5 continuous covariates and

9.12 Problems

219

assume the model is everywhere linear. Fit an unpenalized model, then
solve for the optimum quadratic penalty . Relate the resulting e↵ective
d.f. to the 15:1 rule of thumb, and compute the heuristic shrinkage coefficient ˆ for the unpenalized model and for the optimally penalized model,
inserting the e↵ective d.f. for the number of non-intercept parameters in
the model.
6. For a similar setup as the binary logistic model simulation in Section 9.7,
do a Monte Carlo simulation to determine the coverage probabilities for
ordinary Wald and for three types of bootstrap confidence intervals for the
true x=5 to x=1 log odds ratio. In addition, consider the Wald-type confidence interval arising from the sandwich covariance estimator. Estimate
the non-coverage probabilities in both tails. Use a sample size n = 200
with the single predictor x1 having a standard log-normal distribution,
and the true model being logit(Y = 1) = 1 + x1 /2. Determine whether
increasing the sample size relieves any problem you observed. Some R code
for this simulation is on the web site.

Chapter 10

Binary Logistic Regression

10.1 Model
Binary responses are commonly studied in many fields. Examples include
the presence or absence of a particular disease, death during surgery, or
a consumer purchasing a product. Often one wishes to study how a set of
predictor variables X is related to a dichotomous response variable Y . The
predictors may describe such quantities as treatment assignment, dosage, risk
factors, and calendar time.
For convenience we define the response to be Y = 0 or 1, with Y = 1
denoting the occurrence of the event of interest. Often a dichotomous outcome
can be studied by calculating certain proportions, for example, the proportion
of deaths among females and the proportion among males. However, in many
situations, there are multiple descriptors, or one or more of the descriptors
are continuous. Without a statistical model, studying patterns such as the
relationship between age and occurrence of a disease, for example, would
require the creation of arbitrary age groups to allow estimation of disease
prevalence as a function of age.
Letting X denote the vector of predictors {X1 , X2 , . . . , Xk }, a first attempt
at modeling the response might use the ordinary linear regression model
E{Y |X} = X ,

(10.1)

since the expectation of a binary variable Y is Prob{Y = 1}. However, such
a model by definition cannot fit the data over the whole range of the predictors since a purely linear model E{Y |X} = Prob{Y = 1|X} = X can
allow Prob{Y = 1} to exceed 1 or fall below 0. The statistical model that is
generally preferred for the analysis of binary responses is instead the binary
logistic regression model, stated in terms of the probability that Y = 1 given
X, the values of the predictors:
Prob{Y = 1|X} = [1 + exp( X )]

1

.

(10.2)

221

1

222

As before, X stands for 0 + 1 X1 + 2 X2 + . . . + k Xk . The binary logistic regression model was developed primarily by Cox126 and Walker and
Duncan.639 The regression parameters
are estimated by the method of
maximum likelihood (see below).
The function
P = [1 + exp( x)] 1
(10.3)
is called the logistic function. This function is plotted in Figure 10.1 for x
varying from 4 to +4. This function has an unlimited range for x while P
is restricted to range from 0 to 1.
1.0
0.8
0.6

P

2

10 Binary Logistic Regression

0.4
0.2
0.0
−4

−2

0

2

4

X
Fig. 10.1 Logistic function

For future derivations it is useful to express x in terms of P . Solving the
equation above for x by using
1

P = exp( x)/[1 + exp( x)]

(10.4)

yields the inverse of the logistic function:
x = log[P/(1

P )] = log[odds that Y = 1 occurs] = logit{Y = 1}. (10.5)

Other methods that have been used to analyze binary response data include the probit model, which writes P in terms of the cumulative normal
distribution, and discriminant analysis. Probit regression, although assuming a similar shape to the logistic function for the regression relationship
between X and Prob{Y = 1}, involves more cumbersome calculations, and
there is no natural interpretation of its regression parameters. In the past,
discriminant analysis has been the predominant method since it is the simplest computationally. However, it makes more assumptions than logistic re-

10.1 Model

223

gression. The model used in discriminant analysis is stated in terms of the
distribution of X given the outcome group Y , even though one is seldom interested in the distribution of the predictors per se. The discriminant model
has to be inverted using Bayes’ rule to derive the quantity of primary interest, Prob{Y = 1}. By contrast, the logistic model is a direct probability
model since it is stated in terms of Prob{Y = 1|X}. Since the distribution
of a binary random variable Y is completely defined by the true probability
that Y = 1 and since the model makes no assumption about the distribution of the predictors, the logistic model makes no distributional assumptions
whatsoever.

10.1.1 Model Assumptions and Interpretation of
Parameters
Since the logistic model is a direct probability model, its only assumptions
relate to the form of the regression equation. Regression assumptions are
verifiable, unlike the assumption of multivariate normality made by discriminant analysis. The logistic model assumptions are most easily understood by
transforming Prob{Y = 1} to make a model that is linear in X :
logit{Y = 1|X} = logit(P ) = log[P/(1

P )]

=X ,

(10.6)

where P = Prob{Y = 1|X}. Thus the model is a linear regression model in
the log odds that Y = 1 since logit(P ) is a weighted sum of the Xs. If all
e↵ects are additive (i.e., no interactions are present), the model assumes that
for every predictor Xj ,
logit{Y = 1|X} =
=

+

1 X1

j Xj

+ C,

0

+ ... +

j Xj

+ ... +

k Xk

(10.7)

where if all other factors are held constant, C is a constant given by
C=

0

+

1 X1

+ ... +

j 1 Xj 1

+

j+1 Xj+1

+ ... +

k Xk .

(10.8)

The parameter j is then the change in the log odds per unit change in
Xj if Xj represents a single factor that is linear and does not interact with
other factors and if all other factors are held constant. Instead of writing this
relationship in terms of log odds, it could just as easily be written in terms
of the odds that Y = 1:
odds{Y = 1|X} = exp(X ),
and if all factors other than Xj are held constant,

(10.9)

3

224

10 Binary Logistic Regression

odds{Y = 1|X} = exp( j Xj + C) = exp( j Xj ) exp(C).

(10.10)

The regression parameters can also be written in terms of odds ratios. The
odds that Y = 1 when Xj is increased by d, divided by the odds at Xj is
odds{Y = 1|X1 , X2 , . . . , Xj + d, . . . , Xk }
odds{Y = 1|X1 , X2 , . . . , Xj , . . . , Xk }
exp[ j (Xj + d)] exp(C)
=
[exp( j Xj ) exp(C)]
= exp[ j Xj + j d
j Xj ] = exp( j d).

(10.11)

Thus the e↵ect of increasing Xj by d is to increase the odds that Y = 1 by
a factor of exp( j d), or to increase the log odds that Y = 1 by an increment
of j d. In general, the ratio of the odds of response for an individual with
predictor variable values X ⇤ compared with an individual with predictors X
is
X ⇤ : X odds ratio = exp(X ⇤ )/ exp(X )
= exp[(X ⇤

X) ].

(10.12)

Now consider some special cases of the logistic multiple regression model.
If there is only one predictor X and that predictor is binary, the model can
be written
logit{Y = 1|X = 0} =

0

logit{Y = 1|X = 1} =

0

+

1.

(10.13)

Here 0 is the log odds of Y = 1 when X = 0. By subtracting the two
equations above, it can be seen that 1 is the di↵erence in the log odds
when X = 1 as compared with X = 0, which is equivalent to the log of the
ratio of the odds when X = 1 compared with the odds when X = 0. The
quantity exp( 1 ) is the odds ratio for X = 1 compared with X = 0. Letting
P 0 = Prob{Y = 1|X = 0} and P 1 = Prob{Y = 1|X = 1}, the regression
parameters are interpreted by
0

= logit(P 0 ) = log[P 0 /(1

1

= logit(P 1 )
1

= log[P /(1
= log{[P 1 /(1

P 0 )]

logit(P 0 )
P 1 )]

(10.14)

log[P 0 /(1

P 1 )]/[P 0 /(1

P 0 )]

P 0 )]}.

Since there are only two quantities to model and two free parameters,
there is no way that this two-sample model can’t fit; the model in this case
is essentially fitting two cell proportions. Similarly, if there are g 1 dummy
indicator Xs representing g groups, the ANOVA-type logistic model must
always fit.

10.1 Model

225

If there is one continuous predictor X, the model is
logit{Y = 1|X} =

+

0

1 X,

(10.15)

and without further modification (e.g., taking log transformation of the predictor), the model assumes a straight line in the log odds, or that an increase
in X by one unit increases the odds by a factor of exp( 1 ).
Now consider the simplest analysis of covariance model in which there are
two treatments (indicated by X1 = 0 or 1) and one continuous covariable
(X2 ). The simplest logistic model for this setup is
logit{Y = 1|X} =

0

+

1 X1

+

2 X2 ,

(10.16)

which can be written also as
logit{Y = 1|X1 = 0, X2 } =
logit{Y = 1|X1 = 1, X2 } =

0

+

2 X2

0

+

1

+

2 X2 .

(10.17)

The X1 = 1 : X1 = 0 odds ratio is exp( 1 ), independent of X2 . The odds
ratio for a one-unit increase in X2 is exp( 2 ), independent of X1 .
This model, with no term for a possible interaction between treatment
and covariable, assumes that for each treatment the relationship between X2
and log odds is linear, and that the lines have equal slope; that is, they are
parallel. Assuming linearity in X2 , the only way that this model can fail is
for the two slopes to di↵er. Thus, the only assumptions that need verification
are linearity and lack of interaction between X1 and X2 .
To adapt the model to allow or test for interaction, we write
logit{Y = 1|X} =

0

+

1 X1

+

2 X2

+

3 X3 ,

(10.18)

where the derived variable X3 is defined to be X1 X2 . The test for lack of
interaction (equal slopes) is H0 : 3 = 0. The model can be amplified as
logit{Y = 1|X1 = 0, X2 } =
logit{Y = 1|X1 = 1, X2 } =
=

0
0
0
0

+

2 X2

+

1 + 2 X2
0
2 X2 ,

+

+

3 X2

(10.19)

where 00 = 0 + 1 and 20 = 2 + 3 . The model with interaction is therefore
equivalent to fitting two separate logistic models with X2 as the only predictor, one model for each treatment group. Here the X1 = 1 : X1 = 0 odds
ratio is exp( 1 + 3 X2 ).

226

10 Binary Logistic Regression

Table 10.1

Without Risk Factor With Risk Factor
Probability Odds Odds Probability
.2
.25
.5
.33
.5
1
2
.67
.8
4
8
.89
.9
9
18
.95
.98
49
98
.99

10.1.2 Odds Ratio, Risk Ratio, and Risk Di↵erence
As discussed above, the logistic model quantifies the e↵ect of a predictor in
terms of an odds ratio or log odds ratio. An odds ratio is a natural description of an e↵ect in a probability model since an odds ratio can be constant.
For example, suppose that a given risk factor doubles the odds of disease.
Table 10.1 shows the e↵ect of the risk factor for various levels of initial risk.
Since odds have an unlimited range, any positive odds ratio will still yield
a valid probability. If one attempted to describe an e↵ect by a risk ratio, the
e↵ect can only occur over a limited range of risk (probability). For example, a
risk ratio of 2 can only apply to risks below .5; above that point the risk ratio
must diminish. (Risk ratios are similar to odds ratios if the risk is small.) Risk
di↵erences have the same difficulty; the risk di↵erence cannot be constant
and must depend on the initial risk. Odds ratios, on the other hand, can
describe an e↵ect over the entire range of risk. An odds ratio can, for example,
describe the e↵ect of a treatment independently of covariables a↵ecting risk.
Figure 10.2 depicts the relationship between risk of a subject without the
risk factor and the increase in risk for a variety of relative increases (odds
ratios). It demonstrates how absolute risk increase is a function of the baseline
risk. Risk increase will also be a function of factors that interact with the
risk factor, that is, factors that modify its relative e↵ect. Once a model is
developed for estimating Prob{Y = 1|X}, this model can easily be used to
estimate the absolute risk increase as a function of baseline risk factors as well
as interacting factors. Let X1 be a binary risk factor and let A = {X2 , . . . , Xp }
be the other factors (which for convenience we assume do not interact with
X1 ). Then the estimate of Prob{Y = 1|X1 = 1, A} Prob{Y = 1|X1 = 0, A}
is
1
1 + exp [ ˆ0 + ˆ1 + ˆ2 X2 + . . . + ˆp Xp ]
1
1 + exp [ ˆ0 + ˆ2 X2 + . . . + ˆp Xp ]

(10.20)

10.1 Model

227
0.6
10

Increase in Risk

0.5
0.4

5
4

0.3

3

0.2

2
1.75
1.5

0.1

1.25
1.1

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Risk for Subject Without Risk Factor
Fig. 10.2 Absolute benefit as a function of risk of the event in a control subject and
the relative e↵ect (odds ratio) of the risk factor. The odds ratios are given for each
curve.
Table 10.2
Females Age:
Response:
Males
Age:
Response:

37
0
34
1

39
0
38
1

39
0
40
0

42
0
40
0

47
0
41
0

=

48
0
43
1

48
1
43
1

52
0
43
1

53
0
44
0

55
0
46
0

56
0
47
1

57
0
48
1

58
0
48
1

1
1+

( 1 R̂R̂ ) exp(

ˆ1 )

58
1
50
0

60
0
50
1

64
0
52
1

65
1
55
1

68
1
60
1

68
1
61
1

70
1
61
1

R̂,

where R̂ is the estimate of the baseline risk, Prob{Y = 1|X1 = 0}. The risk
di↵erence estimate can be plotted against R̂ or against levels of variables in A
to display absolute risk increase against overall risk (Figure 10.2) or against
specific subject characteristics.

10.1.3 Detailed Example
Consider the data in Table 10.2. A graph of the data, along with a fitted
logistic model (described later), appears in Figure 10.3. The graph also displays proportions of responses obtained by stratifying the data by sex and

4

odds became popular
through gambling!

228

10 Binary Logistic Regression

age group (< 45, 45 54, 55). The age points on the abscissa for these
groups are the overall mean ages in the three age intervals (40.2, 49.1, and
61.1, respectively).
r e q u i r e ( rms )
getHdata ( s e x . a g e . r e s p o n s e )
d
sex.age.response
dd
d a t a d i s t ( d ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )
f
lrm ( r e s p o n s e ⇠ s e x + age , data=d )
fasr
f
# Save for later
w
function ( . . . )
with ( d , {
m
s e x== ’ male ’
f
s e x== ’ f e m a l e ’
l p o i n t s ( age [ f ] , r e s p o n s e [ f ] , pch =1)
l p o i n t s ( age [m] , r e s p o n s e [m] , pch =2)
af
c u t 2 ( age , c ( 4 5 , 5 5 ) , l e v e l s . m e a n=TRUE)
prop
t a p p l y ( r e s p o n s e , l i s t ( a f , s e x ) , mean ,
na.rm=TRUE)
agem
a s . n u m e r i c ( row.names ( prop ) )
l p o i n t s ( agem , prop [ , ’ f e m a l e ’ ] ,
pch =4 , c e x=1. 3 , c o l= ’ g r e e n ’ )
l p o i n t s ( agem , prop [ , ’ male ’ ] ,
pch =5 , c e x=1. 3 , c o l= ’ g r e e n ’ )
x
rep (62 , 4 ) ; y
s e q ( . 2 5 , . 1 , l e n g t h =4)
l p o i n t s ( x , y , pch=c ( 1 , 2 , 4 , 5 ) ,
c o l=r e p ( c ( ’ b l u e ’ , ’ g r e e n ’ ) , each =2))
l t e x t ( x+5 , y ,
c ( ’F Observed ’ , ’M Observed ’ ,
’F P r o p o r t i o n ’ , ’M P r o p o r t i o n ’ ) , c e x=. 8 )
} )
# Figure 10.3
p l o t ( P r e d i c t ( f , age=s e q ( 3 4 , 7 0 , l e n g t h =200) , sex , f u n=p l o g i s ) ,
y l a b= ’ Pr [ r e s p o n s e ] ’ , y l i m=c ( .02 , 1 . 0 2 ) , a d d p a n e l=w)
ltx
f u n c t i o n ( f i t ) l a t e x ( f i t , i n l i n e=TRUE, columns =54 ,
f i l e = ’ ’ , a f t e r= ’ $ . ’ , d i g i t s =3 ,
s i z e= ’ S s i z e ’ , b e f o r e= ’ $X\\ hat {\\ b e t a}= ’ )
ltx ( f )
Xˆ=

9.84 + 3.49[male] + 0.158 age.

Descriptive statistics for assessing the association between sex and response, age group and response, and age group and response stratified by sex
are found below. Corresponding fitted logistic models, with sex coded as 0 =
female, 1 = male are also given. Models were fitted first with sex as the only
predictor, then with age as the (continuous) predictor, then with sex and
age simultaneously. First consider the relationship between sex and response,
ignoring the e↵ect of age.

10.1 Model

229

●

●

●

●

●

male

Pr[response]

0.8

0.6

0.4

●

0.2
female
●

●

●

●●

40

●●

●●●●

50

●

60

F Observed
M Observed
F Proportion
M Proportion
●

70

age
Fig. 10.3 Data, subgroup proportions, and fitted logistic model, with 0.95 pointwise
confidence bands
sex
Frequency
Row Pct

response
0

1

Total

Odds/Log

F

14
70.00

6
30.00

20

6/14=.429
-.847

M

6
30.00

14
70.00

20

14/6=2.33
.847

20

20

40

Total

M:F odds ratio = (14/6)/(6/14) = 5.44, log=1.695

Statistics for sex ⇥ response
Statistic

d.f. Value

2

Likelihood Ratio

2

P

1 6.400 0.011
1 6.583 0.010

230

10 Binary Logistic Regression
Parameter Estimate Std Err Wald
0.8473 0.4880
1.6946 0.6901

0
1

2

P

3.0152
6.0305 0.0141

Note that the estimate of 0 , ˆ0 is the log odds for females and that ˆ1 is the
log odds (M:F) ratio. ˆ0 + ˆ1 = .847, the log odds for males. The likelihood
ratio test for H0 : no e↵ect of sex on probability of response is obtained as
follows.
Log likelihood ( 1 = 0) :
Log likelihood (max) :
LR 2 (H0 : 1 = 0)
:

27.727
24.435
2( 27.727

24.435) = 6.584.

(Note the agreement of the LR 2 with the contingency table likelihood ratio
2
, and compare 6.584 with the Wald statistic 6.03.)
Next, consider the relationship between age and response, ignoring sex.
age
Frequency
Row Pct

response
0

1

Total

Odds/Log

<45

8
61.5

5
38.4

13

5/8=.625
-.47

45-54

6
50.0

6
50.0

12

6/6=1
0

55+

6
40.0

9
60.0

15

9/6=1.5
.405

20

20

40

Total

55+ : <45 odds ratio = (9/6)/(5/8) = 2.4, log=.875

Parameter Estimate Std Err Wald
0
1

2.7338 1.8375
0.0540 0.0358

2

P

2.2134 0.1368
2.2763 0.1314

The estimate of 1 is in rough agreement with that obtained from the
frequency table. The 55+ : < 45 log odds ratio is .875, and since the respective
mean ages in the 55+ and <45 age groups are 61.1 and 40.2, an estimate of
the log odds ratio increase per year is .875/(61.1 40.2) = .875/20.9 = .042.
The likelihood ratio test for H0 : no association between age and response
is obtained as follows.
Log likelihood ( 1 = 0) :
Log likelihood (max) :
LR 2 (H0 : 1 = 0)
:

27.727
26.511
2( 27.727

26.511) = 2.432.

(Compare 2.432 with the Wald statistic 2.28.)
Next we consider the simultaneous association of age and sex with response.

10.1 Model

231
sex=F
age
Frequency
Row Pct

response
0

1

Total

4
100.0

0
0.0

4

45-54

4
80.0

1
20.0

5

55+

6
54.6

5
45.4

11

14

6

20

0

1

Total

<45

4
44.4

5
55.6

9

45-54

2
28.6

5
71.4

7

0
0.0

4
100.0

4

6

14

20

<45

Total

sex=M
age
Frequency
Row Pct

55+
Total

response

A logistic model for relating sex and age simultaneously to response is
given below.
Parameter Estimate Std Err Wald
0
1
2

(sex)
(age)

9.8429 3.6758
3.4898 1.1992
0.1581 0.0616

2

P

7.1706 0.0074
8.4693 0.0036
6.5756 0.0103

Likelihood ratio tests are obtained from the information below.
Log
Log
Log
Log
LR
LR
LR

likelihood ( 1 = 0, 2 = 0)
likelihood (max)
likelihood ( 1 = 0)
likelihood ( 2 = 0)
2
(H0 : 1 = 2 = 0)
2
(H0 : 1 = 0) sex|age
2
(H0 : 2 = 0) age|sex

:
:
:
:
:
:
:

27.727
19.458
26.511
24.435
2( 27.727
2( 26.511
2( 24.435

19.458) = 16.538
19.458) = 14.106
19.458) = 9.954.

The 14.1 should be compared with the Wald statistic of 8.47, and 9.954
should be compared with 6.58. The fitted logistic model is plotted separately
for females and males in Figure 10.3. The fitted model is
logit{Response = 1|sex,age} =

9.84 + 3.49 ⇥ sex + .158 ⇥ age,

(10.21)

232

10 Binary Logistic Regression

where as before sex = 0 for females, 1 for males. For example, for a 40-yearold female, the predicted logit is 9.84 + .158(40) = 3.52. The predicted
probability of a response is 1/[1 + exp(3.52)] = .029. For a 40-year-old male,
the predicted logit is 9.84 + 3.49 + .158(40) = .03, with a probability of
.492.

10.1.4 Design Formulations

5

The logistic multiple regression model can incorporate the same designs as
can ordinary linear regression. An analysis of variance (ANOVA) model for
a treatment with k levels can be formulated with k 1 dummy variables.
This logistic model is equivalent to a 2 ⇥ k contingency table. An analysis
of covariance logistic model is simply an ANOVA model augmented with
covariables used for adjustment.
One unique design that is interesting to consider in the context of logistic
models is a simultaneous comparison of multiple factors between two groups.
Suppose, for example, that in a randomized trial with two treatments one
wished to test whether any of 10 baseline characteristics are mal-distributed
between the two groups. If the 10 factors are continuous, one could perform a
two-sample Wilcoxon–Mann–Whitney test or a t-test for each factor (if each
is normally distributed). However, this procedure would result in multiple
comparison problems and would also not be able to detect the combined effect of small di↵erences across all the factors. A better procedure would be a
multivariate test. The Hotelling T 2 test is designed for just this situation. It
is a k-variable extension of the one-variable unpaired t-test. The T 2 test, like
discriminant analysis, assumes multivariate normality of the k factors. This
assumption is especially tenuous when some of the factors are polytomous. A
better alternative is the global test of no regression from the logistic model.
This test is valid because it can be shown that H0 : mean X is the same for
both groups (= H0 : mean X does not depend on group = H0 : mean X|
group = constant) is true if and only if H0 : Prob{group|X} = constant. Thus
k factors can be tested simultaneously for di↵erences between the two groups
using the binary logistic model, which has far fewer assumptions than does
the Hotelling T 2 test. The logistic global test of no regression (with k d.f.)
would be expected to have greater power if there is non-normality. Since the
logistic model makes no assumption regarding the distribution of the descriptor variables, it can easily test for simultaneous group di↵erences involving a
mixture of continuous, binary, and nominal variables. In observational studies, such models for treatment received or exposure (propensity score models)
hold great promise for adjusting for confounding.114, 372, 521, 525, 526
O’Brien472 has developed a general test for comparing group 1 with group
2 for a single measurement. His test detects location and scale di↵erences by
fitting a logistic model for Prob{Group 2} using X and X 2 as predictors.

10.2 Estimation

233

For a randomized study where adjustment for confounding is seldom necessary, adjusting for covariables using a binary logistic model results in increases
in standard errors of regression coefficients.522 This is the opposite of what
happens in linear regression where there is an unknown variance parameter
that is estimated using the residual squared error. Fortunately, adjusting for
covariables using logistic regression, by accounting for subject heterogeneity,
will result in larger regression coefficients even for a randomized treatment
variable. The increase in estimated regression coefficients more than o↵sets
the increase in standard error.

basis for propensity scores.
10.2 Estimation
10.2.1 Maximum Likelihood Estimates
The parameters in the logistic regression model are estimated using the maximum likelihood (ML) method. The method is based on the same principles
as the one-sample proportion example described in Section 9.1. The di↵erence is that the general logistic model is not a single sample or a two-sample
problem. The probability of response for the ith subject depends on a particular set of predictors Xi , and in fact the list of predictors may not be the
same for any two subjects. Denoting the response and probability of response
of the ith subject by Yi and Pi , respectively, the model states that
Pi = Prob{Yi = 1|Xi } = [1 + exp( Xi )]

1

.

(10.22)

The likelihood of an observed response Yi given predictors Xi and the unknown parameters is
PiYi [1 Pi ]1 Yi .
(10.23)
The joint likelihood of all responses Y1 , Y2 , . . . , Yn is the product of these
likelihoods for i = 1, . . . , n. The likelihood and log likelihood functions are
rewritten by using the definition of Pi above to allow them to be recognized
as a function of the unknown parameters . Except in simple special cases
(such as the k-sample problem in which all Xs are dummy variables), the
ML estimates (MLE) of cannot be written explicitly. The Newton–Raphson
method described in Section 9.4 is usually used to solve iteratively for the
list of values that maximize the log likelihood. The MLEs are denoted by
ˆ. The inverse of the estimated observed information matrix is taken as the
estimate of the variance–covariance matrix of ˆ.
Under H0 : 1 = 2 = . . . = k = 0, the intercept parameter 0 can be
estimated explicitly and the log likelihood under this global null hypothesis
can be computed explicitly. Under the global null hypothesis, Pi = P =
[1 + exp( 0 )] 1 and the MLE of P is P̂ = s/n where s is the number of

p-values are not great
for logistic models
because they are
symmetric (Wald) and
shouldn't be- should
use profile likelihood
CI - that is better.

234

10 Binary Logistic Regression

responses and n is the sample size. The MLE of
likelihood under this null hypothesis is
s log(P̂ ) + (n
=

s log(s/n) + (n

= s log s + (n

0

s) log(1

P̂ )

s) log[(n

s) log(n

s)

is ˆ0 = logit(P̂ ). The log

s)/n]

(10.24)

n log(n).

6

10.2.2 Estimation of Odds Ratios and Probabilities
Once is estimated, one can estimate any log odds, odds, or odds ratios.
The MLE of the Xj + 1 : Xj log odds ratio is ˆj , and the estimate of the
Xj + d : Xj log odds ratio is ˆj d, all other predictors remaining constant
(assuming the absence of interactions and nonlinearities involving Xj ). For
large enough samples, the MLEs are normally distributed with variances that
are consistently estimated from the estimated variance–covariance matrix.
Letting z denote the 1 ↵/2 critical value of the standard normal distribution,
a two-sided 1 ↵ confidence interval for the log odds ratio for a one-unit
increase in Xj is [ ˆj zs, ˆj + zs], where s is the estimated standard error
of ˆj . (Note that for ↵ = .05, i.e., for a 95% confidence interval, z = 1.96.)
A theorem in statistics states that the MLE of a function of a parameter
is that same function of the MLE of the parameter. Thus the MLE of the
Xj + 1 : Xj odds ratio is exp( ˆj ). Also, if a 1 ↵ confidence interval of a
parameter is [c, d] and f (u) is a one-to-one function, a 1 ↵ confidence
interval of f ( ) is [f (c), f (d)]. Thus a 1 ↵ confidence interval for the Xj +1 :
Xj odds ratio is exp[ ˆj ± zs]. Note that while the confidence interval for j is
symmetric about ˆj , the confidence interval for exp( j ) is not. By the same
theorem just used, the MLE of Pi = Prob{Yi = 1|Xi } is
P̂i = [1 + exp( Xi ˆ)]

7

1

.

(10.25)

A confidence interval for Pi could be derived by computing the standard
error of P̂i , yielding a symmetric confidence interval. However, such an interval would have the disadvantage that its endpoints could fall below zero
or exceed one. A better approach uses the fact that for large samples X ˆ
is approximately normally distributed. An estimate of the variance of X ˆ
in matrix notation is XV X 0 where V is the estimated variance–covariance
matrix of ˆ (see Equation 9.51). This variance is the sum of all variances and
covariances of ˆ weighted by squares and products of the predictors. The estimated standard error of X ˆ, s, is the square root of this variance estimate.
A 1 ↵ confidence interval for Pi is then

10.2 Estimation

235

{1 + exp[ (Xi ˆ ± zs)]}

1

.

(10.26)

10.2.3 Minimum Sample Size Requirement
Suppose there were no covariates, so that the only parameter in the model is
the intercept. What is the sample size required to allow the estimate of the
intercept to be precise enough so that the predicted probability is within 0.1
of the true probability with 0.95 confidence, when the true intercept is in the
neighborhood of zero? The answer is n=96. What if there were one covariate,
and it was binary with a prevalence of 12 ? One would need 96 subjects with
X = 0 and 96 with X = 1 to have an upper bound on the margin of error
for estimating Prob{Y = 1|X = x} not exceed 0.1 for either value of xa .
Now consider a very simple single continuous predictor case in which X
has a normal distribution with mean zero and standard deviation , with the
true Prob{Y = 1|X = x} = [1 + exp( x)] 1 . The expected number of events
is n2 b . The following simulation answers the question “What should n be so
that the expected maximum absolute error (over x 2 [ 1.5, 1.5]) in P̂ is less
than ✏?”
sigmas
ns
nsim
xs
pactual

c ( .5 , .75 , 1 , 1 .25 , 1 .5 , 1 .75 , 2 , 2 .5 , 3 , 4)
s e q ( 2 5 , 3 0 0 , by=25)
1000
s e q ( 1.5 , 1 . 5 , l e n g t h =200)
p l o g i s ( xs )

dn
l i s t ( sigma=f o r m a t ( s i g m a s ) , n=f o r m a t ( ns ) )
maxerr
N1
a r r a y (NA, c ( l e n g t h ( s i g m a s ) , l e n g t h ( ns ) ) , dn )
r e q u i r e ( rms )
i
0
f o r ( s in sigmas ) {
i
i + 1
j
0
f o r ( n i n ns ) {
j
j + 1
n1
maxe
0
f o r ( k i n 1 : nsim ) {
x
rnorm ( n , 0 , s )
P
plogis (x)
y
i f e l s e ( r u n i f (n)  P, 1 , 0)
a

The general formula for the sample size required to achieve a margin of error of in
estimating a true probability of ✓ at the 0.95 confidence level is n = ( 1.96 )2 ⇥✓(1 ✓).
Set ✓ = 12 (intercept=0) for the worst case.
b
The R code can easily be modified for other event frequencies, or the minimum of
the number of events and non-events for a dataset at hand can be compared with n
2
in this simulation. An average maximum absolute error of 0.05 corresponds roughly
to a half-width of the 0.95 confidence interval of 0.1.

236

10 Binary Logistic Regression
n1
beta
phat
maxe

n1 + sum ( y )
l r m . f i t (x , y)$ c o e f f i c i e n t s
p l o g i s ( b e t a [ 1 ] + b e t a [ 2 ] ⇤ xs )
maxe + max( abs ( phat
pactual ))

}
n1
n1 / nsim
maxe
maxe/ nsim
maxerr [ i , j ]
maxe
N1 [ i , j ]
n1
}
}
xrange
simerr

r a n g e ( xs )
l l i s t (N1 , maxerr , sigmas , ns , nsim , x r a n g e )

maxe
r e S h a p e ( maxerr )
# Figure 10.4
xYplot ( maxerr ⇠ n , g r o u p s=sigma , data=maxe ,
y l a b=e x p r e s s i o n ( p a s t e ( ’ Average Maximum ’ ,
abs ( hat (P)
P) ) ) ,
t y p e= ’ l ’ , l t y=r e p ( 1 : 2 , 5 ) , l a b e l . c u r v e=FALSE,
a b l i n e= l i s t ( h=c ( . 1 5 , . 1 , . 0 5 ) , c o l=g r a y ( . 8 5 ) ) )
Key ( . 8 , . 6 8 , o t h e r= l i s t ( c e x=. 7 ,
t i t l e =e x p r e s s i o n (⇠⇠⇠⇠⇠⇠⇠⇠⇠⇠⇠sigma ) ) )

10.3 Test Statistics
The likelihood ratio, score, and Wald statistics discussed earlier can be used
to test any hypothesis in the logistic model. The likelihood ratio test is generally preferred. When true parameters are near the null values all three
statistics usually agree. The Wald test has a significant drawback when the
true parameter value is very far from the null value. In such case the standard error estimate becomes too large. As ˆj increases from 0, the Wald test
statistic for H0 : j = 0 becomes larger, but after a certain point it becomes
smaller. The statistic will eventually drop to zero if ˆj becomes infinite.274
Infinite estimates can occur in the logistic model especially when there is a
binary predictor whose mean is near 0 or 1. Wald statistics are especially
problematic in this case. For example, if 10 out of 20 males had a disease and
5 out of 5 females had the disease, the female : male odds ratio is infinite and
so is the logistic regression coefficient for sex. If such a situation occurs, the
likelihood ratio or score statistic should be used instead of the Wald statistic.
For k-sample (ANOVA-type) logistic models, logistic model statistics are
equivalent to contingency table 2 statistics. As exemplified in the logistic
model relating sex to response described previously, the global likelihood
ratio statistic for all dummy variables in a k-sample model is identical to the
contingency table (k-sample binomial) likelihood ratio 2 statistic. The score

10.4 Residuals

237

^
Average Maximum P − P

0.25

σ
0.5
0.75
1
1.25
1.5
1.75
2
2.5
3
4

0.20

0.15

0.10

0.05
50

100

150

200

250

300

n
Fig. 10.4 Simulated expected maximum error in estimating probabilities for x 2
[ 1.5, 1.5] with a single normally distributed X with mean zero

statistic for this same situation turns out to be identical to the k 1 degrees
of freedom Pearson 2 for a k ⇥ 2 table.
As mentioned in Section 2.6, it can be dangerous to interpret individual
parameters, make pairwise treatment comparisons, or test linearity if the
overall test of association for a factor represented by multiple parameters is
insignificant.

10.4 Residuals
Several types of residuals can be computed for binary logistic model fits.
Many of these residuals are used to examine the influence of individual observations on the fit. The partial residual is useful for directly assessing how
each predictor should be transformed. For the ith observation, the partial
residual for the jth element of X is defined by

8

238

10 Binary Logistic Regression

rij = ˆj Xij +

9

Yi
P̂i (1

P̂i
P̂i )

,

(10.27)

where Xij is the value of the jth variable in the ith observation, Yi is the
corresponding value of the response, and P̂i is the predicted probability that
Yi = 1. A smooth plot (using, e.g., loess) of Xij against rij will provide an
estimate of how Xj should be transformed, adjusting for the other Xs (using
their current transformations). Typically one tentatively models Xj linearly
and checks the smoothed plot for linearity. A U -shaped relationship in this
plot, for example, indicates that a squared term or spline function needs to
be added for Xj . This approach does assume additivity of predictors.

10.5 Assessment of Model Fit
As the logistic regression model makes no distributional assumptions, only
the assumptions of linearity and additivity need to be verified (in addition
to the usual assumptions about independence of observations and inclusion
of important covariables). In ordinary linear regression there is no global
test for lack of model fit unless there are replicate observations at various
settings of X. This is because ordinary regression entails estimation of a
separate variance parameter 2 . In logistic regression there are global tests
for goodness of fit. Unfortunately, some of the most frequently used ones are
inappropriate. For example, it is common to see a deviance test of goodness
of fit based on the “residual” log likelihood, with P -values obtained from a 2
distribution with n p d.f. This P -value is inappropriate since the deviance
does not have an asymptotic 2 distribution, due to the facts that the number
of parameters estimated is increasing at the same rate as n and the expected
cell frequencies are far below five (by definition).
Hosmer and Lemeshow297 have developed a commonly used test for goodness of fit for binary logistic models based on grouping into deciles of predicted probability and performing an ordinary 2 test for the mean predicted
probability against the observed fraction of events (using 8 d.f. to account
for evaluating fit on the model development sample). The Hosmer–Lemeshow
test is fairly dependent on the choice of how predictions are grouped296 and
it is not clear that the choice of the number of groups should be independent of n. Hosmer et al.296 have compared a number of global goodness of
fit tests for binary logistic regression. They concluded that the simple unweighted sum of squares test of Copas121 as modified by le Cessie and van
Houwelingen379 is as good as any. They used a normal Z-test for the sum of
squared errors (n ⇥ B, where B is the Brier index in Equation 10.35). This
test takes into account the fact that one cannot obtain a 2 distribution for
the sum of squares. It also takes into account the estimation of . It is not yet
clear for which types of lack of fit this test has reasonable power. Returning

<-arbitrary, low power,
does not reveal the
culprits. HosmerLemeshow tried in
multiple packages (SAS,
STATA, R...) all gave
different results based on
what is considered a
decile.

10.5 Assessment of Model Fit

239

to the external validation case where uncertainty of does not need to be
accounted for, Stallard580 has further documented the lack of power of the
original Hosmer-Lemeshow test and found more power with a logarithmic
scoring rule (deviance test) and a 2 test that, unlike the simple unweighted
sum of squares test, weights each squared error by dividing it by P̂i (1 P̂i ).
A scaled 2 distribution seemed to provide the best approximation to the
null distribution of the test statistics.
More power for detecting lack of fit is expected to be obtained from testing
specific alternatives to the model. In the model
logit{Y = 1|X} =

0

+

1 X1

+

2 X2 ,

(10.28)

where X1 is binary and X2 is continuous, one needs to verify that the log
odds is related to X1 and X2 according to Figure 10.5.

logit{Y=1}

X1 = 1

X1 = 0

percent classified
correctly has lots of
problems!
X2
Fig. 10.5 Logistic regression assumptions for one binary and one continuous predictor

The simplest method for validating that the data are consistent with the
no-interaction linear model involves stratifying the sample by X1 and quantile groups (e.g., deciles) of X2 .260 Within each stratum the proportion of
responses P̂ is computed and the log odds calculated from log[P̂ /(1 P̂ )].
The number of quantile groups should be such that there are at least 20 (and
perhaps many more) subjects in each X1 ⇥ X2 group. Otherwise, probabilities cannot be estimated precisely enough to allow trends to be seen above
“noise” in the data. Since at least 3 X2 groups must be formed to allow as-

240

10 Binary Logistic Regression

sessment of linearity, the total sample size must be at least 2 ⇥ 3 ⇥ 20 = 120
for this method to work at all.
Figure 10.6 demonstrates this method for a large sample size of 3504 subjects stratified by sex and deciles of age. Linearity is apparent for males while
there is evidence for slight interaction between age and sex since the age trend
for females appears curved.
getHdata ( a c a t h )
acath $ sex
f a c t o r ( a c a t h $ sex , 0 : 1 , c ( ’ male ’ , ’ f e m a l e ’ ) )
dd
d a t a d i s t ( a c a t h ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )
f
lrm ( s i g d z ⇠ r c s ( age , 4 ) ⇤ sex , data=a c a t h )
w

function ( . . . )
with ( acath , {
plsmo ( age , s i g d z , group=sex , f u n=q l o g i s , l t y= ’ d o t t e d ’ ,
add=TRUE, g r i d=TRUE)
af
c u t 2 ( age , g =10 , l e v e l s . m e a n=TRUE)
prop
q l o g i s ( t a p p l y ( s i g d z , l i s t ( a f , s e x ) , mean ,
na.rm=TRUE) )
agem
a s . n u m e r i c ( row.names ( prop ) )
l p o i n t s ( agem , prop [ , ’ f e m a l e ’ ] , pch =4 , c o l= ’ g r e e n ’ )
l p o i n t s ( agem , prop [ , ’ male ’ ] ,
pch =2 , c o l= ’ g r e e n ’ )
} )
# Figure 10.6
p l o t ( P r e d i c t ( f , age , s e x ) , y l i m=c ( 2 , 4 ) , a d d p a n e l=w,
l a b e l . c u r v e= l i s t ( o f f s e t=u n i t ( 0 . 5 , ’cm ’ ) ) )

10

The subgrouping method requires relatively large sample sizes and does
not use continuous factors e↵ectively. The ordering of values is not used at all
between intervals, and the estimate of the relationship for a continuous variable has little resolution. Also, the method of grouping chosen (e.g., deciles
vs. quintiles vs. rounding) can alter the shape of the plot.
In this dataset with only two variables, it is efficient to use a nonparametric smoother for age, separately for males and females. Nonparametric
smoothers, such as loess108 used here, work well for binary response variables (see Section 2.4.7); the logit transformation is made on the smoothed
probability estimates. The smoothed estimates are shown in Figure 10.6.
When there are several predictors, the restricted cubic spline function is
better for estimating the true relationship between X2 and logit{Y = 1} for
continuous variables without assuming linearity. By fitting a model containing
X2 expanded into k 1 terms, where k is the number of knots, one can obtain
an estimate of the transformation of X2 as discussed in Section 2.4:
logit{Y = 1|X} = ˆ0 + ˆ1 X1 + ˆ2 X2 + ˆ3 X20 + ˆ4 X200
= ˆ0 + ˆ1 X1 + f (X2 ),

(10.29)

where X20 and X200 are constructed spline variables (when k = 4). Plotting
the estimated spline function f (X2 ) versus X2 will estimate how the e↵ect of
X2 should be modeled. If the sample is sufficiently large, the spline function
can be fitted separately for X1 = 0 and X1 = 1, allowing detection of even

10.5 Assessment of Model Fit

241

log odds

3
male

2
1
0
−1

female
30

40

50

60

70

80

Age, Year
Fig. 10.6 Logit proportions of significant coronary artery disease by sex and deciles
of age for n=3504 patients, with spline fits (smooth curves). Spline fits are for k = 4
knots at age= 36, 48, 56, and 68 years, and interaction between age and sex is
allowed. Shaded bands are pointwise 0.95 confidence limits for predicted log odds.
Smooth nonparametric estimates are shown as dotted curves. Data courtesy of the
Duke Cardiovascular Disease Databank.

unusual interaction patterns. A formal test of linearity in X2 is obtained by
testing H0 : 3 = 4 = 0.
For testing interaction between X1 and X2 , a product term (e.g., X1 X2 )
can be added to the model and its coefficient tested. A more general simultaneous test of linearity and lack of interaction for a two-variable model in
which one variable is binary (or is assumed linear) is obtained by fitting the
model
logit{Y = 1|X} =
+

0

+

1 X1

5 X1 X2

+

0
00
2 X2 + 3 X2 + 4 X2
0
00
6 X1 X2 + 7 X1 X2

+

(10.30)

and testing H0 : 3 = . . . = 7 = 0. This formulation allows the shape of the
X2 e↵ect to be completely di↵erent for each level of X1 . There is virtually
no departure from linearity and additivity that cannot be detected from this
expanded model formulation. The most computationally efficient test for lack
of fit is the score test (e.g., X1 and X2 are forced into a tentative model and
the remaining variables are candidates). Figure 10.6 also depicts a fitted
spline logistic model with k = 4, allowing for general interaction between
age and sex as parameterized above. The fitted function, after expanding the
restricted cubic spline function for simplicity (see Equation 2.27), is given

242

10 Binary Logistic Regression

Table 10.3
Model / Hypothesis

Likelihood
Ratio 2
a: sex, age (linear, no interaction) 766.0
b: sex, age, age ⇥ sex
768.2
c: sex, spline in age
769.4
d: sex, spline in age, interaction
782.5
H0 : no age ⇥ sex interaction
2.2
given linearity
H0 : age linear | no interaction
3.4
H0 : age linear, no interaction
16.6
H0 : age linear, product form
14.4
interaction
13.1
H0 : no interaction, allowing for
nonlinearity in age

d.f. P
2
3
4
7
1 .14

Formula

(b

a)

2 .18 (c
5 .005 (d
4 .006 (d

a)
a)
b)

3 .004 (d

c)

understand every number in this table.
above. Note the good agreement between the empirical estimates of log odds
and the spline fits and nonparametric estimates in this large dataset.
An analysis of log likelihood for this model and various sub-models is found
in Table 10.3. The 2 for global tests is corrected for the intercept and the
degrees of freedom does not include the intercept.
This analysis confirms the first impression from the graph, namely, that
age ⇥ sex interaction is present but it is not of the form of a simple product
between age and sex (change in slope). In the context of a linear age e↵ect,
there is no significant product interaction e↵ect (P = .14). Without allowing
for interaction, there is no significant nonlinear e↵ect of age (P = .18). However, the general test of lack of fit with 5 d.f. indicates a significant departure
from the linear additive model (P = .005).
In Figure 10.7, data from 2332 patients who underwent cardiac catheterization at Duke University Medical Center and were found to have significant
( 75%) diameter narrowing of at least one major coronary artery were analyzed (the dataset is available from the Web site). The relationship between
the time from the onset of symptoms of coronary artery disease (e.g., angina,
myocardial infarction) to the probability that the patient has severe (threevessel disease or left main disease—tvdlm) coronary disease was of interest.
There were 1129 patients with tvdlm. A logistic model was used with the
duration of symptoms appearing as a restricted cubic spline function with
k = 3, 4, 5, and 6 equally spaced knots in terms of quantiles between .05 and
.95. The best fit for the number of parameters was chosen using Akaike’s
information criterion (AIC), computed in Table 10.4 as the model likelihood
ratio 2 minus twice the number of parameters in the model aside from the
intercept. The linear model is denoted k = 0.
dz
dd

s u b s e t ( acath , s i g d z ==1)
d a t a d i s t ( dz )

10.5 Assessment of Model Fit

243

Table 10.4

k Model 2
0 99.23
3 112.69
4 121.30
5 123.51
6 124.41

AIC
97.23
108.69
115.30
115.51
114.51

AIC penalizes by subtracting 2*df to chi sq
here.

lrm ( tvdlm ⇠ r c s ( c a d . d u r , 5 ) , data=dz )
function ( . . . )
with ( dz , {
plsmo ( c a d . d u r , tvdlm , f u n=q l o g i s , add=TRUE,
g r i d=TRUE, l t y= ’ d o t t e d ’ )
x
c u t 2 ( c a d . d u r , g =15 , l e v e l s . m e a n=TRUE)
prop
q l o g i s ( t a p p l y ( tvdlm , x , mean , na.rm=TRUE) )
xm
a s . n u m e r i c ( names ( prop ) )
l p o i n t s (xm, prop , pch =2 , c o l= ’ g r e e n ’ )
} )
# Figure 10.7
p l o t ( P r e d i c t ( f , c a d . d u r ) , a d d p a n e l=w)

f
w

log odds

2
1
0
−1
0

100

200

300

Duration of Symptoms of Coronary Artery Disease
Fig. 10.7 Estimated relationship between duration of symptoms and the log odds
of severe coronary artery disease for k = 5. Knots are marked with arrows. Solid line
is spline fit; dotted line is a nonparametric loess estimate.

244

10 Binary Logistic Regression

Figure 10.7 displays the spline fit for k = 5. The triangles represent subgroup estimates obtained by dividing the sample into groups of 150 patients.
For example, the leftmost triangle represents the logit of the proportion of
tvdlm in the 150 patients with the shortest duration of symptoms, versus the
mean duration in that group. A Wald test of linearity, with 3 d.f., showed
highly significant nonlinearity ( 2 = 23.92 with 3 d.f.). The plot of the spline
transformation suggests a log transformation, and when log (duration of
symptoms in months + 1) was fitted in a logistic model, the log likelihood
of the model (119.33 with 1 d.f.) was virtually as good as the spline model
(123.51 with 4 d.f.); the corresponding Akaike information criteria (on the 2
scale) are 117.33 and 115.51. To check for adequacy in the log transformation,
a five-knot restricted cubic spline function was fitted to log10 (months + 1),
as displayed in Figure 10.8. There is some evidence for lack of fit on the right,
but the Wald 2 for testing linearity yields P = .27.
lrm ( tvdlm ⇠ l o g 1 0 ( c a d . d u r + 1 ) , data=dz )
function ( . . . )
with ( dz , {
x
c u t 2 ( c a d . d u r , m=150 , l e v e l s . m e a n=TRUE)
prop
t a p p l y ( tvdlm , x , mean , na.rm=TRUE)
xm
a s . n u m e r i c ( names ( prop ) )
l p o i n t s (xm, prop , pch =2 , c o l= ’ g r e e n ’ )
} )
#
Figure 10.8
p l o t ( P r e d i c t ( f , c a d . d u r , f u n=p l o g i s ) , y l a b= ’P ’ ,
y l i m=c ( . 2 , . 8 ) , a d d p a n e l=w)

f
w

If the model contains two continuous predictors, they may both be expanded with spline functions in order to test linearity or to describe nonlinear
relationships. Testing interaction is more difficult here. If X1 is continuous,
one might temporarily group X1 into quantile groups. Consider the subset
of 2258 (1490 with disease) of the 3504 patients used in Figure 10.6 who
have serum cholesterol measured. A logistic model for predicting significant
coronary disease was fitted with age in tertiles (modeled with two dummy
variables), sex, age ⇥ sex interaction, four-knot restricted cubic spline in
cholesterol, and age tertile ⇥ cholesterol interaction. Except for the sex adjustment this model is equivalent to fitting three separate spline functions in
cholesterol, one for each age tertile. The fitted model is shown in Figure 10.9
for cholesterol and age tertile against logit of significant disease. Significant
age ⇥ cholesterol interaction is apparent from the figure and is suggested by
the Wald 2 statistic (10.03) that follows. Note that the test for linearity of
the interaction with respect to cholesterol is very insignificant ( 2 = 2.40 on
4 d.f.), but we retain it for now. The fitted function is
acath

t r a n s f o r m ( acath ,
cholesterol = choleste ,
a g e . t e r t i l e = c u t 2 ( age , g =3) ,
sx = a s . i n t e g e r ( a c a t h $ s e x )
1)
# sx for loess, need to code as numeric
dd
d a t a d i s t ( a c a t h ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )

10.5 Assessment of Model Fit

245

0.7

P

0.6
0.5
0.4
0.3

0

100

200

300

Duration of Symptoms of Coronary Artery Disease
Fig. 10.8 Fitted linear logistic model in log10 (duration+1), with subgroup estimates using groups of 150 patients. Fitted equation is logit(tvdlm) = .9809 +
.7122 log10 (months + 1).

# First model stratifies age into tertiles to get more
# empirical estimates of age x cholesterol interaction
lrm ( s i g d z ⇠ a g e . t e r t i l e ⇤ ( s e x + r c s ( c h o l e s t e r o l , 4 ) ) ,
data=a c a t h )
p r i n t ( f , l a t e x=TRUE)
f

Logistic Regression Model
lrm(formula = sigdz ˜ age.tertile * (sex + rcs(cholesterol, 4)),
data = acath)
Frequencies of Missing Values Due to Each Variable
sigdz age.tertile
0
0

sex cholesterol
0
1246

246

10 Binary Logistic Regression

Model Likelihood
Ratio Test
Obs
2258 LR 2
533.52
0
768 d.f.
14
1
1490 Pr(> 2 ) < 0.0001
L
max | @ log
| 2⇥10 8
@

Intercept
age.tertile=[49,58)
age.tertile=[58,82]
sex=female
cholesterol
cholesterol’
cholesterol”
age.tertile=[49,58) * sex=female
age.tertile=[58,82] * sex=female
age.tertile=[49,58) * cholesterol
age.tertile=[58,82] * cholesterol
age.tertile=[49,58) * cholesterol’
age.tertile=[58,82] * cholesterol’
age.tertile=[49,58) * cholesterol”
age.tertile=[58,82] * cholesterol”

Discrimination
Indexes
R2
0.291
g
1.316
gr
3.729
gp
0.252
Brier
0.173

Coef
-0.4155
0.8781
4.7861
-1.6123
0.0029
0.0384
-0.1148
-0.7900
-0.4530
0.0011
-0.0158
-0.0183
0.0127
0.0582
-0.0092

Rank Discrim.
Indexes
C
0.780
Dxy
0.560
0.562
⌧a
0.251

S.E. Wald Z Pr(> |Z|)
1.0987
-0.38
0.7053
1.7337
0.51
0.6125
1.8143
2.64
0.0083
0.1751
-9.21 < 0.0001
0.0060
0.48
0.6347
0.0242
1.59
0.1126
0.0768
-1.49
0.1350
0.2537
-3.11
0.0018
0.2978
-1.52
0.1283
0.0095
0.11
0.9093
0.0099
-1.59
0.1111
0.0365
-0.50
0.6162
0.0406
0.31
0.7550
0.1140
0.51
0.6095
0.1301
-0.07
0.9436

ltx ( f )
X ˆ = 0.415+0.878[age.tertile 2 [49, 58)]+4.79[age.tertile 2 [58, 82]] 1.61[female]+
0.00287cholesterol+1.52⇥10 6 (cholesterol 160)3+ 4.53⇥10 6 (cholesterol 208)3+ +
3.44⇥10 6 (cholesterol 243)3+ 4.28⇥10 7 (cholesterol 319)3+ +[female][ 0.79[age.tertile 2
[49, 58)] 0.453 [age.tertile 2 [58, 82]]] + [age.tertile 2 [49, 58)][0.00108cholesterol
7.23⇥10 7 (cholesterol 160)3+ +2.3⇥10 6 (cholesterol 208)3+ 1.84⇥10 6 (cholesterol
243)3+ + 2.69⇥10 7 (cholesterol 319)3+ ] + [age.tertile 2 [58, 82]][ 0.0158cholesterol +
5⇥10 7 (cholesterol 160)3+ 3.64⇥10 7 (cholesterol 208)3+ 5.15⇥10 7 (cholesterol
243)3+ + 3.78⇥10 7 (cholesterol 319)3+ ].
# Table 10.6:
l a t e x ( anova ( f ) , f i l e = ’ ’ , s i z e= ’ s m a l l e r ’ ,
c a p t i o n= ’ Crudely c a t e g o r i z i n g age i n t o t e r t i l e s ’ ,
l a b e l= ’ tab : a n o v a t e r t i l e s ’ )
yl
c ( 1, 5 )
plot ( Predict ( f , cholesterol , a g e . t e r t i l e ) ,
a d j . s u b t i t l e=FALSE, y l i m=y l )
# Figure 10.9

Before fitting a parametric model that allows interaction between age and
cholesterol, let us use the local regression model of Cleveland et al.93 discussed in Section 2.4.7. This nonparametric smoothing method is not meant

10.5 Assessment of Model Fit

247

Table 10.6 Crudely categorizing age into tertiles
age.tertile (Factor+Higher Order Factors)
All Interactions
sex (Factor+Higher Order Factors)
All Interactions
cholesterol (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
age.tertile ⇥ sex (Factor+Higher Order Factors)
age.tertile ⇥ cholesterol (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

2
d.f.
P
120.74 10 < 0.0001
21.87 8 0.0052
329.54 3 < 0.0001
9.78 2 0.0075
93.75 9 < 0.0001
10.03 6 0.1235
9.96 6 0.1263
9.78 2 0.0075
10.03 6 0.1235
2.62 4 0.6237
2.62 4 0.6237
9.96 6 0.1263
21.87 8 0.0052
29.67 10 0.0010
410.75 14 < 0.0001

log odds

4
3

[58,82]

2
[49,58)

1
0

[17,49)
100

200

300

400

Cholesterol, mg %
Fig. 10.9 Log odds of significant coronary artery disease modeling age with two
dummy variables

248

10 Binary Logistic Regression

to handle binary Y , but it can still provide useful graphical displays in the
binary case. Figure 10.10 depicts the fit from a local regression model predicting Y = 1 = significant coronary artery disease. Predictors are sex (modeled
parametrically with a dummy variable), age, and cholesterol, the last two
fitted nonparametrically. The e↵ect of not explicitly modeling a probability
is seen in the figure, as the predicted probabilities exceeded 1. Because of this
we do not take the logit transformation but leave the predicted values in raw
form. However, the overall shape is in agreement with Figure 10.10.
# Re-do model with continuous age
f
l o e s s ( s i g d z ⇠ age ⇤ ( sx + c h o l e s t e r o l ) , data=acath ,
p a r a m e t r i c=” sx ” , d r o p . s q u a r e=” sx ” )
ages
seq (25 ,
7 5 , l e n g t h =40)
chols
s e q ( 1 0 0 , 4 0 0 , l e n g t h =40)
g
e x p a n d . g r i d ( c h o l e s t e r o l=c h o l s , age=ag es , sx =0)
# drop sex dimension of grid since held to 1 value
p
drop ( p r e d i c t ( f , g ) )
p [ p < 0 .001 ]
0 .001
p [ p > 0 .999 ]
0 .999
zl
c ( 3, 6)
# Figure 10.10
w i r e f r a m e ( q l o g i s ( p ) ⇠ c h o l e s t e r o l ⇤ age ,
x l a b= l i s t ( r o t =30) , y l a b= l i s t ( r o t= 40 ) ,
z l a b= l i s t ( l a b e l= ’ l o g odds ’ , r o t =90) , z l i m=z l ,
s c a l e s = l i s t ( a r r o w s = FALSE) , data=g )

Chapter 2 discussed linear splines, which can be used to construct linear
spline surfaces by adding all cross-products of the linear variables and spline
terms in the model. With a sufficient number of knots for each predictor, the
linear spline surface can fit a wide variety of patterns. However, it requires
a large number of parameters to be estimated. For the age–sex–cholesterol
example, a linear spline surface is fitted for age and cholesterol, and a sex
⇥ age spline interaction is also allowed. Figure 10.11 shows a fit that placed
knots at quartiles of the two continuous variablesc . The algebraic form of the
fitted model is shown below.
lrm ( s i g d z ⇠ l s p ( age , c ( 4 6 , 5 2 , 5 9 ) ) ⇤
( sex + l s p ( c h o l e s t e r o l , c (196 ,224 ,259))) ,
data=a c a t h )
ltx ( f )

f

X ˆ = 1.83 + 0.0232 age + 0.0759(age 46)+ 0.0025(age 52)+ + 2.27(age 59)+ +
0.131(cholesterol
3.02[female]
0.0177 cholesterol + 0.114(cholesterol
196)+
224)+ + 0.0651(cholesterol
259)+ + [female][ 0.112 age + 0.0852 (age
46)+
0.0302(age 52)+ +0.176(age 59)+ ]+age[0.000577cholesterol 0.00286(cholesterol
196)+ +0.00382(cholesterol 224)+ 0.00205(cholesterol 259)+ ]+(age 46)+ [ 0.000936cholesterol+
0.00643 (cholesterol 196)+ 0.0115 (cholesterol 224)+ + 0.00756 (cholesterol
259)+ ]+(age 52)+ [0.000433cholesterol 0.0037(cholesterol 196)+ +0.00815(cholesterol
c

In the wireframe plots that follow, predictions for cholesterol–age combinations for
which fewer than 5 exterior points exist are not shown, so as to not extrapolate to
regions not supported by at least five points beyond the data perimeter.

10.5 Assessment of Model Fit

249

log odds

6
4
2
0
−2
70

400
350
300
50
250
ag 40
rol
200
e
e
t
s
150
30
ole
100
ch
60

Fig. 10.10 Local regression fit for the logit of the probability of significant coronary
disease vs. age and cholesterol for males, based on the loess function.

224)+ 0.00715(cholesterol 259)+ ]+(age 59)+ [ 0.0124cholesterol+0.015(cholesterol
196)+ 0.0067 (cholesterol 224)+ + 0.00752 (cholesterol 259)+ ].
l a t e x ( anova ( f ) , c a p t i o n= ’ L i n e a r s p l i n e s u r f a c e ’ , f i l e = ’ ’ ,
s i z e= ’ s m a l l e r ’ , l a b e l= ’ tab : anova lsp ’ )
# Table 10.7
perim

with ( acath ,
p e r i m e t e r ( c h o l e s t e r o l , age , x i n c =20 , n=5))
zl
c ( 2, 4)
# Figure 10.11
b p l o t ( P r e d i c t ( f , c h o l e s t e r o l , age , np =40) , perim=perim ,
l f u n=w i r e f r a m e , z l i m=z l , a d j . s u b t i t l e=FALSE)

Chapter 2 also discussed a tensor spline extension of the restricted cubic
spline model to fit a smooth function of two predictors, f (X1 , X2 ). Since
this function allows for general interaction between X1 and X2 , the twovariable cubic spline is a powerful tool for displaying and testing interaction,
assuming the sample size warrants estimating 2(k 1) + (k 1)2 parameters
for a rectangular grid of k ⇥ k knots. Unlike the linear spline surface, the
cubic surface is smooth. It also requires fewer parameters in most situations.

250

10 Binary Logistic Regression

Table 10.7 Linear spline surface
2

age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
sex (Factor+Higher Order Factors)
All Interactions
cholesterol (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
age ⇥ sex (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
age ⇥ cholesterol (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
f(A,B) vs. Af(B) + Bg(A)
Nonlinear Interaction in age vs. Af(B)
Nonlinear Interaction in cholesterol vs. Bg(A)
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

164.17
42.28
25.21
343.80
23.90
100.13
16.27
16.35
23.90
12.97
12.97
16.27
11.45
11.45
9.38
9.99
10.75
33.22
42.28
49.03
449.26

d.f.
24
20
18
5
4
20
16
15
4
3
3
16
15
15
9
12
12
24
20
26
29

P
< 0.0001
0.0025
0.1192
< 0.0001
0.0001
< 0.0001
0.4341
0.3595
0.0001
0.0047
0.0047
0.4341
0.7204
0.7204
0.4033
0.6167
0.5503
0.0995
0.0025
0.0041
< 0.0001

The general cubic model with k = 4 (ignoring the sex e↵ect here) is
0

+

+
+

0
00
0
00
2 X1 + 3 X1 + 4 X2 + 5 X2 + 6 X2 + 7 X1 X2
0
00
0
0 0
(10.31)
8 X1 X2 + 9 X1 X2 + 10 X1 X2 + 11 X1 X2
0 00
00
00 0
00 00
12 X1 X2 + 13 X1 X2 + 14 X1 X2 + 15 X1 X2 ,

1 X1

+

+

where X10 , X100 , X20 , and X200 are restricted cubic spline component variables
for X1 and X2 for k = 4. A general test of interaction with 9 d.f. is H0 : 7 =
. . . = 15 = 0. A test of adequacy of a simple product form interaction is
H0 : 8 = . . . = 15 = 0 with 8 d.f. A 13 d.f. test of linearity and additivity
is H0 : 2 = 3 = 5 = 6 = 7 = 8 = 9 = 10 = 11 = 12 = 13 = 14 =
15 = 0 .
Figure 10.12 depicts the fit of this model. There is excellent agreement with
Figures 10.9 and 10.11, including an increased (but probably insignificant)
risk with low cholesterol for age 57.
lrm ( s i g d z ⇠ r c s ( age , 4 ) ⇤ ( s e x + r c s ( c h o l e s t e r o l , 4 ) ) ,
data=acath , t o l =1e 11 )
ltx ( f )

f

X ˆ = 6.41 + 0.166age 0.00067(age 36)3+ + 0.00543(age 48)3+ 0.00727(age
56)3+ +0.00251(age 68)3+ +2.87[female]+0.00979cholesterol+1.96⇥10 6 (cholesterol
160)3+ 7.16⇥10 6 (cholesterol 208)3+ + 6.35⇥10 6 (cholesterol 243)3+ 1.16⇥
10 6 (cholesterol 319)3+ +[female][ 0.109age+7.52⇥10 5 (age 36)3+ +0.00015(age

10.5 Assessment of Model Fit

251

4

log odds

3
2
1
0
−1
−2
70

Ag

400
350
300
g%
250
e, 50 40
,m
l
200
o
Ye
r
150
ar
30
ste
e
100
l
o
60

Ch

Fig. 10.11 Linear spline surface for males, with knots for age at 46, 52, 59 and knots
for cholesterol at 196, 224, and 259 (quartiles).

48)3+ 0.00045(age 56)3+ + 0.000225(age 68)3+ ] + age[ 0.00028cholesterol + 2.68⇥
10 9 (cholesterol 160)3+ + 3.03⇥10 8 (cholesterol 208)3+ 4.99⇥10 8 (cholesterol
243)3+ +1.69⇥10 8 (cholesterol 319)3+ ]+age0 [0.00341cholesterol 4.02⇥10 7 (cholesterol
160)3+ + 9.71⇥10 7 (cholesterol 208)3+ 5.79⇥10 7 (cholesterol 243)3+ + 8.79⇥
10 9 (cholesterol 319)3+ ]+age00 [ 0.029cholesterol+3.04⇥10 6 (cholesterol 160)3+
7.34⇥10 6 (cholesterol 208)3+ +4.36⇥10 6 (cholesterol 243)3+ 5.82⇥10 8 (cholesterol
319)3+ ].
l a t e x ( anova ( f ) , c a p t i o n= ’ Cubic s p l i n e s u r f a c e ’ , f i l e = ’ ’ ,
s i z e= ’ s m a l l e r ’ , l a b e l= ’ tab : anova rcs ’ ) #Table 10.8
# Figure 10.12:
b p l o t ( P r e d i c t ( f , c h o l e s t e r o l , age , np =40) , perim=perim ,
l f u n=w i r e f r a m e , z l i m=z l , a d j . s u b t i t l e=FALSE)

Statistics for testing age ⇥ cholesterol components of this fit are above.
None of the nonlinear interaction components is significant, but we again
retain them.
The general interaction model can be restricted to be of the form

252

10 Binary Logistic Regression

Table 10.8 Cubic spline surface
2

age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
sex (Factor+Higher Order Factors)
All Interactions
cholesterol (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
age ⇥ sex (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
age ⇥ cholesterol (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
f(A,B) vs. Af(B) + Bg(A)
Nonlinear Interaction in age vs. Af(B)
Nonlinear Interaction in cholesterol vs. Bg(A)
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

165.23
37.32
21.01
343.67
23.31
97.50
12.95
13.62
23.31
13.37
13.37
12.95
7.27
7.27
5.41
6.44
6.27
29.22
37.32
45.41
450.88

d.f.
15
12
10
4
3
12
9
8
3
2
2
9
8
8
4
6
6
14
12
16
19

P
< 0.0001
0.0002
0.0210
< 0.0001
< 0.0001
< 0.0001
0.1649
0.0923
< 0.0001
0.0013
0.0013
0.1649
0.5078
0.5078
0.2480
0.3753
0.3931
0.0097
0.0002
0.0001
< 0.0001

f (X1 , X2 ) = f1 (X1 ) + f2 (X2 ) + X1 g2 (X2 ) + X2 g1 (X1 )

(10.32)

by removing the parameters 11 , 12 , 14 , and 15 from the model. The previous table of Wald statistics included a test of adequacy of this reduced form
( 2 = 5.41 on 4 d.f., P = .248). The resulting fit is in Figure 10.13.
lrm ( s i g d z ⇠ s e x ⇤ r c s ( age , 4 ) + r c s ( c h o l e s t e r o l , 4 ) +
r c s ( age , 4 ) %i a% r c s ( c h o l e s t e r o l , 4 ) , data=a c a t h )
l a t e x ( anova ( f ) , f i l e = ’ ’ , s i z e= ’ s m a l l e r ’ ,
c a p t i o n= ’ S i n g l y n o n l i n e a r c u b i c s p l i n e s u r f a c e ’ ,
l a b e l= ’ tab : anova ria ’ ) #Table 10.9
f

# Figure 10.13:
b p l o t ( P r e d i c t ( f , c h o l e s t e r o l , age , np =40) , perim=perim ,
l f u n=w i r e f r a m e , z l i m=z l , a d j . s u b t i t l e=FALSE)
ltx ( f )
X ˆ = 7.2 + 2.96[female] + 0.164age + 7.23 ⇥ 10 5 (age 36)3+
0.000106(age
48)3+ 1.63⇥10 5 (age 56)3+ + 4.99⇥10 5 (age 68)3+ + 0.0148cholesterol + 1.21⇥
10 6 (cholesterol 160)3+ 5.5⇥10 6 (cholesterol 208)3+ + 5.5⇥10 6 (cholesterol
243)3+ 1.21⇥10 6 (cholesterol 319)3+ +age[ 0.00029cholesterol+9.28⇥10 9 (cholesterol
160)3+ + 1.7 ⇥ 10 8 (cholesterol 208)3+
4.43 ⇥ 10 8 (cholesterol 243)3+ + 1.79 ⇥
8
3
10 (cholesterol 319)+ ] + cholesterol[2.3 ⇥ 10 7 (age 36)3+ + 4.21 ⇥ 10 7 (age
48)3+ 1.31⇥10 6 (age 56)3+ + 6.64⇥10 7 (age 68)3+ ] + [female][ 0.111age + 8.03⇥
10 5 (age 36)3+ + 0.000135(age 48)3+ 0.00044(age 56)3+ + 0.000224(age 68)3+ ].

10.5 Assessment of Model Fit

253

4

log odds

3
2
1
0
−1
−2
70

Ag

400
350
300
g%
250
e, 50 40
,m
l
200
o
Ye
r
150
ar
30
ste
e
100
l
o
60

Ch

Fig. 10.12 Restricted cubic spline surface in two variables, each with k = 4 knots

The fit is similar to the former one except that the climb in risk for lowcholesterol older subjects is less pronounced. The test for nonlinear interaction is now more concentrated (P = .54 with 4 d.f.). Figure 10.14 accordingly
depicts a fit that allows age and cholesterol to have nonlinear main e↵ects,
but restricts the interaction to be a product between (untransformed) age
and cholesterol. The function agrees substantially with the previous fit.
lrm ( s i g d z ⇠ r c s ( age , 4 ) ⇤ s e x + r c s ( c h o l e s t e r o l , 4 ) +
age %i a% c h o l e s t e r o l , data=a c a t h )
l a t e x ( anova ( f ) , c a p t i o n= ’ L i n e a r i n t e r a c t i o n s u r f a c e ’ , f i l e = ’ ’ ,
s i z e= ’ s m a l l e r ’ , l a b e l= ’ tab : a n o v a l i a ’ ) #Table 10.10
f

# Figure 10.14:
b p l o t ( P r e d i c t ( f , c h o l e s t e r o l , age , np =40) , perim=perim ,
l f u n=w i r e f r a m e , z l i m=z l , a d j . s u b t i t l e=FALSE)
f.linia
f # save linear interaction fit for later
ltx ( f )
X ˆ = 7.36 + 0.182age 5.18⇥10
10 6 (age 56)3+ 2.99⇥10 5 (age

5

(age 36)3+ + 8.45⇥10 5 (age 48)3+ 2.91⇥
68)3+ + 2.8[female] + 0.0139cholesterol + 1.76⇥

<- this perim will make
the graph only contain
areas where there is data
to support it (maybe at
least 10 or something).

254

10 Binary Logistic Regression

Table 10.9 Singly nonlinear cubic spline surface
2

sex (Factor+Higher Order Factors)
All Interactions
age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
cholesterol (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
age ⇥ cholesterol (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
Nonlinear Interaction in age vs. Af(B)
Nonlinear Interaction in cholesterol vs. Bg(A)
sex ⇥ age (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

d.f.
4
3
11
8
6
8
5
4
5
4
4
2
2
3
2
2
10
8
12
15

P
< 0.0001
< 0.0001
< 0.0001
< 0.0001
0.0111
< 0.0001
0.0548
0.0281
0.0548
0.5372
0.5372
0.4496
0.4400
< 0.0001
0.0011
0.0011
0.0019
< 0.0001
< 0.0001
< 0.0001

2
d.f.
167.83 7
31.03 4
14.58 4
345.88 4
22.30 3
89.37 4
7.99 1
10.65 2
7.99 1
22.30 3
12.06 2
12.06 2
25.72 6
31.03 4
43.59 8
452.75 11

P
< 0.0001
< 0.0001
0.0057
< 0.0001
0.0001
< 0.0001
0.0047
0.0049
0.0047
0.0001
0.0024
0.0024
0.0003
< 0.0001
< 0.0001
< 0.0001

343.42
24.05
169.35
34.80
16.55
93.62
10.83
10.87
10.83
3.12
3.12
1.60
1.64
24.05
13.58
13.58
27.89
34.80
45.45
453.10

Table 10.10 Linear interaction surface
age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
sex (Factor+Higher Order Factors)
All Interactions
cholesterol (Factor+Higher Order Factors)
All Interactions
Nonlinear
age ⇥ cholesterol (Factor+Higher Order Factors)
age ⇥ sex (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

this is probably okay, but it is
a multi-step process, which
may be dangerous

10.5 Assessment of Model Fit

255

4

log odds

3
2
1
0
−1
−2
70

Ag

400
350
300
g%
250
e, 50 40
,m
l
200
o
Ye
r
150
ar
30
ste
e
100
l
o
60

Ch

Fig. 10.13 Restricted cubic spline fit with age ⇥ spline(cholesterol) and cholesterol
⇥ spline(age)
10 6 (cholesterol 160)3+ 4.88⇥10 6 (cholesterol 208)3+ + 3.45⇥10 6 (cholesterol
243)3+ 3.26⇥10 7 (cholesterol 319)3+ 0.00034age⇥cholesterol+[female][ 0.107age+
7.71⇥10 5 (age 36)3+ + 0.000115(age 48)3+ 0.000398(age 56)3+ + 0.000205(age
68)3+ ].

The Wald test for age ⇥ cholesterol interaction yields 2 = 7.99 with 1
d.f., P = .005. These analyses favor the nonlinear model with simple product interaction in Figure 10.14 as best representing the relationships among
cholesterol, age, and probability of prognostically severe coronary artery disease. A nomogram depicting this model is shown in Figure 10.21.
Using this simple product interaction model, Figure 10.15 displays predicted cholesterol e↵ects at the mean age within each age tertile. Substantial
agreement with Figure 10.9 is apparent.
# Make estimates of cholesterol effects for mean age in
# tertiles corresponding to initial analysis
mean.age
with ( acath ,
a s . v e c t o r ( t a p p l y ( age , a g e . t e r t i l e , mean , na.rm=TRUE) ) )

256

10 Binary Logistic Regression

4

log odds

3
2
1
0
−1
−2
70

Ag

400
350
300
g%
250
e, 50 40
,m
l
200
o
Ye
r
150
ar
30
ste
e
100
l
o
60

Ch

Fig. 10.14 Spline fit with nonlinear e↵ects of cholesterol and age and a simple
product interaction

p l o t ( P r e d i c t ( f , c h o l e s t e r o l , age=round ( mean.age , 2 ) ,
s e x=” male ” ) ,
a d j . s u b t i t l e=FALSE, y l i m=y l ) #3 curves, Figure 10.15

The partial residuals discussed in Section 10.4 can be used to check logistic model fit (although it may be difficult to deal with interactions). As
an example, reconsider the “duration of symptoms” fit in Figure 10.7. Figure 10.16 displays “loess smoothed” and raw partial residuals for the original
and log-transformed variable. The latter provides a more linear relationship,
especially where the data are most dense.
f
lrm ( tvdlm ⇠ c a d . d u r , data=dz , x=TRUE, y=TRUE)
r e s i d ( f , ” p a r t i a l ” , p l=” l o e s s ” , x l i m=c ( 0 , 2 5 0 ) , y l i m=c ( 3 , 3 ) )
s c a t 1 d ( dz $ c a d . d u r )
log.cad.dur
l o g 1 0 ( dz $ c a d . d u r + 1 )
f
lrm ( tvdlm ⇠ l o g . c a d . d u r , data=dz , x=TRUE, y=TRUE)
r e s i d ( f , ” p a r t i a l ” , p l=” l o e s s ” , y l i m=c ( 3 , 3 ) )
scat1d ( log.cad.dur )
# Figure 10.16

10.5 Assessment of Model Fit

257

log odds

4
3

63.73

2
1
0

53.06
41.74
100

200

300

400

Cholesterol, mg %
Fig. 10.15 Predictions from linear interaction model with mean age in tertiles indicated.

2
1
0
−1
−2
−3

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●

50

150

cad.dur

3

●●
●
●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●

2
1
0
−1

●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●

−2
−3

●

0

●

Partial Residual

Partial Residual

3

●
●●
●●
●●
●●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●

●
250
●

0.0

1.0

2.0

log.cad.dur

●
●

Fig. 10.16 Partial residuals for duration and log10 (duration+1). Data density shown
at top of each plot.

258

10 Binary Logistic Regression

Table 10.11 Merits of Methods for Checking Logistic Model Assumptions
Method

Choice
Assumes Uses Ordering
Required Additivity
of X

Stratification
Intervals
Smoother on X1 Bandwidth
stratifying on X2
Smooth partial
Bandwidth
residual plot
Spline model
Knots
for all Xs

x
x

Low
Variance

x
x
(not on X2 ) (if min. strat.)
x
x
x

x

Good
Resolution
on X
x
(X1 )
x
x

Table 10.11 summarizes the relative merits of stratification, nonparametric
smoothers, and regression splines for determining or checking binary logistic
model fits.

10.6 Collinearity
The variance inflation factors (VIFs) discussed in Section 4.6 can apply to any
regression fit.144, 648 These VIFs allow the analyst to isolate which variable(s)
are responsible for highly correlated parameter estimates. Recall that, in
general, collinearity is not a large problem compared with nonlinearity and
overfitting.

10.7 Overly Influential Observations
Pregibon504 developed a number of regression diagnostics that apply to the
family of regression models of which logistic regression is a member. Influence
statistics based on the “leave-out-one” method use an approximation to avoid
having to refit the model n times for n observations. This approximation
uses the fit and covariance matrix at the last iteration and assumes that the
“weights” in the weighted least squares fit can be kept constant, yielding
a computationally feasible one-step estimate of the leave-out-one regression
coefficients.
Hosmer and Lemeshow [298, pp. 149–170] discuss many diagnostics for
logistic regression and show how the final fit can be used in any least squares
program that provides diagnostics. A new dependent variable to be used in
that way is

10.8 Quantifying Predictive Ability

Zi = X ˆ +

259

Yi

P̂i
Vi

,

(10.33)

where Vi = P̂i (1 P̂i ), and P̂i = [1 + exp X ˆ] 1 is the predicted probability
that Yi = 1. The Vi , i = 1, 2, . . . , n are used as weights in an ordinary weighted
least squares fit of X against Z. This least squares fit will provide regression
coefficients identical to b. The new standard errors will be o↵ from the actual
logistic model ones by a constant.
As discussed in Section 4.9, the standardized change in the regression coefficients upon leaving out each observation in turn (DFBETAS) is one of the
most useful diagnostics, as these can pinpoint which observations are influential on each part of the model. After carefully modeling predictor transformations, there should be no lack of fit due to improper transformations.
However, as the white blood count example in Section 4.9 indicates, it is
commonly the case that extreme predictor values can still have too much
influence on the estimates of coefficients involving that predictor.
In the age–sex–response example of Section 10.1.3, both DFBETAS and
DFFITS identified the same influential observations. The observation given
by age = 48 sex = female response = 1 was influential for both age and sex,
while the observation age = 34 sex = male response = 1 was influential for
age and the observation age = 50 sex = male response = 0 was influential
for sex. It can readily be seen from Figure 10.3 that these points do not fit
the overall trends in the data. However, as these data were simulated from a
population model that is truly linear in age and additive in age and sex, the
apparent influential observations are just random occurrences. It is unwise
to assume that in real data all points will agree with overall trends. Removal
of such points would bias the results, making the model apparently more
predictive than it will be prospectively. See Table 10.12.

11

f
update ( f a s r , x=TRUE, y=TRUE)
which.influence ( f , .4 )
# Table 10.12

10.8 Quantifying Predictive Ability
The test statistics discussed above allow one to test whether a factor or set of
factors is related to the response. If the sample is sufficiently large, a factor
that grades risk from .01 to .02 may be a significant risk factor. However, that
factor is not very useful in predicting the response for an individual subject.
There is controversy regarding the appropriateness of R2 from ordinary least
2
squares in this setting.133, 417 The generalized RN
index of Nagelkerke464 and
134
424
425
Cragg and Uhler , Maddala , and Magee
described in Section 9.8.3
can be useful for quantifying the predictive strength of a model:

12

260

10 Binary Logistic Regression

Table 10.12
Females
Males
DFBETAS
DFFITS
DFBETAS
DFFITS
Intercept Age Sex
Intercept Age Sex
0.0
0.0
0.0
0
0.5
-0.5 -0.2
2
0.0
0.0
0.0
0
0.2
-0.3
0.0
1
0.0
0.0
0.0
0
-0.1
0.1
0.0
-1
0.0
0.0
0.0
0
-0.1
0.1
0.0
-1
-0.1
0.1
0.1
0
-0.1
0.1 -0.1
-1
-0.1
0.1
0.1
0
0.0
0.0
0.1
0
0.7
-0.7 -0.8
3
0.0
0.0
0.1
0
-0.1
0.1
0.1
0
0.0
0.0
0.1
0
-0.1
0.1
0.1
0
0.0
0.0 -0.2
-1
-0.1
0.1
0.1
0
0.1
-0.1 -0.2
-1
-0.1
0.1
0.1
0
0.0
0.0
0.1
0
-0.1
0.0
0.1
0
-0.1
0.1
0.1
0
-0.1
0.0
0.1
0
-0.1
0.1
0.1
0
0.1
0.0 -0.2
1
0.3
-0.3 -0.4
-2
0.0
0.0
0.1
-1
-0.1
0.1
0.1
0
0.1
-0.2
0.0
-1
-0.1
0.1
0.1
0
-0.1
0.2
0.0
1
-0.1
0.1
0.1
0
-0.2
0.2
0.0
1
0.0
0.0
0.0
0
-0.2
0.2
0.0
1
0.0
0.0
0.0
0
-0.2
0.2
0.1
1
0.0
0.0
0.0
0

2
RN
=

13

1
1

exp( LR/n)
,
exp( L0 /n)

(10.34)

where LR is the global log likelihood ratio statistic for testing the importance
of all p predictors in the model and L0 is the 2 log likelihood for the null
model.
Tjur606 coined the term “coefficient of discrimination” D, defined as the
average P̂ when Y = 1 minus the average P̂ when Y = 0, and showed how it
ties in with sum of squares–based R2 measures. D has many advantages as
an index of predictive powerd .
Linnet409 advocates quadratic and logarithmic probability scoring rules
for measuring predictive performance for probability models. Linnet shows
how to bootstrap such measures to get bias-corrected estimates and how to
use bootstrapping to compare two correlated scores. The quadratic scoring
rule is Brier’s score, frequently used in judging meteorologic forecasts30, 72 :
n

B=

14

1X
(P̂i
n i=1

Yi ) 2 ,

(10.35)

where P̂i is the predicted probability and Yi the corresponding observed response for the ith observation.
A unitless index of the strength of the rank correlation between predicted
probability of response and actual response is a more interpretable measure of
d

Note that D and B (below) and other indexes not related to c (below) do not work
well in case-control studies because of their reliance on absolute probability estimates.

10.8 Quantifying Predictive Ability

261

the fitted model’s predictive discrimination. One such index is the probability
of concordance, c, between predicted probability and response. The c index,
which is derived from the Wilcoxon–Mann–Whitney two-sample rank test,
is computed by taking all possible pairs of subjects such that one subject
responded and the other did not. The index is the proportion of such pairs
with the responder having a higher predicted probability of response than
the nonresponder.
Bamber38 and Hanley and McNeil250 have shown that c is identical to
a widely used measure of diagnostic discrimination, the area under a “receiver operating characteristic” (ROC) curve. A value of c of .5 indicates
random predictions, and a value of 1 indicates perfect prediction (i.e., perfect separation of responders and nonresponders). A model having c greater
than roughly .8 has some utility in predicting the responses of individual
subjects. The concordance index is also related to another widely used index,
Somers’ Dxy rank correlation575 between predicted probabilities and observed
responses, by the identity
Dxy = 2(c

.5).

(10.36)

Dxy is the di↵erence between concordance and discordance probabilities.
When Dxy = 0, the model is making random predictions. When Dxy = 1,
the predictions are perfectly discriminating. These rank-based indexes have
the advantage of being insensitive to the prevalence of positive responses.
A commonly used measure of predictive ability for binary logistic models is
the fraction of correctly classified responses. Here one chooses a cuto↵ on the
predicted probability of a positive response and then predicts that a response
will be positive if the predicted probability exceeds this cuto↵. There are a
number of reasons why this measure should be avoided.
1. It’s highly dependent on the cutpoint chosen for a “positive” prediction.
2. You can add a highly significant variable to the model and have the percentage classified correctly actually decrease. Classification error is a very
insensitive and statistically inefficient measure259, 625 since if the threshold
for “positive” is, say 0.75, a prediction of 0.99 rates the same as one of
0.751.
3. It gets away from the purpose of fitting a logistic model. A logistic model
is a model for the probability of an event, not a model for the occurrence
of the event. For example, suppose that the event we are predicting is
the probability of being struck by lightning. Without having any data,
we would predict that you won’t get struck by lightning. However, you
might develop an interesting model that discovers real risk factors that
yield probabilities of being struck that range from 0.000000001 to 0.001.
4. If you make a classification rule from a probability model, you are being
presumptuous. Suppose that a model is developed to assist physicians
in diagnosing a disease. Physicians sometimes profess to desiring a binary
decision model, but if given a probability they will rightfully apply di↵erent

15

262

16

10 Binary Logistic Regression

thresholds for treating di↵erent patients or for ordering other diagnostic
tests. Even though the age of the patient may be a strong predictor of
the probability of disease, the physician will often use a lower threshold
of disease likelihood for treating a young patient. This usage is above and
beyond how age a↵ects the likelihood.
5. If a disease were present in only 0.02 of the population, one could be 0.98
accurate in diagnosing the disease by ruling that everyone is disease–free,
i.e., by avoiding predictors. The proportion classified correctly fails to take
the difficulty of the task into account.
6. van Houwelingen and le Cessie625 demonstrated a peculiar property that
occurs when you try to obtain an honest estimate of classification error
using cross-validation. The cross-validated error rate corrects the apparent
error rate only if the predicted probability is exactly 1/2 or is 1/2±1/(2n).
The cross-validation estimate of optimism is “zero for n even and negligibly
small for n odd.” Better measures of error rate such as the Brier score and
logarithmic scoring rule do not have this problem. They also have the
nice property of being maximized when the predicted probabilities are the
population probabilities.409 .

10.9 Validating the Fitted Model
The major cause of unreliable models is overfitting the data. The methods described in Section 5.3 can be used to assess the accuracy of models
fairly. If a sample has been held out and never used to study associations
with the response, indexes of predictive accuracy can now be estimated using
that sample. More efficient is cross-validation, and bootstrapping is the most
efficient validation procedure. As discussed earlier, bootstrapping does not
require holding out any data, since all aspects of model development (stepwise variable selection, tests of linearity, estimation of coefficients, etc.) are
re-validated on samples taken with replacement from the whole sample.
Cox127 proposed and Harrell and Lee262 and Miller et al.450 further developed the idea of fitting a new binary logistic model to a new sample to
estimate the relationship between the predicted probability and the observed
outcome in that sample. This fit provides a simple calibration equation that
can be used to quantify unreliability (lack of calibration) and to calibrate
the predictions for future use. This logistic calibration also leads to indexes
of unreliability (U ), discrimination (D), and overall quality (Q = D U )
which are derived from likelihood ratio tests262 . Q is a logarithmic scoring
rule, which can be compared with Brier’s index (Equation 10.35). See [625]
for many more ideas.
With bootstrapping we do not have a separate validation sample for assessing calibration, but we can estimate the overoptimism in assuming that
the final model needs no calibration, that is, it has overall intercept=0 and

10.9 Validating the Fitted Model

263

slope=1. As discussed in Section 5.3, refitting the model
Pc = Prob{Y = 1|X ˆ} = [1 + exp (

0

+

1X

ˆ)]

1

(10.37)

(where Pc denotes the calibrated probability and the original predicted probability is P̂ = [1 + exp( X ˆ)] 1 ) in the original sample will always result in
= ( 0 , 1 ) = (0, 1), since a logistic model will always “fit” the training sample when assessed overall. We thus estimate by using Efron’s169 method to
estimate the overoptimism in (0, 1) to obtain bias-corrected estimates of the
true calibration. Simulations have shown this method produces an efficient
estimate of .254
A good set of indexes to estimate for summarizing a model validation is the
c or Dxy indexes and measures of calibration. In addition, the overoptimism
in the indexes may be reported to quantify the amount of overfitting present.
The estimate of can be used to draw a calibration curve by plotting P̂
on the x-axis and P̂c = [1 + exp ( 0 + 1 L)] 1 on the y-axis, where L =
logit(P̂ ).127, 262 An easily interpreted index of unreliability, Emax , follows
immediately from this calibration model:
Emax (a, b) = max |P̂
aP̂ b

P̂c |,

(10.38)

the maximum error in predicted probabilities over the range a  P̂  b. In
some cases, we would compute the maximum absolute di↵erence in predicted
and calibrated probabilities over the entire interval, that is, use Emax (0, 1).
The null hypothesis H0 : Emax (0, 1) = 0 can easily be tested by testing
H0 : 0 = 0, 1 = 1 as above. Since Emax does not weight the discrepancies
by the actual distribution of predictions, it may be preferable to compute the
average absolute discrepancy over the actual distribution of predictions (or
to use a mean squared error, incorporating the same calibration function).
If stepwise variable selection is being done, a matrix depicting which factors are selected at each bootstrap sample will shed light on how arbitrary is
the selection of “significant” factors. See Section 5.3 for reasons to compare
full and stepwise model fits.
As an example using bootstrapping to validate the calibration and discrimination of a model, consider the data in Section 10.1.3. Using 150 samples with
replacement, we first validate the additive model with age and sex forced into
every model. The optimism-corrected discrimination and calibration statistics produced by validate (see Section 10.11) are in the table below.
d
sex.age.response
dd
d a t a d i s t ( d ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )
f
lrm ( r e s p o n s e ⇠ s e x + age , data=d , x=TRUE, y=TRUE)
s e t . s e e d ( 3 ) # for reproducibility
v1
v a l i d a t e ( f , B=150)
l a t e x ( v1 ,
c a p t i o n= ’ B o o t s t r a p V a l i d a t i o n , 2 P r e d i c t o r s Without Stepdown ’ ,

264

10 Binary Logistic Regression
d i g i t s =2 , s i z e= ’ S s i z e ’ , f i l e = ’ ’ )
Bootstrap Validation, 2 Predictors Without Stepdown
Index
Original Training Test Optimism Corrected n
Sample

Dxy
R2
Intercept
Slope
Emax
D
U
Q
B
g
gp

0.70
0.45
0.00
1.00
0.00
0.39
0.05
0.44
0.16
2.10
0.35

Sample

0.70
0.48
0.00
1.00
0.00
0.44
0.05
0.49
0.15
2.49
0.35

Sample

0.67
0.43
0.01
0.91
0.02
0.36
0.04
0.32
0.18
1.97
0.34

Index

0.04
0.05
0.01
0.09
0.02
0.07
0.09
0.16
0.03
0.52
0.01

0.66
0.40
0.01
0.91
0.02
0.32
0.04
0.28
0.19
1.58
0.34

150
150
150
150
150
150
150
150
150
150
150

Now we incorporate variable selection. The variables selected in the first
10 bootstrap replications are shown below. The apparent Somers’ Dxy is 0.7,
and the bias-corrected Dxy is 0.66. The slope shrinkage factor is 0.91. The
maximum absolute error in predicted probability is estimated to be 0.02.
We next allow for step-down variable selection at each resample. For illustration purposes only, we use a suboptimal stopping rule based on significance
of individual variables at the ↵ = 0.10 level. Of the 150 repetitions, both age
and sex were selected in 137, and neither variable was selected in 3 samples.
The validation statistics are in the table below.
v2

v a l i d a t e ( f , B=150 , bw=TRUE,
r u l e= ’ p ’ , s l s =. 1 , t y p e= ’ i n d i v i d u a l ’ )

l a t e x ( v2 ,
c a p t i o n= ’ B o o t s t r a p V a l i d a t i o n , 2 P r e d i c t o r s with Stepdown ’ ,
d i g i t s =2 , B=15 , f i l e = ’ ’ , s i z e= ’ S s i z e ’ )
Bootstrap Validation, 2 Predictors with Stepdown
Index
Original Training Test Optimism Corrected n
Sample

Dxy
R2
Intercept
Slope
Emax
D
U
Q
B
g
gp

0.70
0.45
0.00
1.00
0.00
0.39
0.05
0.44
0.16
2.10
0.35

Sample

0.70
0.49
0.00
1.00
0.00
0.45
0.05
0.50
0.14
2.60
0.35

Sample

0.64
0.41
0.04
0.84
0.05
0.34
0.06
0.28
0.18
1.88
0.33

Index

0.07
0.09
0.04
0.16
0.05
0.11
0.11
0.22
0.04
0.72
0.02

0.63
0.37
0.04
0.84
0.05
0.28
0.06
0.22
0.20
1.38
0.33

150
150
150
150
150
150
150
150
150
150
150

10.9 Validating the Fitted Model

265

Factors Retained in Backwards Elimination
First 15 Resamples
sex age
• •
• •
• •
• •
• •
• •
• •
• •
• •
• •
• •
• •
• •
• •
•
Frequencies of Numbers of Factors Retained
0 1 2
3 10 137

The apparent Somers’ Dxy is 0.7 for the original stepwise model (which actually retained both age and sex), and the bias-corrected Dxy is 0.63, slightly
worse than the more correct model which forced in both variables. The calibration was also slightly worse as reflected in the slope correction factor
estimate of 0.84 versus 0.91.
Next, five additional candidate variables are considered. These variables
are random uniform variables, x1, . . . , x5 on the [0, 1] interval, and have no
association with the response.
s e t . s e e d (133)
n
nrow ( d )
x1
runif (n)
x2
runif (n)
x3
runif (n)
x4
runif (n)
x5
runif (n)
f
lrm ( r e s p o n s e ⇠ age + s e x + x1 + x2 + x3 + x4 + x5 ,
data=d , x=TRUE, y=TRUE)
v3
v a l i d a t e ( f , B=150 , bw=TRUE,
r u l e= ’ p ’ , s l s =. 1 , t y p e= ’ i n d i v i d u a l ’ )
k
a t t r ( v3 , ’ k e p t ’ )
# Compute number of x1-x5 selected
nx
a p p l y ( k [ , 3 : 7 ] , 1 , sum )
# Get selections of age and sex
v
colnames ( k )
as
apply ( k [ , 1 : 2 ] , 1 ,
f u n c t i o n ( x ) p a s t e ( v [ 1 : 2 ] [ x ] , c o l l a p s e= ’ , ’ ) )
t a b l e ( p a s t e ( as , ’ ’ , nx , ’ Xs ’ ) )

266

10 Binary Logistic Regression

age, sex
age, sex
sex

0 Xs
50
0 Xs age, sex
34
3 Xs age, sex
7
1 Xs
3

1 Xs
age
3
1 Xs age, sex
17
4 Xs
sex
1

2 Xs
1
2 Xs
11
0 Xs
12

50 cases where no
variables were selected. 3
times when 1 x and
nothing else. 34 times it
did the right thing (age &
sex only)

l a t e x ( v3 ,
c a p t i o n= ’ B o o t s t r a p V a l i d a t i o n with 5 N o i s e V a r i a b l e s and Stepdown ’ ,
d i g i t s =2 , B=15 , s i z e= ’ S s i z e ’ , f i l e = ’ ’ )
Bootstrap Validation with 5 Noise Variables and Stepdown
Index
Original Training Test Optimism Corrected n
Sample

Dxy
R2
Intercept
Slope
Emax
D
U
Q
B
g
gp

Sample

0.70
0.45
0.00
1.00
0.00
0.39
0.05
0.44
0.16
2.10
0.35

Sample

0.47
0.34
0.00
1.00
0.00
0.31
0.05
0.36
0.17
1.81
0.23

Index

0.38
0.23
0.03
0.78
0.06
0.18
0.07
0.11
0.22
1.06
0.19

0.09
0.11
0.03
0.22
0.06
0.13
0.12
0.25
0.04
0.75
0.04

0.60
0.34
0.03
0.78
0.06
0.26
0.07
0.19
0.20
1.36
0.31

Factors Retained in Backwards Elimination
First 15 Resamples
age sex x1 x2 x3 x4 x5
• •
• • • •
the
• • •
•
•

•

•
•

•
•

•
•
•

•
•
•

•

•

139
139
139
139
139
139
139
139
139
139
139

data cannot tell you what to do is the
take home message here (in terms of
variable selection).

• •

•

•
•

10.10 Describing the Fitted Model

267

Frequencies of Numbers of Factors Retained
0 1 2 3 4 56
50 15 37 18 11 7 1

Using step-down variable selection with the same stopping rule as before, the “final” model on the original sample correctly deleted x1, . . . , x5.
Of the 150 bootstrap repetitions, 11 samples yielded a singularity or nonconvergence either in the full-model fit or after step-down variable selection.
Of the 139 successful repetitions, the frequencies of the number of factors selected, as well as the frequency of variable combinations selected, are shown
above. Validation statistics are also shown above.
Figure 10.17 depicts the calibration (reliability) curves for the three strategies using the corrected intercept and slope estimates in the above tables as
1
,
0 and 1 , and the logistic calibration model Pc = [1 + exp ( 0 + 1 L)]
where Pc is the “actual” or calibrated probability, L is logit(P̂ ), and P̂ is the
predicted probability. The shape of the calibration curves (driven by slopes
< 1) is typical of overfitting—low predicted probabilities are too low and high
predicted probabilities are too high. Predictions near the overall prevalence
of the outcome tend to be calibrated even when overfitting is present.
g
function (v) v [ c ( ’ Intercept ’ , ’ Slope ’ ) , ’ i n d e x . c o r r e c t e d ’ ]
k
r b i n d ( g ( v1 ) , g ( v2 ) , g ( v3 ) )
co
c (2 ,5 ,4 ,1)
p l o t ( 0 , 0 , y l i m=c ( 0 , 1 ) , x l i m=c ( 0 , 1 ) ,
x l a b=” P r e d i c t e d P r o b a b i l i t y ” ,
y l a b=” A c t u a l P r o b a b i l i t y ” , t y p e=”n” )
l e g e n d ( . 4 5 , . 3 5 , c ( ” age , s e x ” , ” age , s e x stepdown ” ,
” age , sex , x1 x5” , ” i d e a l ” ) ,
l t y =1 , c o l=co , c e x=. 8 , bty=”n” )
probs
s e q ( 0 , 1 , l e n g t h = 20 0) ; L
q l o g i s ( probs )
for ( i in 1:3) {
P
p l o g i s ( k [ i , ’ I n t e r c e p t ’ ] + k [ i , ’ S l o p e ’ ] ⇤ L)
l i n e s ( probs , P , c o l=co [ i ] , lwd =1)
}
a b l i n e ( a =0 , b=1 , c o l=co [ 4 ] , lwd =1)
# Figure 10.17

“Honest” calibration curves may also be estimated using nonparametric
smoothers in conjunction with bootstrapping and cross-validation (see Section 10.11).

10.10 Describing the Fitted Model
Once the proper variables have been modeled and all model assumptions have
been met, the analyst needs to present and interpret the fitted model. There
are at least three ways to proceed. The coefficients in the model may be
interpreted. For each variable, the change in log odds for a sensible change in
the variable value (e.g., interquartile range) may be computed. Also, the odds

268

10 Binary Logistic Regression

Actual Probability

1.0

0.8

0.6

0.4
age, sex
age, sex stepdown
age, sex, x1−x5
ideal

0.2

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Predicted Probability
Fig. 10.17 Estimated logistic calibration (reliability) curves obtained by bootstrapping three modeling strategies.
Table 10.13 E↵ects

Response : sigdz

Low High
age
46
59
Odds Ratio
46
59
cholesterol
196 259
Odds Ratio
196 259
sex — female:male
1
2
Odds Ratio
1
2

13
13
63
63

E↵ect
S.E. Lower 0.95 Upper 0.95
0.90629 0.18381 0.546030
1.26650
2.47510
1.726400
3.54860
0.75479 0.13642 0.487410
1.02220
2.12720
1.628100
2.77920
-2.42970 0.14839 -2.720600
-2.13890
0.08806
0.065837
0.11778

ratio or factor by which the odds increases for a certain change in a predictor,
holding all other predictors constant, may be displayed. Table 10.13 contains
such summary statistics for the linear age ⇥ cholesterol interaction surface
fit described in Section 10.5.
s
summary ( f . l i n i a )
# Table 10.13
l a t e x ( s , f i l e = ’ ’ , s i z e= ’ S s i z e ’ ,
l a b e l= ’ tab : lrm cholx age confbar ’ )
plot ( s )

# Figure 10.18

The outer quartiles of age are 46 and 59 years, so the “half-sample” odds
ratio for age is 2.47, with 0.95 confidence interval [1.63, 3.74] when sex is male
and cholesterol is set to its median. The e↵ect of increasing cholesterol from
196 (its lower quartile) to 259 (its upper quartile) is to increase the log odds
by 0.79 or to increase the odds by a factor of 2.21. Since there are interactions

10.10 Describing the Fitted Model

269

Odds Ratio
0.10

0.75

1.50

2.50

3.50

age − 59:46

cholesterol − 259:196

sex − female:male

Adjusted to:age=52 sex=male cholesterol=224.5
Fig. 10.18 Odds ratios and confidence bars, using quartiles of age and cholesterol
for assessing their e↵ects on the odds of coronary disease

allowed between age and sex and between age and cholesterol, each odds ratio
in the above table depends on the setting of at least one other factor. The
results are shown graphically in Figure 10.18. The shaded confidence bars
show various levels of confidence and do not pin the analyst down to, say,
the 0.95 level.
For those used to thinking in terms of odds or log odds, the preceding
description may be sufficient. Many prefer instead to interpret the model in
terms of predicted probabilities instead of odds. If the model contains only
a single predictor (even if several spline terms are required to represent that
predictor), one may simply plot the predictor against the predicted response.
Such a plot is shown in Figure 10.19 which depicts the fitted relationship
between age of diagnosis and the probability of acute bacterial meningitis
(ABM) as opposed to acute viral meningitis (AVM), based on an analysis of
422 cases from Duke University Medical Center.576 The data may be found
on the web site. A linear spline function with knots at 1, 2, and 22 years was
used to model this relationship.
When the model contains more than one predictor, one may graph the predictor against log odds, and barring interactions, the shape of this relationship
will be independent of the level of the other predictors. When displaying the
model on what is usually a more interpretable scale, the probability scale, a
difficulty arises in that unlike log odds the relationship between one predictor
and the probability of response depends on the levels of all other factors. For
example, in the model

270

10 Binary Logistic Regression

Fig. 10.19 Linear spline fit for probability of bacterial versus viral meningitis as a
function of age at onset576 . Copyright 1989, American Medical Association. Reprinted
by permission.

Prob{Y = 1|X} = {1 + exp[ (

0

+

1 X1

+

2 X2 )]}

1

(10.39)

there is no way to factor out X1 when examining the relationship between
X2 and the probability of a response. For the two-predictor case one can plot
X2 versus predicted probability for each level of X1 . When it is uncertain
whether to include an interaction in this model, consider presenting graphs
for two models (with and without interaction terms included) as was done
in [652].
When three factors are present, one could draw a separate graph for each
level of X3 , a separate curve on each graph for each level of X1 , and vary X2
on the x-axis. Instead of this, or if more than three factors are present, a good
way to display the results may be to plot “adjusted probability estimates” as
a function of one predictor, adjusting all other factors to constants such as
the mean. For example, one could display a graph relating serum cholesterol
to probability of myocardial infarction or death, holding age constant at 55,
sex at 1 (male), and systolic blood pressure at 120 mmHg.
The final method for displaying the relationship between several predictors
and probability of response is to construct a nomogram.39, 249 A nomogram
not only sheds light on how the e↵ect of one predictor on the probability of
response depends on the levels of other factors, but it allows one to quickly
estimate the probability of response for individual subjects. The nomogram
in Figure 10.20 allows one to predict the probability of acute bacterial meningitis (given the patient has either viral or bacterial meningitis) using the same
sample as in Figure 10.19. Here there are four continuous predictor values,
none of which are linearly related to log odds of bacterial meningitis: age
at admission (expressed as a linear spline function), month of admission (expressed as |month 8|), cerebrospinal fluid glucose/blood glucose ratio (linear

10.10 Describing the Fitted Model

271

e↵ect truncated at .6; that is, the e↵ect is the glucose ratio if it is  .6, and .6
if it exceeded .6), and the cube root of the total number of polymorphonuclear
leukocytes in the cerebrospinal fluid.

17

<-cube root transformation this works often when you are
taking a volume count

Fig. 10.20 Nomogram for estimating probability of bacterial (ABM) versus viral
(AVM) meningitis. Step 1, place ruler on reading lines for patient’s age and month
of presentation and mark intersection with line A; step 2, place ruler on values for
glucose ratio and total polymorphonuclear leukocyte (PMN) count in cerebrospinal
fluid and mark intersection with line B; step 3, use ruler to join marks on lines A and
B, then read o↵ the probability of ABM versus AVM.576 Copyright 1989, American
Medical Association. Reprinted by permission.

The model associated with Figure 10.14 is depicted in what could be called
a “precision nomogram” in Figure 10.21. Discrete cholesterol levels were required because of the interaction between two continuous variables.
# Draw a nomogram that shows examples of confidence intervals
nom
nomogram ( f . l i n i a , c h o l e s t e r o l=s e q ( 1 5 0 , 4 0 0 , by =50) ,
i n t e r a c t= l i s t ( age=s e q ( 3 0 , 7 0 , by = 1 0 ) ) ,
l p . a t=s e q ( 2 , 3 . 5 , by=. 5 ) ,
c o n f . i n t=TRUE, c o n f . l p=” a l l ” ,
f u n=f u n c t i o n ( x ) 1 /(1+ exp ( x ) ) , # or plogis
f u n l a b e l=” P r o b a b i l i t y o f CAD” ,
f u n . a t=c ( s e q ( . 1 , . 9 , by=. 1 ) , . 9 5 , . 9 9 )
)
# Figure 10.21
p l o t (nom , c o l . g r i d = g r a y ( c ( 0 . 8 , 0 . 9 5 ) ) ,

272

10 Binary Logistic Regression
v a r n a m e . l a b e l=FALSE, i a . s p a c e =1 , x f r a c=. 4 6 , lmgp=. 2 )

0

10

20

30

40

50

60

70

80

90

100

Points
cholesterol (age=30
sex=male)

150

cholesterol (age=40
sex=male)

250

300

350

150

250

300
250

cholesterol (age=50
sex=male)

400
350
400
300 350 400

200

250

cholesterol (age=60
sex=male)

200

250

cholesterol (age=70
sex=male)
cholesterol (age=30
sex=female)

350
400

200
150

cholesterol (age=40
sex=female)

250

150

cholesterol (age=50
sex=female)

250
250

300
300
350
300 350 400

200

250

cholesterol (age=60
sex=female)

350

400

400

350

200

250

cholesterol (age=70
sex=female)

400

200

Total Points
0

10

20

30

40

50

60

70

80

90

100

Linear Predictor
−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

3

3.5

Probability of CAD
0.2

0.3 0.4 0.5 0.6 0.7

0.8

0.9

0.95

Fig. 10.21 Nomogram relating age, sex, and cholesterol to the log odds and to
the probability of significant coronary artery disease. Select one axis corresponding
to sex and to age 2 {30, 40, 50, 60, 70}. There is linear interaction between age and
sex and between age and cholesterol. 0.70 and 0.90 confidence intervals are shown
(0.90 in gray). Note that for the “Linear Predictor” scale there are various lengths
of confidence intervals near the same value of X ˆ, demonstrating that the standard
error of X ˆ depends on the individual X values. Also note that confidence intervals
corresponding to smaller patient groups (e.g., females) are wider.

10.11 R Functions

273

10.11 R Functions
The general S statistical modeling functions93 described in Section 6.2 work
with the author’s lrm function for fitting binary and ordinal logistic regression models. lrm has several options for doing penalized maximum likelihood
estimation, with special treatment of categorical predictors so as to shrink
all estimates (including the reference cell) to the mean. The following example fits a logistic model containing predictors age, blood.pressure, and sex,
with age fitted with a smooth five-knot restricted cubic spline function and
a di↵erent shape of the age relationship for males and females.

18

fit
lrm ( death ⇠ b l o o d . p r e s s u r e + s e x ⇤ r c s ( age , 5 ) )
anova ( f i t )
p l o t ( P r e d i c t ( f i t , age , s e x ) )

The pentrace function makes it easy to check the e↵ects of a sequence of
penalties. The following code fits an unpenalized model and plots the AIC
and Schwarz BIC for a variety of penalties so that approximately the best
cross-validating model can be chosen (and so we can learn how the penalty
relates to the e↵ective degrees of freedom). Here we elect to only penalize the
nonlinear or non-additive parts of the model.
lrm ( death ⇠ r c s ( age , 5 ) ⇤ t r e a t m e n t + l s p ( sbp , c ( 1 2 0 , 1 4 0 ) ) ,
x=TRUE, y=TRUE)
plot ( pentrace ( f ,
p e n a l t y= l i s t ( n o n l i n e a r=s e q ( . 2 5 , 1 0 , by=. 2 5 ) ) ) )
f

See Sections 9.8.1 and 9.10 for more information.
The residuals function for lrm and the which.influence function can be
used to check predictor transformations as well as to analyze overly influential
observations in binary logistic regression. See Figure 10.16 for one application.
The residuals.lrm function will also perform the unweighted sum of squares
test for global goodness of fit described in Section 10.5.
The validate function when used on an object created by lrm does resampling validation of a logistic regression model, with or without backward
step-down variable deletion. It provides bias-corrected Somers’ Dxy rank cor2
relation, RN
index, the intercept and slope of an overall logistic calibration
equation, the maximum absolute di↵erence in predicted and calibrated probabilities Emax , the discrimination index D [(model L.R. 2 1)/n], the unreliability index U = (di↵erence in 2 log likelihood between uncalibrated
X and X with overall intercept and slope calibrated to test sample)/n,
and the overall quality index Q = D U .262 The “corrected” slope can
be thought of as a shrinkage factor that takes overfitting into account. See
predab.resample in Section 6.2 for the list of resampling methods.
The calibrate function produces bootstrapped or cross-validated calibration curves for logistic and linear models. The “apparent” calibration
accuracy is estimated using a nonparametric smoother relating predicted
probabilities to observed binary outcomes. The nonparametric estimate is

19

274

10 Binary Logistic Regression

evaluated at a sequence of predicted probability levels. Then the distances
from the 45 line are compared with the di↵erences when the current model is
evaluated back on the whole sample (or omitted sample for cross-validation).
The di↵erences in the di↵erences are estimates of overoptimism. After averaging over many replications, the predicted-value-specific di↵erences are then
subtracted from the apparent di↵erences and an adjusted calibration curve
is obtained. Unlike validate, calibrate does not assume a linear logistic
calibration. For an example, see the end of Section 11. calibrate will print
the mean absolute calibration error, the 0.9 quantile of the absolute error,
and the mean squared error, all over the observed distribution of predicted
values.
The val.prob function is used to compute measures of discrimination
and calibration of predicted probabilities for a separate sample from the one
used to derive the probability estimates. Thus val.prob is used in external validation and data-splitting. The function computes similar indexes as
validate plus the Brier score and a statistic for testing for unreliability or
H0 : 0 = 0, 1 = 1.
In the following example, a logistic model is fitted on 100 observations
simulated from the actual model given by
Prob{Y = 1|X1 , X2 , X3 } = [1 + exp[ ( 1 + 2X1 )]]

1

,

(10.40)

where X1 is a random uniform [0, 1] variable. Hence X2 and X3 are irrelevant.
After fitting a linear additive model in X1 , X2 , and X3 , the coefficients are
used to predict Prob{Y = 1} on a separate sample of 100 observations.
s e t .s e e d (13)
n
200
x1
runif (n)
x2
runif (n)
x3
runif (n)
logit
2 ⇤ ( x1 .5 )
P
1 /(1+ exp ( l o g i t ) )
y
i f e l s e ( r u n i f (n)  P, 1 , 0)
d
d a t a . f r a m e ( x1 , x2 , x3 , y )
f
lrm ( y ⇠ x1 + x2 + x3 , s u b s e t = 1 : 1 0 0 )
phat
p r e d i c t ( f , d [ 1 0 1 : 2 0 0 , ] , t y p e= ’ f i t t e d ’ )
# Figure 10.22
v
v a l . p r o b ( phat , y [ 1 0 1 : 2 0 0 ] , m=20 , c e x=. 5 )

The output is shown in Figure 10.22.
The R built-in function glm, a very general modeling function, can fit
binary logistic models. The response variable must be coded 0/1 for glm to
work. Glm is a slight modification of the built-in glm function in the rms
package that allows fits to use rms methods. This facilitates Poisson and
several other types of regression analysis.

10.12 Further Reading

Actual Probability

1.0

0.8

0.6

275

Dxy
C (ROC)
R2
D
U
Q
Brier
Intercept
Slope
Emax
S:z
S:p

0.339
0.670
0.010
−0.003
−0.020
0.017
0.235
−0.371
0.544
0.211
2.351
0.019

0.4
Ideal
Logistic calibration
Nonparametric
Grouped observations

0.2

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Predicted Probability
Fig. 10.22 Validation of a logistic model in a test sample of size n = 100. The
calibrated risk distribution (histogram of logistic-calibrated probabilities) is shown.

10.12 Further Reading
1
2
3

4
5

6

7

8
9

See [585] for modeling strategies specific to binary logistic regression.
See [624] for a nice review of logistic modeling. Agresti6 is an excellent source
for categorical Y in general.
Not only does discriminant analysis assume the same regression model as logistic regression, but it also assumes that the predictors are each normally
distributed and that jointly the predictors have a multivariate normal distribution. These assumptions are unlikely to be met in practice, especially when
one of the predictors is a discrete variable such as sex group. When discriminant analysis assumptions are violated, logistic regression yields more accurate
estimates.246, 507 Even when discriminant analysis is optimal (i.e., when all
its assumptions are satisfied) logistic regression is virtually as accurate as the
discriminant model.259
See [569] for a review of measures of e↵ect for binary outcomes.
Cepedaet al.92 found that propensity adjustment is better than covariate adjustment with logistic models when the number of events per variable is less
than 8.
Pregibon505 developed a modification of the log likelihood function that when
maximized results in a fit that is resistant to overly influential and outlying
observations.
See Hosmer and Lemeshow299 for methods of testing for a di↵erence in the
observed event proportion and the predicted event probability (average of predicted probabilities) for a group of heterogeneous subjects.
See Hosmer and Lemeshow,298 Kay and Little,335 and Collett [112, Chap. 5].
Landwehr et al.366 proposed the partial residual (see also Fowlkes195 ).
See Berk and Booth50 for other partial-like residuals.

276
10
11

12

13

14
15

16

17

18
19

10 Binary Logistic Regression
See [335] for an example comparing a smoothing method with a parametric
logistic model fit.
See Collett [112, Chap. 5] and Pregibon505 for more information about influence
statistics. Pregibon’s resistant estimator of handles overly influential groups
of observations and allows one to estimate the weight that an observation contributed to the fit after making the fit robust. Observations receiving low weight
are partially ignored but are not deleted.
Buyse84 showed that in the case of a single categorical predictor, the ordinary R2 has a ready interpretation in terms of variance explained for binary
responses. Menard447 studied various indexes for binary logistic regression. He
2
criticized RN
for being too dependent on the proportion of observations with
Y = 1. Hu et al.302 further studied the properties of variance-based R2 measures for binary responses. Tjur606 has a nice discussion discrimination graphics
and sum of squares–based R2 measures for binary logistic regression, as well as
a good discussion of “separation” and infinite regression coefficients. Sums of
squares are approximated various ways.
Very little work has been done on developing adjusted R2 measures in logistic
regression and other non-linear model setups. Liao and McGee399 developed
one adjusted R2 measure for binary logistic regression, but it uses simulation
to adjust for the bias of overfitting. One might as well use the bootstrap to
adjust any of the indexes discussed in this section.
[120, 625] have more pertinent discussion of probability accuracy scores.
Copas118 demonstrated how ROC areas can be misleading when applied to
di↵erent responses having greatly di↵erent prevalences. He proposed another
approach, the logit rank plot. Newsom466 is an excellent reference on Dxy .
Newson467 developed several generalizations to Dxy including a stratified version, and discussed the jackknife variance estimator for them. ROC areas are
not very useful for comparing two models115, 486 (but see483 ).
Gneiting and Raftery215 have an excellent review of proper scoring rules.
Hand248 contains much information about assessing classification accuracy.
Mittlböck and Schemper454 have an excellent review of indexes of explained
variation for binary logistic models. See also Korn and Simon359 and Zheng
and Agresti.680 .
Pryor et al.508 presented nomograms for a 10-variable logistic model. One of the
variables was sex, which interacted with some of the other variables. Evaluation
of predicted probabilities was simplified by the construction of separate nomograms for females and males. Seven terms for discrete predictors were collapsed
into one weighted point score axis in the nomograms, and age by risk factor
interactions were captured by having four age scales.
Moons et al.455 presents a case study in penalized binary logistic regression
modeling.
The rcspline.plot function in the Hmisc R package does not allow for interactions as does lrm, but it can provide detailed output for checking spline
fits. This function plots the estimated spline regression and confidence limits,
placing summary statistics on the graph. If there are no adjustment variables,
rcspline.plot can also plot two alternative estimates of the regression function: proportions or logit proportions on grouped data, and a nonparametric
estimate. The nonparametric regression estimate is based on smoothing the binary responses and taking the logit transformation of the smoothed estimates, if
desired. The smoothing uses the “super smoother” of Friedman203 implemented
in the S function supsmu.

10.13 Problems

277

10.13 Problems
1. Consider the age–sex–response example in Section 10.1.3. This dataset is
available from the text’s web site in the Datasets area.
a. Duplicate the analyses done in Section 10.1.3.
b. For the model containing both age and sex, test H0 : logit response is
linear in age versus Ha : logit response is quadratic in age. Use the best
test statistic.
c. Using a Wald test, test H0 : no age ⇥ sex interaction. Interpret all
parameters in the model.
d. Plot the estimated logit response as a function of age and sex, with and
without fitting an interaction term.
e. Perform a likelihood ratio test of H0 : the model containing only age
and sex is adequate versus Ha : model is inadequate. Here, “inadequate”
may mean nonlinearity (quadratic) in age or presence of an interaction.
f. Assuming no interaction is present, test H0 : model is linear in age
versus Ha : model is nonlinear in age. Allow “nonlinear” to be more
general than quadratic. (Hint: use a restricted cubic spline function
with knots at age=39, 45, 55, 64 years.)
g. Plot age against the estimated spline transformation of age (the transformation that would make age fit linearly). You can set the sex and
intercept terms to anything you choose. Also plot Prob{response = 1 |
age, sex} from this fitted restricted cubic spline logistic model.
2. Consider a binary logistic regression model using the following predictors:
age (years), sex, race (white, African-American, Hispanic, Oriental, other),
blood pressure (mmHg). The fitted model is given by
logit Prob[Y = 1|X] = X ˆ = 1.36 + .03(race = African-American)
.04(race = hispanic) + .05(race = oriental) .06(race = other)
+ .07|blood pressure 110| + .3(sex = male) .1age + .002age2 +
(sex = male)[.05age .003age2 ].
a. Compute the predicted logit (log odds) that Y = 1 for a 50-year-old
female Hispanic with a blood pressure of 90 mmHg. Also compute the
odds that Y = 1 (Prob[Y = 1]/Prob[Y = 0]) and the estimated probability that Y = 1.
b. Estimate odds ratios for each nonwhite race compared with the reference group (white), holding all other predictors constant. Why can
you estimate the relative e↵ect of race for all types of subjects without
specifying their characteristics?
c. Compute the odds ratio for a blood pressure of 120 mmHg compared
with a blood pressure of 105, holding age first to 30 years and then to
40 years.
d. Compute the odds ratio for a blood pressure of 120 mmHg compared
with a blood pressure of 105, all other variables held to unspecified

278

e.

f.
g.
h.

10 Binary Logistic Regression

constants. Why is this relative e↵ect meaningful without knowing the
subject’s age, race, or sex?
Compute the estimated risk di↵erence in changing blood pressure from
105 mmHg to 120 mmHg, first for age = 30 then for age = 40, for a
white female. Why does the risk di↵erence depend on age?
Compute the relative odds for males compared with females, for age = 50
and other variables held constant.
Same as the previous question but for females : males instead of males
: females.
Compute the odds ratio resulting from increasing age from 50 to 55
for males, and then for females, other variables held constant. What is
wrong with the following question: What is the relative e↵ect of changing age by one year?

Chapter 11

Case Study in Binary Logistic Regression,
Model Selection and Approximation:
Predicting Cause of Death

11.1 Overview
This chapter contains a case study on developing, describing, and validating
a binary logistic regression model. In addition, the following methods are
exemplified:
1. Data reduction using incomplete linear and nonlinear principal components
2. Use of AIC to choose from five modeling variations, deciding which is best
for the number of parameters
3. Model simplification using stepwise variable selection and approximation
of the full model
4. The relationship between the degree of approximation and the degree of
predictive discrimination loss
5. Bootstrap validation that includes penalization for model uncertainty
(variable selection) and that demonstrates a loss of predictive discrimination over the full model even when compensating for overfitting the full
model.
The data reduction and pre-transformation methods used here were discussed
in more detail in Chapter 8. Single imputation will be used because of the
limited quantity of missing data.

11.2 Background
Consider the randomized trial of estrogen for treatment of prostate cancer85
described in Chapter 8. In this trial, larger doses of estrogen reduced the e↵ect
of prostate cancer but at the cost of increased risk of cardiovascular death.
Kay334 did a formal analysis of the competing risks for cancer, cardiovascular,
and other deaths. It can also be quite informative to study how treatment
279

280

11 Binary Logistic Regression Case Study 1

and baseline variables relate to the cause of death for those patients who
died.368 We subset the original dataset of those patients dying from prostate
cancer (n = 130), heart or vascular disease (n = 96), or cerebrovascular
disease (n = 31). Our goal is to predict cardiovascular–cerebrovascular death
(cvd, n = 127) given the patient died from either cvd or prostate cancer. Of
interest is whether the time to death has an e↵ect on the cause of death, and
whether the importance of certain variables depends on the time of death.

11.3 Data Transformations and Single Imputation
In R, first obtain the desired subset of the data and do some preliminary
calculations such as combining an infrequent category with the next category,
and dichotomizing ekg for use in ordinary principal components (PCs).
r e q u i r e ( rms )
getHdata ( p r o s t a t e )
prostate
within ( prostate , {
l e v e l s ( ekg ) [ l e v e l s ( ekg ) %i n%
c ( ’ o l d MI ’ , ’ r e c e n t MI ’ ) ]
’MI ’
ekg.norm
1 ⇤ ( ekg %i n% c ( ’ normal ’ , ’ b e n i g n ’ ) )
l e v e l s ( ekg )
a b b r e v i a t e ( l e v e l s ( ekg ) )
pfn
as.numeric ( pf )
l e v e l s ( pf )
l e v e l s ( pf ) [ c ( 1 , 2 , 3 , 3 ) ]
cvd
s t a t u s %i n% c ( ” dead
heart or v ascular ” ,
” dead
cerebrovascular ”)
rxn = a s . n u m e r i c ( r x ) } )
# Use transcan to compute optimal pre-transformations
ptrans
# See Figure 8.3
t r a n s c a n (⇠ s z + s g + ap + sbp + dbp +
age + wt + hg + ekg + p f + bm + hx + dtime + rx ,
imputed=TRUE, t r a n s f o r m e d=TRUE,
data=p r o s t a t e , p l=FALSE, pr=FALSE)
# Use transcan single imputations
imp
impute ( p t r a n s , data=p r o s t a t e , l i s t . o u t =TRUE)

Imputed missing values with the following frequencies
and stored them in variables with their original names:
sz
5

sg age
11
1

wt ekg
2
8

NAvars
a l l . v a r s (⇠ s z + s g + age + wt + ekg )
f o r ( x i n NAvars ) p r o s t a t e [ [ x ] ]
imp [ [ x ] ]
subset
p r o s t a t e $ s t a t u s %i n% c ( ” dead
heart or v ascular ” ,
” dead
c e r e b r o v a s c u l a r ” , ” dead
p r o s t a t i c ca ” )

11.4 Principal Components, Pretransformations
trans
psub

281

ptrans $ transformed [ subset , ]
prostate [ subset , ]

11.4 Regression on Original Variables, Principal
Components and Pretransformations
We first examine the performance of data reduction in predicting the cause
of death, similar to what we did for survival time in Section 8.6. The first
analyses assess how well PCs (on raw and transformed variables) predict the
cause of death.
There are 127 cvds. We use the 15:1 rule of thumb discussed on P. 74 to
justify using the first 8 PCs. ap is log-transformed because of its extreme
distribution.
# Function to compute the first k PCs
ipc
f u n c t i o n ( x , k=1 , . . . )
princomp ( x , . . . , c o r=TRUE) $ s c o r e s [ , 1 : k ]
# Compute the first 8 PCs on raw variables then on
# transformed ones
pc8
i p c (⇠ s z + s g + l o g ( ap ) + sbp + dbp + age +
wt + hg + ekg.norm + p f n + bm + hx + rxn + dtime ,
data=psub , k=8)
f8
lrm ( cvd ⇠ pc8 , data=psub )
pc8t
i p c ( t r a n s , k=8)
f8t
lrm ( cvd ⇠ pc8t , data=psub )
# Fit binary logistic model on original variables
f
lrm ( cvd ⇠ s z + s g + l o g ( ap ) + sbp + dbp + age +
wt + hg + ekg + p f + bm + hx + r x + dtime , data=psub )
# Expand continuous variables using splines
g
lrm ( cvd ⇠ r c s ( sz , 4 ) + r c s ( sg , 4 ) + r c s ( l o g ( ap ) , 4 ) +
r c s ( sbp , 4 ) + r c s ( dbp , 4 ) + r c s ( age , 4 ) + r c s ( wt , 4 ) +
r c s ( hg , 4 ) + ekg + p f + bm + hx + r x + r c s ( dtime , 4 ) ,
data=psub )
# Fit binary logistic model on individual transformed var.
h
lrm ( cvd ⇠ t r a n s , data=psub )

this is probably too many
AIC to look at...really
should not look at more
than 3 without biasing
your results.

The five approaches to modeling the outcome are compared using AIC (where
smaller is better).
c ( f 8=AIC ( f 8 ) , f 8 t=AIC ( f 8 t ) , f=AIC ( f ) , g=AIC ( g ) , h=AIC ( h ) )
f8
f8t
f
g
h
257.6573 254.5172 255.8545 263.8413 254.5317

Based on AIC, the more traditional model fitted to the raw data and assuming linearity for all the continuous predictors has only a slight chance
of producing worse cross-validated predictive accuracy than other methods.
The chances are also good that e↵ect estimates from this simple model will
have competitive mean squared errors.

the variation here is in
AIC is not too
large...except g wasn't
great but that is to be
expected because we
had tons of df in that
model

282

11 Binary Logistic Regression Case Study 1

11.5 Description of Fitted Model
Here we describe the simple all-linear full model. Summary statistics and a
Wald-ANOVA table are below, followed by partial e↵ects plots with pointwise
confidence bands, and odds ratios over default ranges of predictors.
p r i n t ( f , l a t e x=TRUE)

Logistic Regression Model
lrm(formula = cvd ˜ sz + sg + log(ap) + sbp + dbp + age + wt +
hg + ekg + pf + bm + hx + rx + dtime, data = psub)
Model Likelihood
Ratio Test
Obs
257 LR 2
144.39
FALSE
130 d.f.
21
TRUE
127 Pr(> 2 ) < 0.0001
L
max | @ log
| 6⇥10 11
@

Intercept
sz
sg
ap
sbp
dbp
age
wt
hg
ekg=bngn
ekg=rd&ec
ekg=hbocd
ekg=hrts
ekg=MI
pf=in bed < 50% daytime
pf=in bed > 50% daytime
bm
hx
rx=0.2 mg estrogen
rx=1.0 mg estrogen
rx=5.0 mg estrogen
dtime

Coef
-4.5130
-0.0640
-0.2967
-0.3927
-0.0572
0.3917
0.0926
-0.0177
0.0860
1.0781
-0.1929
-1.3679
0.4365
0.3039
0.9604
-2.3232
0.1456
1.0913
-0.3022
0.7526
0.6868
-0.0136

Discrimination
Indexes
R2
0.573
g
2.688
gr
14.701
gp
0.394
Brier
0.133

Rank Discrim.
Indexes
C
0.893
Dxy
0.786
0.787
⌧a
0.395

S.E. Wald Z Pr(> |Z|)
3.2210
-1.40
0.1612
0.0168
-3.80
0.0001
0.1149
-2.58
0.0098
0.1411
-2.78
0.0054
0.0890
-0.64
0.5201
0.1629
2.40
0.0162
0.0286
3.23
0.0012
0.0140
-1.26
0.2069
0.0925
0.93
0.3524
0.8793
1.23
0.2202
0.6318
-0.31
0.7601
0.8279
-1.65
0.0985
0.4582
0.95
0.3407
0.5618
0.54
0.5886
0.6956
1.38
0.1673
1.2464
-1.86
0.0623
0.5067
0.29
0.7738
0.3782
2.89
0.0039
0.4908
-0.62
0.5381
0.5272
1.43
0.1534
0.5043
1.36
0.1733
0.0107
-1.27
0.2040

<-Kendall's tau is highly
penalizing for having ties in
x and y, we have a lot of
ties in y here.

11.5 Description of Fitted Model

283

an
anova ( f )
l a t e x ( an , f i l e = ’ ’ , t a b l e . e n v=FALSE)
2

d.f.
P
sz
14.42 1 0.0001
sg
6.67 1 0.0098
ap
7.74 1 0.0054
sbp
0.41 1 0.5201
dbp
5.78 1 0.0162
age
10.45 1 0.0012
wt
1.59 1 0.2069
hg
0.86 1 0.3524
ekg
6.76 5 0.2391
pf
5.52 2 0.0632
bm
0.08 1 0.7738
hx
8.33 1 0.0039
rx
5.72 3 0.1260
dtime
1.61 1 0.2040
TOTAL 66.87 21 < 0.0001
p l o t ( an )
# Figure 11.1
s
f$ stats
gamma.hat
( s [ ’ Model L . R . ’ ]

s [ ’ d . f . ’ ] ) / s [ ’ Model L . R . ’ ]

bm ●
sbp ●
hg ●
●
wt
●
dtime
●
ekg
●
rx
●
pf
●
dbp
●
sg
●
ap
●
hx
●
age
sz
0

2

4

6

8

if you were to bootstrap these rankings,
they would not be nearly as stable as
they appear here.

●

10 12

χ2 − df
Fig. 11.1 Ranking of apparent importance of predictors of cause of death

dd
d a t a d i s t ( psub ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )
g g p l o t ( P r e d i c t ( f ) , s e p d i s c r e t e= ’ v e r t i c a l ’ , vnames= ’ names ’ ,
r d a t a=psub , h i s t S p i k e . o p t s= l i s t ( f r a c=. 1 ) ) # Figure 11.2

284

11 Binary Logistic Regression Case Study 1

age

4
2
0
−2
−4
−6

log odds

55

60

4
2
0
−2
−4
−6
0

ap

65 70
dtime

20

75

80 0

20

8

10

40

5.0

40

60

10

12
14
sz

12

14 0

10

20

30

16

1

●

0

●

12.5

40

bm

80

15.0
wt

●

17.5

●
●
●
●
●

hx
1
0

rx

●
●
●

−6 −4 −2 0 2 4

placebo
5.0 mg estrogen
1.0 mg estrogen
0.2 mg estrogen

12.5

90 100 110 120

ekg
rd&ec
nrml
MI
hrts
hbocd
bngn

pf
normal activity
in bed > 50% daytime
in bed < 50% daytime

7.5 10.0
sbp

hg

sg

4
2
0
−2
−4
−6

dbp

●
●
●
●

−6 −4 −2 0 2 4

log odds
Fig. 11.2 Partial e↵ects (log odds scale) in full model for cause of death, along with
vertical line segments showing the raw data distribution of predictors

●
●

11.6 Backwards Step-Down
p l o t ( summary ( f ) , l o g=TRUE)

285
# Figure 11.3

Odds Ratio
0.10

0.50

2.00

8.00

sz − 25:6
sg − 12:9
ap − 7:0.5999756
sbp − 16:13
dbp − 9:7
age − 76:70
wt − 106:89
hg − 14.59961:12
bm − 1:0
hx − 1:0
dtime − 37:11
ekg − nrml:hrts
ekg − bngn:hrts
ekg − rd&ec:hrts
ekg − hbocd:hrts
ekg − MI:hrts
pf − in bed < 50% daytime:normal activity
pf − in bed > 50% daytime:normal activity
rx − 0.2 mg estrogen:placebo
rx − 1.0 mg estrogen:placebo
rx − 5.0 mg estrogen:placebo

Fig. 11.3 Interquartile-range odds ratios for continuous predictors and simple odds
ratios for categorical predictors. Numbers at left are upper quartile : lower quartile or
current group : reference group. The bars represent 0.9, 0.95, 0.99 confidence limits.
The intervals are drawn on the log odds ratio scale and labeled on the odds ratio
scale. Ranges are on the original scale.

The van Houwelingen–Le Cessie heuristic shrinkage estimate (Equation 4.1)
is ˆ = 0.85, indicating that this model will validate on new data about 15%
worse than on this dataset.

11.6 Backwards Step-Down
Now use fast backward step-down (with total residual AIC as the stopping
rule) to identify the variables that explain the bulk of the cause of death.
Later validation will take this screening of variables into account.The greatly
reduced model results in a simple nomogram.

286

11 Binary Logistic Regression Case Study 1

fastbw ( f )

Deleted
ekg
bm
hg
sbp
wt
dtime
rx
pf
sg
dbp

Chi-Sq
6.76
0.09
0.38
0.48
1.11
1.47
5.65
4.78
4.28
5.84

d.f.
5
1
1
1
1
1
3
2
1
1

P
0.2391
0.7639
0.5378
0.4881
0.2932
0.2253
0.1302
0.0915
0.0385
0.0157

Residual
6.76
6.85
7.23
7.71
8.82
10.29
15.93
20.71
25.00
30.83

d.f.
5
6
7
8
9
10
13
15
16
17

P
AIC
0.2391 -3.24
0.3349 -5.15
0.4053 -6.77
0.4622 -8.29
0.4544 -9.18
0.4158 -9.71
0.2528 -10.07
0.1462 -9.29
0.0698 -7.00
0.0209 -3.17

this is an approach that
deletes too many
variables and runs in the
way of statistical
inference.

Approximate Estimates after Deleting Factors
Coef
S.E. Wald Z
P
Intercept -3.74986 1.82887 -2.050 0.0403286
sz
-0.04862 0.01532 -3.174 0.0015013
ap
-0.40694 0.11117 -3.660 0.0002518
age
0.06000 0.02562 2.342 0.0191701
hx
0.86969 0.34339 2.533 0.0113198
Factors in Final Model
[1] sz

ap

age hx

fred
lrm ( cvd ⇠ s z + l o g ( ap ) + age + hx , data=psub )
latex ( fred , f i l e=’ ’ )

Prob{cvd} =

1
, where
1 + exp( X )

Xˆ=
5.009276

0.05510121 sz

0.509185 log(ap) + 0.0788052 age + 1.070601 hx

nom

nomogram ( f r e d , ap=c ( . 1 , . 5 , 1 , 5 , 1 0 , 5 0 ) ,
f u n=p l o g i s , f u n l a b e l=” P r o b a b i l i t y ” ,
f u n . a t=c ( . 0 1 , . 0 5 , . 1 , . 2 5 , . 5 , . 7 5 , . 9 , . 9 5 , . 9 9 ) )
p l o t (nom , x f r a c=. 4 5 )
# Figure 11.4

It is readily seen from this model that patients with a history of heart disease, and patients with less extensive prostate cancer are those more likely
to die from cvd rather than from cancer. But beware that it is easy to overinterpret findings when using unpenalized estimation, and confidence intervals are too narrow. Let us use the bootstrap to study the uncertainty in

11.6 Backwards Step-Down
0

287
10

20

30

40

50

60

70

80

90

100

Points
Size of Primary Tumor
(cm^2)
Serum Prostatic Acid
Phosphatase

70

65

60

50

55

50

45

10

5

55

1 60

40

35

30

25

1

0.5

70

75

20

15

10

5

0

0.1

Age in Years
45

History of Cardiovascular
Disease

50

65

80

85

90

0

Total Points
0

50

100

150

200

250

300

Linear Predictor
−5

−4

−3

−2

−1

0

1

0.25

0.5

0.75

2

3

4

Probability
0.01

0.05 0.1

0.9 0.95

Fig. 11.4 Nomogram calculating X ˆ and P̂ for cvd as the cause of death, using
the step-down model. For each predictor, read the points assigned on the 0–100 scale
and add these points. Read the result on the Total Points scale and then read the
corresponding predictions below it.

the selection of variables and to penalize for this uncertainty when estimating predictive performance of the model. The variables selected in the first 20
bootstrap resamples are shown, making it obvious that the set of “significant”
variables, i.e., the final model, is somewhat arbitrary.
f
v

update ( f , x=TRUE, y=TRUE)
v a l i d a t e ( f , B=200 , bw=TRUE)

l a t e x ( v , B=20 , d i g i t s =3)

288

11 Binary Logistic Regression Case Study 1

Index

Original Training Test Optimism Corrected n
Sample Sample Sample
Index
Dxy
0.682
0.714 0.646
0.068
0.614 200
R2
0.439
0.480 0.396
0.084
0.355 200
Intercept
0.000
0.000 0.005
0.005
0.005 200
Slope
1.000
1.000 0.821
0.179
0.821 200
Emax
0.000
0.000 0.045
0.045
0.045 200
D
0.395
0.447 0.350
0.097
0.299 200
U
0.008
0.008 0.015
0.023
0.015 200
Q
0.403
0.455 0.335
0.119
0.284 200
B
0.162
0.151 0.173
0.021
0.184 200
g
1.932
2.207 1.771
0.436
1.496 200
gp
0.341
0.355 0.322
0.033
0.308 200

sz
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•

Factors Retained in Backwards Elimination
First 20 Resamples
sg ap sbp dbp age wt hg ekg pf bm hx rx dtime
•
•
• •
•
•
• •
•
•
• •
•
• •
•
•
•
• •
•
•
•
• •
• •
•
• •
•
•
•
•
• •
•
•
•
•
•
•
• •
•
•
• •
• • •
•
• •
•
• •
• • • • • • •
• •
•
• •
Frequencies of Numbers of Factors Retained
1 2 3 4 5 6 7 8 9 10 11 12 14
8 33 49 47 33 13 7 5 1 1 1 1 1

The slope shrinkage (ˆ ) is a bit lower than was estimated above. There is
drop-o↵ in all indexes. The estimated likely future predictive discrimination

11.6 Backwards Step-Down

289

of the model as measured by Somers’ Dxy fell from 0.682 to 0.614. The
latter estimate is the one that should be claimed when describing model
performance.
A nearly unbiased estimate of future calibration of the stepwise-derived
model is given below.
cal
c a l i b r a t e ( f , B=200 , bw=TRUE)
plot ( cal )
# Figure 11.5

Actual Probability

1.0

0.8

0.6

0.4

Apparent

0.2

Bias−corrected
Ideal

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Predicted Pr{cvd}
B= 200 repetitions, boot
Mean absolute error=0.029 n=257
Fig. 11.5 Bootstrap overfitting–corrected calibration curve estimate for the backwards step-down cause of death logistic model, along with a rug plot showing the distribution of predicted risks. The smooth nonparametric calibration estimator (loess)
is used.

The amount of overfitting seen in Figure 11.5 is consistent with the indexes
produced by the validate function.
For comparison, consider a bootstrap validation of the full model without
using variable selection.
vfull
v a l i d a t e ( f , B=200)
l a t e x ( v f u l l , d i g i t s =3)

290

11 Binary Logistic Regression Case Study 1

Index

Original Training Test Optimism Corrected n
Sample Sample Sample
Index
Dxy
0.786
0.834 0.739
0.095
0.691 200
R2
0.573
0.643 0.501
0.142
0.431 200
Intercept
0.000
0.000 0.004
0.004
0.004 200
Slope
1.000
1.000 0.685
0.315
0.685 200
Emax
0.000
0.000 0.084
0.084
0.084 200
D
0.558
0.656 0.468
0.188
0.370 200
U
0.008
0.008 0.052
0.060
0.052 200
Q
0.566
0.664 0.416
0.248
0.318 200
B
0.133
0.114 0.149
0.035
0.169 200
g
2.688
3.519 2.371
1.148
1.540 200
gp
0.394
0.416 0.366
0.051
0.343 200
Compared to the validation of the full model, the step-down model has less
optimism, but it started with a smaller Dxy due to loss of information from
removing moderately important variables. The improvement in optimism was
not enough to o↵set the e↵ect of eliminating variables. If shrinkage were used
with the full model, it would have better calibration and discrimination than
the reduced model, since shrinkage does not diminish Dxy . Thus stepwise
variable selection failed at delivering excellent predictive discrimination.
Finally, compare previous results with a bootstrap validation of a stepdown model using a better significance level for a variable to stay in the
model (↵ = 0.5,584 ) and using individual approximate Wald tests rather
than tests combining all deleted variables.
v5

v a l i d a t e ( f , bw=TRUE,

s l s =0. 5 , t y p e= ’ i n d i v i d u a l ’ , B=200)

Backwards Step-down - Original Model
Deleted
ekg
bm
hg
sbp
wt
dtime
rx

Chi-Sq
6.76
0.09
0.38
0.48
1.11
1.47
5.65

d.f.
5
1
1
1
1
1
3

P
Residual d.f.
0.2391 6.76
5
0.7639 6.85
6
0.5378 7.23
7
0.4881 7.71
8
0.2932 8.82
9
0.2253 10.29
10
0.1302 15.93
13

P
AIC
0.2391 -3.24
0.3349 -5.15
0.4053 -6.77
0.4622 -8.29
0.4544 -9.18
0.4158 -9.71
0.2528 -10.07

Approximate Estimates after Deleting Factors
Intercept
sz
sg
ap
dbp
age
pf=in bed < 50% daytime

Coef
-4.86308
-0.05063
-0.28038
-0.24838
0.28288
0.08502
0.81151

S.E.
2.67292
0.01581
0.11014
0.12369
0.13036
0.02690
0.66376

Wald Z
-1.819
-3.202
-2.546
-2.008
2.170
3.161
1.223

P
0.068852
0.001366
0.010903
0.044629
0.030008
0.001572
0.221485

11.7 Model Approximation

291

pf=in bed > 50% daytime -2.19885 1.21212 -1.814 0.069670
hx
0.87834 0.35203 2.495 0.012592
Factors in Final Model
[1] sz

sg

ap

dbp age pf

hx

l a t e x ( v5 , d i g i t s =3 , B=0)

Index

Original Training Test Optimism Corrected n
Sample Sample Sample
Index
Dxy
0.739
0.806 0.718
0.088
0.651 200
R2
0.517
0.604 0.483
0.121
0.396 200
Intercept
0.000
0.000 0.004
0.004
0.004 200
Slope
1.000
1.000 0.736
0.264
0.736 200
Emax
0.000
0.000 0.069
0.069
0.069 200
D
0.486
0.603 0.447
0.156
0.330 200
U
0.008
0.008 0.038
0.046
0.038 200
Q
0.494
0.611 0.409
0.201
0.292 200
B
0.147
0.123 0.155
0.032
0.178 200
g
2.351
3.050 2.192
0.859
1.492 200
gp
0.372
0.403 0.359
0.044
0.329 200

the method that give the best
discrimination is maybe not
giving the best absolute
accuracy. Maybe the best
model is all the variables with
penalization.

The performance statistics are midway between the full model and the
smaller stepwise model.
the

uncertainty in selecting
the variables - if you don't
know which variables to
11.7 Model Approximation
get rid of, it is better to
Frequently a better approach than stepwise variable selection is to approx-leave them all in because
imate the full model, using its estimates of precision, as discussed in Sec-you have a lot less
tion 5.5. Stepwise variable selection as well as regression trees are useful for
uncertainty. And you are
making the approximations, and the sacrifice in predictive accuracy is always
effectively modeling all the
apparent.
We begin by computing the “gold standard” linear predictor from the fulldf (with phantom df) when
model fit (R2 = 1.0), then running backwards step-down OLS regression to
you do the stepwise model.
approximate it.
lp
predict ( f )
# Compute linear predictor from full model
# Insert sigma=1 as otherwise sigma=0 will cause problems
a
o l s ( l p ⇠ s z + s g + l o g ( ap ) + sbp + dbp + age + wt +
hg + ekg + p f + bm + hx + r x + dtime , sigma =1 ,
data=psub )
# Specify silly stopping criterion to remove all variables
s
f a s t b w ( a , a i c s =10000)
betas
s$ Coefficients
# matrix, rows=iterations
X
c b i n d ( 1 , f $x )
# design matrix

292

11 Binary Logistic Regression Case Study 1

Fraction of Full Model LR χ2 Preserved

# Compute the series of approximations to lp
ap
X %⇤% t ( b e t a s )
# For each approx. compute approximation R^ 2 and ratio of
# likelihood ratio chi-square for approximate model to that
# of original model
m
n c o l ( ap )
1
# all but intercept-only model
r2
frac
numeric (m)
fullchisq
f $ s t a t s [ ’ Model L . R . ’ ]
f o r ( i i n 1 :m) {
lpa
ap [ , i ]
r2 [ i ]
c o r ( lpa , l p ) ^ 2
fapprox
lrm ( cvd ⇠ lpa , data=psub )
frac [ i ]
f a p p r o x $ s t a t s [ ’ Model L . R . ’ ] / f u l l c h i s q
}
# Figure 11.6:
p l o t ( r2 , f r a c , t y p e= ’ b ’ ,
x l a b=e x p r e s s i o n ( p a s t e ( ’ Approximation ’ , R^ 2 ) ) ,
y l a b=e x p r e s s i o n ( p a s t e ( ’ F r a c t i o n o f F u l l Model LR ’ ,
c h i^ 2 , ’ Preserved ’ ) ) )
a b l i n e ( h=. 9 5 , c o l=g r a y ( . 8 3 ) ) ; a b l i n e ( v=. 9 5 , c o l=g r a y ( . 8 3 ) )
a b l i n e ( a =0 , b=1 , c o l=g r a y ( . 8 3 ) )

●
●
●●

1.0
●

0.9

●
●

0.8

●
●

0.7
●

0.6
●

0.5
0.4

●

0.5

0.6

0.7

0.8

0.9

1.0

2

Approximation R

Fig. 11.6 Fraction of explainable variation in cvd that was explained by approximate
models, along with approximation accuracy (x–axis)

After 6 deletions, slightly more than 0.05 of both the LR 2 and the approximation R2 are lost (see Figure 11.6). Therefore we take as our approximate
model the one that removed 6 predictors. The equation for this model is
below, and its nomogram is in Figure 11.7.

11.7 Model Approximation

293

o l s ( l p ⇠ s z + s g + l o g ( ap ) + age + ekg + p f + hx +
rx , data=psub )
f a p p r o x $ s t a t s [ ’ R2 ’ ]
# as a check
fapprox

R2 0.9453396
l a t e x ( fapprox ,

f i l e=’ ’ )

E(lp) = X , where
Xˆ=
2.868303

0.06233241 sz

0.3157901 sg

+1.396922[bngn] + 0.06275034[rd&ec]
+1.116028[in bed < 50% daytime]

0.3834479 log(ap) + 0.09089393 age

1.24892[hbocd] + 0.6511938[hrts] + 0.3236771[MI]
2.436734[in bed > 50% daytime]

+1.05316 hx
0.3888534[0.2 mg estrogen] + 0.6920495[1.0 mg estrogen]
+0.7834498[5.0 mg estrogen]

and [c] = 1 if subject is in group c, 0 otherwise.
nom

nomogram ( f a p p r o x , ap=c ( . 1 , . 5 , 1 , 5 , 1 0 , 2 0 , 3 0 , 4 0 ) ,
f u n=p l o g i s , f u n l a b e l=” P r o b a b i l i t y ” ,
l p . a t =( 5 ) : 4 ,
f u n . l p . a t=q l o g i s ( c ( . 0 1 , . 0 5 , . 2 5 , . 5 , . 7 5 , . 9 5 , . 9 9 ) ) )
p l o t (nom , x f r a c=. 4 5 )
# Figure 11.7

294

11 Binary Logistic Regression Case Study 1
0

10

20

30

40

50

60

70

80

90

100

Points
Size of Primary Tumor
(cm^2)
Combined Index of Stage
and Hist. Grade
Serum Prostatic Acid
Phosphatase

70

65

60

55

50

45

40

35

30

25

20

15

14

13

12

11

10

9

8

7

6

5

40

10

5

1 0.5

15

10

5

0

Is this better than
lasso? maybe run a
simulation to see.

0.1

Age in Years
45

50

55 nrml
60

65

70 bngn
75

80

85

90

ekg
hbocd

rd&ec

hrtsnormal activity

pf
in bed > 50% daytime

History of Cardiovascular
Disease

in bed < 50% daytime

1

0 placebo

rx
0.2 mg estrogen

Total Points
0

50

100

150

200

250

300

350

400

Linear Predictor
−5

−3

−1 0

1

2

3

4

Probability
0.01 0.05

0.25 0.5 0.75

0.95 0.99

Fig. 11.7 Nomogram for predicting the probability of cvd based on the approximate
model

Chapter 12

Logistic Model Case Study 2: Survival of
Titanic Passengers

This case study demonstrates the development of a binary logistic regression
model to describe patterns of survival in passengers on the Titanic, based on
passenger age, sex, ticket class, and the number of family members accompanying each passenger. Nonparametric regression is also used. Since many
of the passengers had missing ages, multiple imputation is used so that the
complete information on the other variables can be efficiently utilized. Titanic
passenger data were gathered by many researchers. Primary references are the
Encyclopedia Titanica at www.encyclopedia-titanica.org and Eaton and
Haas.166 Titanic survival patterns have been analyzed previously148, 289, 567
but without incorporation of individual passenger ages. Thomas Cason while
a University of Virginia student compiled and interpreted the data from the
World Wide Web. One thousand three hundred nine of the passengers are
represented in the dataset, which is available from this text’s Web site under the name titanic3. An early analysis of Titanic data may be found in
Bron74 .

12.1 Descriptive Statistics
First we obtain basic descriptive statistics on key variables.
r e q u i r e ( rms )
getHdata ( t i t a n i c 3 )
# get dataset from web site
# List of names of variables to analyze
v
c ( ’ p c l a s s ’ , ’ s u r v i v e d ’ , ’ age ’ , ’ s e x ’ , ’ s i b s p ’ , ’ parch ’ )
t3
titanic3 [ , v]
u n i t s ( t 3 $ age )
’ years ’
l a t e x ( d e s c r i b e ( t3 ) , f i l e=’ ’ )

6 Variables

t3
1309 Observations
295

296

12 Logistic Model Case Study 2: Survival of Titanic Passengers

pclass

n missing unique
1309
0
3

1st (323, 25%), 2nd (277, 21%), 3rd (709, 54%)

survived : Survived

n missing unique Info Sum Mean
1309
0
2 0.71 500 0.382

age : Age [years]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
1046
263
98
1 29.88 5 14 21 28 39 50 57
lowest : 0.1667 0.3333 0.4167 0.6667 0.7500
highest: 70.5000 71.0000 74.0000 76.0000 80.0000

sex

n missing unique
1309
0
2

female (466, 36%), male (843, 64%)

sibsp : Number of Siblings/Spouses Aboard
n missing unique Info Mean
1309
0
7 0.67 0.4989
0
1 2 3 4 5 8
Frequency 891 319 42 20 22 6 9
%
68 24 3 2 2 0 1

parch : Number of Parents/Children Aboard
n missing unique Info Mean
1309
0
8 0.55 0.385
0
1
2 3 4 5 6 9
Frequency 1002 170 113 8 6 6 2 2
%
77 13
9 1 0 0 0 0

Next, we obtain access to the needed variables and observations, and save
data distribution characteristics for plotting and for computing predictor
e↵ects. There are not many passengers having more than 3 siblings or spouses
or more than 3 children, so we truncate two variables at 3 for the purpose of
estimating stratified survival probabilities.
dd
d a t a d i s t ( t3 )
# describe distributions of variables to rms
o p t i o n s ( d a t a d i s t= ’ dd ’ )
s
summary ( s u r v i v e d ⇠ age + s e x + p c l a s s +
c u t 2 ( s i b s p , 0 : 3 ) + c u t 2 ( parch , 0 : 3 ) , data=t 3 )
p l o t ( s , main= ’ ’ , s u b t i t l e s=FALSE)
# Figure 12.1

Note the large number of missing ages. Also note the strong e↵ects of sex and
passenger class on the probability of surviving. The age e↵ect does not appear
to be very strong, because as we show later, much of the e↵ect is restricted to
age < 21 years for one of the sexes. The e↵ects of the last two variables are
unclear as the estimated proportions are not monotonic in the values of these
descriptors. Although some of the cell sizes are small, we can show four-way
empirical relationships with the fraction of surviving passengers by creating
four cells for sibsp ⇥ parch combinations and by creating two age groups.

12.2 Exploring Trends with Nonparametric Regression

297
N

Age[ 0.167,22.0)
[years]

[22.000,28.5)
[28.500,40.0)
[40.000,80.0]
Missing

●
●

●

●

290
246
265
245
263

●

sex

female
male

●

●

pclass

1st
2nd
3rd

●

●

●

Number of Siblings/Spouses Aboard

●

0
1
2
[3,8] ●

●

Number of Parents/Children Aboard

●

●

Overall

891
319
42
57
1002
170
113
24

●

●

0.2

0.3

0.4

466
843
323
277
709

●

●

0
1
2
[3,9]

these age quartiles are
messing up the effect.

1309

0.5

0.6

0.7

Survived

Fig. 12.1 Univariable summaries of Titanic survival

We suppress proportions based on fewer than 25 passengers in a cell. Results
are shown in Figure 12.2.
tn

t r a n s f o r m ( t3 ,
a g e c = i f e l s e ( age < 2 1 , ’ c h i l d ’ , ’ a d u l t ’ ) ,
s i b s p= i f e l s e ( s i b s p == 0 , ’ no s i b / sp ’ , ’ s i b / sp ’ ) ,
parch= i f e l s e ( parch == 0 , ’ no par / c h i l d ’ , ’ par / c h i l d ’ ) )

g
s

f u n c t i o n ( y ) i f ( l e n g t h ( y ) < 2 5 ) NA e l s e mean ( y )
with ( tn , summarize ( s u r v i v e d ,
l l i s t ( agec , sex , p c l a s s , s i b s p , parch ) , g ) )
# llist, summarize in Hmisc package
# Figure 12.2:
g g p l o t ( s u b s e t ( s , a g e c != ’NA ’ ) , a e s ( x=s u r v i v e d , y=p c l a s s , shape=s e x ) ) +
g e o m p o i n t ( ) + f a c e t g r i d ( a g e c ⇠ s i b s p ⇤ parch ) +
xlab ( ’ Proportion Surviving ’ ) + ylab ( ’ Passenger Class ’ ) +
s c a l e x c o n t i n u o u s ( b r e a k s=c ( 0 , . 5 , 1 ) )

Note that none of the e↵ects of sibsp or parch for common passenger groups
appear strong on an absolute risk scale.

12.2 Exploring Trends with Nonparametric Regression
As described in Section 2.4.7, the loess smoother has excellent performance
when the response is binary, as long as outlier detection is turned o↵. Here we

298

12 Logistic Model Case Study 2: Survival of Titanic Passengers

no sib/sp
no par/child

sib/sp
no par/child

sib/sp
par/child

●

2nd

adult

●

1st

●

sex

●

●

3rd

female
male

●

child

Passenger Class

3rd

no sib/sp
par/child

2nd
1st
0.0

0.5

1.0
0.0

0.5

1.0
0.0

0.5

1.0
0.0

0.5

1.0

Proportion Surviving
Fig. 12.2 Multi-way summary of Titanic survival

use a ggplot2 add-on function histSpikeg in the Hmisc package to obtain and
plot the loess fit and age distribution. histSpikeg uses the “no iteration”
option for the R lowess function when the response is binary.
# Figure 12.3
b
s c a l e s i z e d i s c r e t e ( r a n g e=c ( . 1 , . 8 5 ) )
yl
y l a b (NULL)
p1
g g p l o t ( t3 , a e s ( x=age , y=s u r v i v e d ) ) +
h i s t S p i k e g ( s u r v i v e d ⇠ age , l o w e s s=TRUE, data=t 3 ) +
ylim ( 0 , 1 ) + y l
p2
g g p l o t ( t3 , a e s ( x=age , y=s u r v i v e d , c o l o r=s e x ) ) +
h i s t S p i k e g ( s u r v i v e d ⇠ age + sex , l o w e s s=TRUE,
data=t 3 ) + y l i m ( 0 , 1 ) + y l
p3
g g p l o t ( t3 , a e s ( x=age , y=s u r v i v e d , s i z e=p c l a s s ) ) +
h i s t S p i k e g ( s u r v i v e d ⇠ age + p c l a s s , l o w e s s=TRUE,
data=t 3 ) + b + y l i m ( 0 , 1 ) + y l
p4
g g p l o t ( t3 , a e s ( x=age , y=s u r v i v e d , c o l o r=sex ,
s i z e=p c l a s s ) ) +
h i s t S p i k e g ( s u r v i v e d ⇠ age + s e x + p c l a s s ,
l o w e s s=TRUE, data=t 3 ) +
b + ylim ( 0 , 1 ) + y l
g r i d E x t r a : : g r i d . a r r a n g e ( p1 , p2 , p3 , p4 , n c o l =2)
# combine 4

Figure 12.3 shows much of the story of passenger survival patterns. “Women
and children first” seems to be true except for women in third class. It is
interesting that there is no real cuto↵ for who is considered a child. For men,
the younger the greater chance of surviving. The interpretation of the e↵ects

12.2 Exploring Trends with Nonparametric Regression

1.00

1.00

0.75

0.75

299

sex
0.50

female

0.50

male
0.25

0.25

0.00

0.00
0

20

40

60

80

0

20

age

40

60

80

age

1.00

1.00
pclass

0.75

1st

0.75
pclass

2nd

1st
0.50

2nd

3rd

0.50
sex

3rd
0.25

female

0.25

male
0.00

0.00
0

20

40

age

60

80

0

20

40

60

80

age

Fig. 12.3 Nonparametric regression (loess) estimates of the relationship between
age and the probability of surviving the Titanic, with tick marks depicting the age
distribution. The top left panel shows unstratified estimates of the probability of
survival. Other panels show nonparametric estimates by various stratifications.

300

12 Logistic Model Case Study 2: Survival of Titanic Passengers

of the “number of relatives”-type variables will be more difficult, as their
definitions are a function of age. Figure 12.4 shows these relationships.
# Figure 12.4
top
theme ( l e g e n d . p o s i t i o n= ’ top ’ )
p1
g g p l o t ( t3 , a e s ( x=age , y=s u r v i v e d , c o l o r=c u t 2 ( s i b s p ,
0 : 2 ) ) ) + s t a t p l s m o ( ) + b + y l i m ( 0 , 1 ) + y l + top +
s c a l e c o l o r d i s c r e t e ( name= ’ s i b l i n g s / s p o u s e s ’ )
p2
g g p l o t ( t3 , a e s ( x=age , y=s u r v i v e d , c o l o r=c u t 2 ( parch ,
0 : 2 ) ) ) + s t a t p l s m o ( ) + b + y l i m ( 0 , 1 ) + y l + top +
s c a l e c o l o r d i s c r e t e ( name= ’ p a r e n t s / c h i l d r e n ’ )
g r i d E x t r a : : g r i d . a r r a n g e ( p1 , p2 , n c o l =2)

siblings/spouses

0

1

[2,8]

parents/children

1.00

1.00

0.75

0.75

0.50

0.50

0.25

0.25

0.00

0.00
0

20

40

age

60

80

0

20

0

40

1

60

age

Fig. 12.4 Relationship between age and survival stratified by the number of siblings
or spouses on board (left panel) or by the number of parents or children of the
passenger on board (right panel).

12.3 Binary Logistic Model With Casewise Deletion of
Missing Values
What follows is the standard analysis based on eliminating observations having any missing data. We develop an initial somewhat saturated logistic
model, allowing for a flexible nonlinear age e↵ect that can di↵er in shape
for all six sex ⇥ class strata. The sibsp and parch variables do not have
sufficiently dispersed distributions to allow for us to model them nonlinearly.
Also, there are too few passengers with nonzero values of these two variables
in sex ⇥ pclass ⇥ age strata to allow us to model complex interactions involving them. The meaning of these variables does depend on the passenger’s
age, so we consider only age interactions involving sibsp and parch.

[2,9]

80

12.3 Binary Logistic Model With Casewise Deletion of Missing Values

301

Table 12.1 Wald Statistics for survived
2

sex (Factor+Higher Order Factors)
All Interactions
pclass (Factor+Higher Order Factors)
All Interactions
age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
sibsp (Factor+Higher Order Factors)
All Interactions
parch (Factor+Higher Order Factors)
All Interactions
sex ⇥ pclass (Factor+Higher Order Factors)
sex ⇥ age (Factor+Higher Order Factors)
Nonlinear (Factor+Higher Order Factors)
Nonlinear Interaction : f(A,B) vs. AB
pclass ⇥ age (Factor+Higher Order Factors)
Nonlinear (Factor+Higher Order Factors)
Nonlinear Interaction : f(A,B) vs. AB
age ⇥ sibsp (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
age ⇥ parch (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
sex ⇥ pclass ⇥ age (Factor+Higher Order Factors)
Nonlinear
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

187.15
59.74
100.10
46.51
56.20
34.57
28.66
19.67
12.13
3.51
3.51
42.43
15.89
14.47
4.17
13.47
12.92
6.88
12.13
1.76
1.76
3.51
1.80
1.80
8.34
7.74
28.66
75.61
79.49
241.93

d.f.
15
14
20
18
32
28
24
5
4
5
4
10
12
9
3
16
12
6
4
3
3
4
3
3
8
6
24
30
33
39

P
< 0.0001
< 0.0001
< 0.0001
0.0003
0.0052
0.1826
0.2331
0.0014
0.0164
0.6217
0.4761
< 0.0001
0.1962
0.1066
0.2441
0.6385
0.3749
0.3324
0.0164
0.6235
0.6235
0.4761
0.6147
0.6147
0.4006
0.2581
0.2331
< 0.0001
< 0.0001
< 0.0001

<-rationalize dropping this by
doing a bootstrap because the
p-value is so big. Even though
we are going against the
principles, we are doing
minimal damage.

lrm ( s u r v i v e d ⇠ s e x ⇤ p c l a s s ⇤ r c s ( age , 5 ) +
r c s ( age , 5 ) ⇤ ( s i b s p + p a r c h ) , d a t a=t 3 )
# Table 12.1
l a t e x ( anova ( f 1 ) , f i l e = ’ ’ , l a b e l= ’ t i t a n i c a n o v a 3 ’ , s i z e= ’ s m a l l ’ )

f1

Three-way interactions are clearly insignificant (P = 0.4) in Table 12.1. So
is parch (P = 0.6 for testing the combined main e↵ect + interaction e↵ects
for parch, i.e., whether parch is important for any age). These e↵ects would
be deleted in almost all bootstrap resamples had we bootstrapped a variable
selection procedure using ↵ = 0.1 for retention of terms, so we can safely
ignore these terms for future steps. The model not containing those terms
is fitted below. The ˆ2 in the model formula means to expand the terms in
parentheses to include all main e↵ects and second-order interactions.

302

12 Logistic Model Case Study 2: Survival of Titanic Passengers

lrm ( s u r v i v e d ⇠ ( s e x + p c l a s s + r c s ( age , 5 ) ) ^ 2 +
r c s ( age , 5 ) ⇤ s i b s p , data=t 3 )
p r i n t ( f , l a t e x=TRUE)
f

Logistic Regression Model
lrm(formula = survived ˜ (sex + pclass + rcs(age, 5))ˆ2 + rcs(age,
5) * sibsp, data = t3)
Frequencies of Missing Values Due to Each Variable
survived
0

sex
0

pclass
0

age
263

Model Likelihood
Ratio Test
Obs
1046 LR 2
553.87
0
619 d.f.
26
1
427 Pr(> 2 ) < 0.0001
L
max | @ log
| 6⇥10 6
@

Intercept
sex=male
pclass=2nd
pclass=3rd
age
age’
age”
age”’
sibsp
sex=male * pclass=2nd
sex=male * pclass=3rd
sex=male * age
sex=male * age’
sex=male * age”
sex=male * age”’
pclass=2nd * age
pclass=3rd * age
pclass=2nd * age’
pclass=3rd * age’
pclass=2nd * age”
pclass=3rd * age”
pclass=2nd * age”’

Coef
3.3075
-1.1478
6.7309
-1.6437
0.0886
-0.7410
4.9264
-6.6129
-1.0446
-0.7682
2.1520
-0.2191
1.0842
-6.5578
8.3716
-0.5446
-0.1634
1.9156
0.8205
-8.9545
-5.4276
9.3926

sibsp
0

Discrimination
Indexes
R2
0.555
g
2.427
gr
11.325
gp
0.365
Brier
0.130

Rank Discrim.
Indexes
C
0.878
Dxy
0.756
0.758
⌧a
0.366

S.E. Wald Z Pr(> |Z|)
1.8427
1.79
0.0727
1.0878
-1.06
0.2914
3.9617
1.70
0.0893
1.8299
-0.90
0.3691
0.1346
0.66
0.5102
0.6513
-1.14
0.2552
4.0047
1.23
0.2186
5.4100
-1.22
0.2216
0.3441
-3.04
0.0024
0.7083
-1.08
0.2781
0.6214
3.46
0.0005
0.0722
-3.04
0.0024
0.3886
2.79
0.0053
2.6511
-2.47
0.0134
3.8532
2.17
0.0298
0.2653
-2.05
0.0401
0.1308
-1.25
0.2118
1.0189
1.88
0.0601
0.6091
1.35
0.1780
5.5027
-1.63
0.1037
3.6475
-1.49
0.1367
6.9559
1.35
0.1769

12.3 Binary Logistic Model With Casewise Deletion of Missing Values

303

Table 12.3 Wald Statistics for survived
2

sex (Factor+Higher Order Factors)
All Interactions
pclass (Factor+Higher Order Factors)
All Interactions
age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
sibsp (Factor+Higher Order Factors)
All Interactions
sex ⇥ pclass (Factor+Higher Order Factors)
sex ⇥ age (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
pclass ⇥ age (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
age ⇥ sibsp (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

pclass=3rd * age”’
age * sibsp
age’ * sibsp
age” * sibsp
age”’ * sibsp

Coef
7.5403
0.0357
-0.0467
0.5574
-1.1937

199.42
56.14
108.73
42.83
47.04
24.51
22.72
19.95
10.99
35.40
10.08
8.17
8.17
6.86
6.11
6.11
10.99
1.81
1.81
22.72
67.58
70.68
253.18

d.f.
7
6
12
10
20
16
15
5
4
2
4
3
3
8
6
6
4
3
3
15
18
21
26

P
< 0.0001
< 0.0001
< 0.0001
< 0.0001
0.0006
0.0789
0.0902
0.0013
0.0267
< 0.0001
0.0391
0.0426
0.0426
0.5516
0.4113
0.4113
0.0267
0.6134
0.6134
0.0902
< 0.0001
< 0.0001
< 0.0001

S.E. Wald Z Pr(> |Z|)
4.8519
1.55
0.1202
0.0340
1.05
0.2933
0.2213
-0.21
0.8330
1.6680
0.33
0.7382
2.5711
-0.46
0.6425

l a t e x ( anova ( f ) , f i l e = ’ ’ , l a b e l= ’ t i t a n i c

a n o v a 2 ’ , s i z e= ’ s m a l l ’ ) #12.3

This is a very powerful model (ROC area = c = 0.88); the survival patterns
are easy to detect. The Wald ANOVA in Table 12.3 indicates especially strong
sex and pclass e↵ects ( 2 = 199 and 109, respectively). There is a very strong
sex ⇥ pclass interaction and a strong age ⇥ sibsp interaction, considering
the strength of sibsp overall.
Let us examine the shapes of predictor e↵ects. With so many interactions
in the model we need to obtain predicted values at least for all combinations
of sex and pclass. For sibsp we consider only two of its possible values.
p
P r e d i c t ( f , age , sex , p c l a s s , s i b s p =0 , f u n=p l o g i s )
ggplot (p)
# Fig. 12.5

Note the agreement between the lower right-hand panel of Figure 12.3 with
Figure 12.5. This results from our use of similar flexibility in the parametric

304

12 Logistic Model Case Study 2: Survival of Titanic Passengers

1st

1.00

2nd

3rd

0.75
sex
female

0.50

male
0.25

0.00
0

20

40

60 0

20

40

60 0

20

40

60

Age, years
Fig. 12.5 E↵ects of predictors on probability of survival of Titanic passengers, estimated for zero siblings or spouses

and nonparametric approaches (and similar e↵ective degrees of freedom). The
estimated e↵ect of sibsp as a function of age is shown in Figure 12.6.
g g p l o t ( P r e d i c t ( f , s i b s p , age=c ( 1 0 , 1 5 , 2 0 , 5 0 ) , c o n f . i n t=FALSE ) )
## Figure 12.6

Note that children having many siblings apparently had lower survival. Married adults had slightly higher survival than unmarried ones.
There will never be another Titanic, so we do not need to validate the
model for prospective use. But we use the bootstrap to validate the model
anyway, in an e↵ort to detect whether it is overfitting the data. We do not
penalize the calculations that follow for having examined the e↵ect of parch
or for testing three-way interactions, in the belief that these tests would
replicate well.
f
update ( f , x=TRUE, y=TRUE)
# x=TRUE, y=TRUE adds raw data to fit object so can bootstrap
s e t . s e e d (131)
# so can replicate re-samples
l a t e x ( v a l i d a t e ( f , B=200) , d i g i t s =2 , s i z e= ’ S s i z e ’ )

12.3 Binary Logistic Model With Casewise Deletion of Missing Values

305

0
−1

log odds

Age, years
−2

10
15

−3

20
−4

50

−5
−6
0

2

4

6

8

Number of Siblings/Spouses Aboard
Adjusted to:sex=male pclass=3rd

Fig. 12.6 E↵ect of number of siblings and spouses on the log odds of surviving, for
third class males

Index

Original Training Test Optimism Corrected n
Sample

Dxy
R2
Intercept
Slope
Emax
D
U
Q
B
g
gp

0.76
0.55
0.00
1.00
0.00
0.53
0.00
0.53
0.13
2.43
0.37

Sample

0.77
0.58
0.00
1.00
0.00
0.56
0.00
0.56
0.13
2.75
0.37

cal
c a l i b r a t e ( f , B=200)
p l o t ( c a l , s u b t i t l e s=FALSE)

Sample

0.74
0.53
0.08
0.87
0.05
0.50
0.01
0.49
0.13
2.37
0.35

Index

0.03
0.05
0.08
0.13
0.05
0.06
0.01
0.07
0.01
0.37
0.02

0.72
0.50
0.08
0.87
0.05
0.46
0.01
0.46
0.14
2.05
0.35

200
200
200
200
200
200
200
200
200
200
200

# Figure 12.7

n=1046
Mean absolute error=0.009
Mean squared error=0.00012
0.9 Quantile of absolute error=0.017

The output of validate indicates minor overfitting. Overfitting would
have been worse had the risk factors not been so strong. The closeness of
the calibration curve to the 45 line in Figure 12.7 demonstrates excellent
validation on an absolute probability scale. But the extent of missing data
casts some doubt on the validity of this model, and on the efficiency of its
parameter estimates.

306

12 Logistic Model Case Study 2: Survival of Titanic Passengers

this fuzz is probability of survival
1.0

0.8

Actual Probability

You want the fuzz at the top
to be "wide" you want a good
number of your patients <.1
or >.9. If you have both
you have something that
is diagnostically useful for
both sides.

10% of the passengers where
the prediction is >.017 - the
black line minus the 45
degree line.

0.6

0.4

Apparent

0.2

Bias−corrected
Ideal

0.0
0.0

0.2

0.4

0.6

0.8

Predicted Pr{survived=1}

1.0

sometimes the fuzz at the
top is the best measure of
discrimination that you can
have, if the calibration is
close to the 45 degree line

Fig. 12.7 Bootstrap overfitting-corrected loess nonparametric calibration curve for
casewise deletion model

12.4 Examining Missing Data Patterns
The first step to dealing with missing data is understanding the patterns
of missing values. To do this we use the Hmisc library’s naclus and naplot
functions, and the recursive partitioning library of Atkinson and Therneau.
Below naclus tells us which variables tend to be missing on the same persons,
and it computes the proportion of missing values for each variable. The rpart
function derives a tree to predict which types of passengers tended to have
age missing.
na.patterns
naclus ( t i t a n i c 3 )
require ( rpart )
# Recursive partitioning package
r p a r t ( i s . n a ( age ) ⇠ s e x + p c l a s s + s u r v i v e d +
s i b s p + parch , data=t i t a n i c 3 , minbucket =15)
n a p l o t ( n a . p a t t e r n s , ’ na p e r v a r ’ )
p l o t ( who.na , margin=. 1 ) ; t e x t ( who.na ) # Figure 12.8
plot ( na.patterns )

who.na

We see in Figure 12.8 that age tends to be missing on the same passengers as
the body bag identifier, and that it is missing in only 0.09 of first or second
class passengers. The category of passengers having the highest fraction of
missing ages is third class passengers having no parents or children on board.
Below we use Hmisc’s summary.formula function to plot simple descriptive
statistics on the fraction of missing ages, stratified by other variables. We see

12.4 Examining Missing Data Patterns

307

Fraction of NAs in each Variable
pclass
survived
name
sex
sibsp
parch
ticket
cabin
boat
fare
embarked
age
home.dest
body

pclass=ab
|

●
●
●
●
●
●
●
●
●
●

parch>=0.5

●
●

0.09167

●
●

0.0

0.2

0.4
0.6
Fraction
of NAs

0.1806

0.8

0.2
0.3

boat
embarked
cabin
fare
ticket
parch
sibsp
age

0.4

body
home.dest

Fraction Missing

0.1

sex
name
pclass
survived

0.0

Fig. 12.8 Patterns of missing data. Upper left panel shows the fraction of observations missing on each predictor. Lower panel depicts a hierarchical cluster analysis of
missingness combinations. The similarity measure shown on the Y -axis is the fraction
of observations for which both variables are missing. Right panel shows the result of
recursive partitioning for predicting is.na(age). The rpart function found only
strong patterns according to passenger class.

that without adjusting for other variables, age is slightly more missing on
nonsurviving passengers.
p l o t ( summary ( i s . n a ( age ) ⇠ s e x + p c l a s s + s u r v i v e d +
s i b s p + parch , data=t 3 ) ) # Figure 12.9

Let us derive a logistic model to predict missingness of age, to see if the
survival bias maintains after adjustment for the other variables.

0.3249

308

12 Logistic Model Case Study 2: Survival of Titanic Passengers
mean

sex

female
male

466
843

●
●

pclass

1st
2nd
3rd

N

323
277
709

●
●
●

Survived

No
Yes

●

809
500

●
0
●
1
●
2
●
3
4 ●
5 ●
8

891
319
42
20
22
6
9

●

Number of Siblings/Spouses Aboard

●

Number of Parents/Children Aboard

●
0
1 ●
●
2
3 ●
●
4
5 ●
6 ●
9

Overall

●

1309

●

0.0

0.2

1002
170
113
8
6
6
2
2

0.4

0.6

0.8

1.0

is.na(age)

Fig. 12.9 Univariable descriptions of proportion of passengers with missing age

12.5 Multiple Imputation

309

lrm ( i s . n a ( age ) ⇠ s e x ⇤ p c l a s s + s u r v i v e d + s i b s p + parch ,
data=t 3 )
p r i n t (m, l a t e x=TRUE, n e e d s p a c e= ’ 2 i n ’ )

m

Logistic Regression Model
lrm(formula = is.na(age) ˜ sex * pclass + survived + sibsp +
parch, data = t3)
Model Likelihood
Ratio Test
Obs
1309 LR 2
114.99
FALSE
1046 d.f.
8
TRUE
263 Pr(> 2 ) < 0.0001
L
max | @ log
| 5⇥10 6
@

Coef
Intercept
-2.2030
sex=male
0.6440
pclass=2nd
-1.0079
pclass=3rd
1.6124
survived
-0.1806
sibsp
0.0435
parch
-0.3526
sex=male * pclass=2nd 0.1347
sex=male * pclass=3rd -0.8563
l a t e x ( anova (m) ,

Discrimination
Indexes
R2
0.133
g
1.015
gr
2.759
gp
0.126
Brier
0.148

Rank Discrim.
Indexes
C
0.703
Dxy
0.406
0.452
⌧a
0.131

S.E. Wald Z Pr(> |Z|)
0.3641
-6.05 < 0.0001
0.3953
1.63
0.1033
0.6658
-1.51
0.1300
0.3596
4.48 < 0.0001
0.1828
-0.99
0.3232
0.0737
0.59
0.5548
0.1253
-2.81
0.0049
0.7545
0.18
0.8583
0.4214
-2.03
0.0422

f i l e = ’ ’ , l a b e l= ’ t i t a n i c

a n o v a . n a ’ ) # Table 12.5

Fortunately, after controlling for other variables, Table 12.5 provides evidence that nonsurviving passengers are no more likely to have age missing.
The only important predictors of missingness are pclass and parch (the more
parents or children the passenger has on board, the less likely age was to be
missing).

12.5 Multiple Imputation
Multiple imputation is expected to reduce bias in estimates as well as to provide an estimate of the variance–covariance matrix of ˆ penalized for imputation. With multiple imputation, survival status can be used to impute missing
ages, so the age relationship will not be as attenuated as with single conditional mean imputation. aregImpute The following uses the Hmisc package

310

12 Logistic Model Case Study 2: Survival of Titanic Passengers

Table 12.5 Wald Statistics for is.na(age)
2

sex (Factor+Higher Order Factors)
All Interactions
pclass (Factor+Higher Order Factors)
All Interactions
survived
sibsp
parch
sex ⇥ pclass (Factor+Higher Order Factors)
TOTAL

5.61
5.58
68.43
5.58
0.98
0.35
7.92
5.58
82.90

d.f.
P
3 0.1324
2 0.0614
4 < 0.0001
2 0.0614
1 0.3232
1 0.5548
1 0.0049
2 0.0614
8 < 0.0001

aregImpute function to do predictive mean matching, using van Buuren’s

“Type 1” matching [83, Section 3.4.2] in conjunction with bootstrapping to
incorporate all uncertainties, in the context of smooth additive imputation
models. Sampling of donors is handled by distance weighting to yield better
distributions of imputed values. By default, aregImpute does not transform
age when it is being predicted from the other variables. Four knots are used
to transform age when used to impute other variables (not needed here as no
other missings were present in the variables of interest). Since the fraction of
263
observations with missing age is 1309
= 0.2 we use 20 imputations.
s e t .s e e d (17)
# so can reproduce random aspects
mi
ar egIm put e (⇠ age + s e x + p c l a s s +
s i b s p + parch + s u r v i v e d ,
data=t3 , n . i m p u t e =20 , nk=4 , pr=FALSE)
mi
Multiple Imputation using Bootstrap and PMM
aregImpute(formula = ⇠age + sex + pclass + sibsp + parch + survived,
data = t3, n.impute = 20, nk = 4, pr = FALSE)
n: 1309

p: 6

Number of NAs:
age
sex
263
0
age
sex
pclass
sibsp
parch
survived

type d.f.
s
1
c
1
c
2
s
2
s
2
l
1

Imputations: 20
pclass
0

sibsp
0

nk: 4
parch survived
0
0

12.5 Multiple Imputation

311

Transformation of Target Variables Forced to be Linear
R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
age
0.295
# Print the first 10 imputations for the first 10 passengers
# having missing age
mi$ imputed $ age [ 1 : 1 0 , 1 : 1 0 ]
16
38
41
47
60
70
71
75
81
107

[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
40
49
24
29 60.0
58
64
36
50
61
33
45
40
49 80.0
2
38
38
36
53
29
24
19
31 40.0
60
64
42
30
65
40
42
29
48 36.0
46
64
30
38
42
52
40
22
31 38.0
22
19
24
40
33
16
14
23
23 18.0
24
19
27
59
23
30
62
57
30 42.0
31
64
40
40
63
43
23
36
61 45.5
58
64
27
24
50
44
57
47
31 45.0
30
64
62
39
67
52
18
24
62 32.5
38
64
47
19
23

p l o t ( mi )
Ecdf ( t 3 $ age , add=TRUE, c o l= ’ g r a y ’ , lwd =2 ,
s u b t i t l e s=FALSE) #Fig. 12.10

Proportion <= x

1.0

this shift seen here is exactly
what it should be, knowing that
those missing age were largely
3rd class.

0.8
0.6
0.4
0.2
0.0
0

20

40

60

80

Imputed age
Fig. 12.10 Distributions of imputed and actual ages for the Titanic dataset. Imputed
values are in black and actual ages in gray.

We now fit logistic models for five completed datasets. The fit.mult.impute
function fits five models and examines the within– and between–imputation

312

12 Logistic Model Case Study 2: Survival of Titanic Passengers

Table 12.6 Wald Statistics for survived
2

sex (Factor+Higher Order Factors)
All Interactions
pclass (Factor+Higher Order Factors)
All Interactions
age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
sibsp (Factor+Higher Order Factors)
All Interactions
sex ⇥ pclass (Factor+Higher Order Factors)
sex ⇥ age (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
pclass ⇥ age (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
age ⇥ sibsp (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
TOTAL NONLINEAR
TOTAL INTERACTION
TOTAL NONLINEAR + INTERACTION
TOTAL

240.42
54.56
114.21
36.43
50.37
25.88
24.21
24.22
12.86
30.99
11.38
8.15
8.15
5.30
4.63
4.63
12.86
1.84
1.84
24.21
67.12
70.99
298.78

d.f.
7
6
12
10
20
16
15
5
4
2
4
3
3
8
6
6
4
3
3
15
18
21
26

P
< 0.0001
< 0.0001
< 0.0001
0.0001
0.0002
0.0557
0.0616
0.0002
0.0120
< 0.0001
0.0226
0.0430
0.0430
0.7246
0.5918
0.5918
0.0120
0.6058
0.6058
0.0616
< 0.0001
< 0.0001
< 0.0001

variances to compute an imputation-corrected variance–covariance matrix
that is stored in the fit object f.mi. fit.mult.impute will also average the five
ˆ vectors, storing the result in f.mi$coefficients. The function also prints
the ratio of imputation-corrected variances to average ordinary variances.
f.mi
fit.mult.impute (
s u r v i v e d ⇠ ( s e x + p c l a s s + r c s ( age , 5 ) ) ^ 2 +
r c s ( age , 5 ) ⇤ s i b s p ,
lrm , mi , data=t3 , pr=FALSE)
l a t e x ( anova ( f . m i ) , f i l e = ’ ’ , l a b e l= ’ t i t a n i c a n o v a . m i ’ , s i z e= ’ s m a l l ’ )
# Table 12.6

The Wald 2 for age is reduced by accounting for imputation but is increased (by a lesser amount) by using patterns of association with survival
status to impute missing age. The Wald tests are all adjusted for multiple imputation. Now examine the fitted age relationship using multiple imputation
vs. casewise deletion.
p1
Predict ( f ,
age , p c l a s s , sex , s i b s p =0 , f u n=p l o g i s )
p2
Predict ( f.mi ,
age , p c l a s s , sex , s i b s p =0 , f u n=p l o g i s )
p
r b i n d ( ’ C a s e w i s e D e l e t i o n ’=p1 , ’ M u l t i p l e I m p u t a t i o n ’=p2 )
g g p l o t ( p , g r o u p s= ’ s e x ’ , y l a b= ’ P r o b a b i l i t y o f S u r v i v i n g ’ )
# Figure 12.11

12.5 Multiple Imputation

313

Casewise Deletion

1.00

Multiple Imputation

0.75
1st

0.50

0.00
1.00
0.75

sex
2nd

Probability of Surviving

0.25

0.50

female
male

0.25
0.00
1.00
0.75
3rd

0.50
0.25
0.00
0

20

40

60

0

20

40

60

Age, years
Fig. 12.11 Predicted probability of survival for males from fit using casewise deletion
again (top) and multiple random draw imputation (bottom). Both sets of predictions
are for sibsp=0.

314

12 Logistic Model Case Study 2: Survival of Titanic Passengers

12.6 Summarizing the Fitted Model
In this section we depict the model fitted using multiple imputation, by computing odds ratios and by showing various predicted values. For age, the odds
ratio for an increase from 1 year old to 30 years old is computed, instead of
the default odds ratio based on outer quartiles of age. The estimated odds
ratios are very dependent on the levels of interacting factors, so Figure 12.12
depicts only one of many patterns.
# Get predicted values for certain types of passengers
s
summary ( f . m i , age=c ( 1 , 3 0 ) , s i b s p =0:1)
# override default ranges for 3 variables
p l o t ( s , l o g=TRUE, main= ’ ’ )
# Figure 12.12

0.10

0.50

2.00

5.00

age − 30:1
sibsp − 1:0
sex − female:male
pclass − 1st:3rd
pclass − 2nd:3rd

these are wider CI because of imputation, but not as wide as casewise
deletion would be since the sample size is smaller in that case.
Adjusted to:sex=male pclass=3rd age=28 sibsp=0
Fig. 12.12 Odds ratios for some predictor settings

Now compute estimated probabilities of survival for a variety of settings of
the predictors.
phat

predict ( f.mi ,
combos
e x p a n d . g r i d ( age=c ( 2 , 2 1 , 5 0 ) , s e x=l e v e l s ( t 3 $ s e x ) ,
p c l a s s=l e v e l s ( t 3 $ p c l a s s ) ,
s i b s p =0) , t y p e= ’ f i t t e d ’ )
# Can also use Predict(f.mi, age=c(2,21,50), sex, pclass,
#
sibsp=0, fun=plogis)$yhat
o p t i o n s ( d i g i t s =1)
d a t a . f r a m e ( combos , phat )

12.6 Summarizing the Fitted Model

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

age
2
21
50
2
21
50
2
21
50
2
21
50
2
21
50
2
21
50

315

sex pclass sibsp phat
female
1st
0 0.97
female
1st
0 0.98
female
1st
0 0.97
male
1st
0 0.88
male
1st
0 0.48
male
1st
0 0.27
female
2nd
0 1.00
female
2nd
0 0.90
female
2nd
0 0.82
male
2nd
0 1.00
male
2nd
0 0.08
male
2nd
0 0.04
female
3rd
0 0.85
female
3rd
0 0.57
female
3rd
0 0.37
male
3rd
0 0.91
male
3rd
0 0.13
male
3rd
0 0.06

o p t i o n s ( d i g i t s =5)

We can also get predicted values by creating an R function that will evaluate
the model on demand.
pred.logit
Function ( f . m i )
# Note: if don’t define sibsp to pred.logit, defaults to 0
# normally just type the function name to see its body
l a t e x ( p r e d . l o g i t , f i l e = ’ ’ , t y p e= ’ S i n p u t ’ , s i z e= ’ s m a l l ’ ,
w i d t h . c u t o f f =49)
pred.logit
f u n c t i o n ( s e x = ” male ” , p c l a s s = ” 3 rd ” , age = 2 8 ,
s i b s p = 0)
{
3 .2427671
0 . 9 5 4 3 1 8 0 9 ⇤ ( s e x == ” male ” ) + 5 . 4 0 8 6 5 0 5 ⇤
( p c l a s s == ” 2nd” )
1 . 3 3 7 8 6 2 3 ⇤ ( p c l a s s ==
” 3 rd ” ) + 0 . 0 9 1 1 6 2 6 4 9 ⇤ age
0 .00031204327 ⇤
pmax ( age
6 , 0 ) ^ 3 + 0 . 0 0 2 1 7 5 0 4 1 3 ⇤ pmax ( age
2 1 , 0 )^3
0 . 0 0 2 7 6 2 7 0 3 2 ⇤ pmax ( age
2 7 , 0 )^3 +
^
0 . 0 0 0 9 8 0 5 1 3 7 ⇤ pmax ( age
36 , 0) 3
8 .0808484e 05 ⇤
pmax ( age
55 . 8 , 0 ) ^ 3
1 .1567976 ⇤ sibsp +
( s e x == ” male ” ) ⇤ ( 0.46061284 ⇤ ( p c l a s s ==
” 2nd” ) + 2 . 0 4 0 6 5 2 3 ⇤ ( p c l a s s == ” 3 rd ” ) ) +
( s e x == ” male ” ) ⇤ ( 0.22469066 ⇤ age + 0 . 0 0 0 4 3 7 0 8 2 9 6 ⇤
pmax ( age
6 , 0 )^3
0 . 0 0 2 6 5 0 5 1 3 6 ⇤ pmax ( age
^
2 1 , 0 ) 3 + 0 . 0 0 3 1 2 0 1 4 0 4 ⇤ pmax ( age
27 ,
0 )^3
0 . 0 0 0 9 7 9 2 3 7 4 9 ⇤ pmax ( age
36 ,
0 ) ^ 3 + 7 .2527708e 05 ⇤ pmax ( age
55 . 8 ,
0 ) ^ 3 ) + ( p c l a s s == ” 2nd” ) ⇤ ( 0.46144083 ⇤
age + 0 . 0 0 0 7 0 1 9 4 8 4 9 ⇤ pmax ( age
6 , 0 )^3
0 . 0 0 3 4 7 2 6 6 6 2 ⇤ pmax ( age
2 1 , 0 )^3 + 0 . 0 0 3 5 2 5 5 3 8 7 ⇤
pmax ( age
2 7 , 0 )^3
0 . 0 0 0 7 9 0 0 8 9 1 ⇤ pmax ( age
3 6 , 0 ) ^ 3 + 3 .5268151e 05 ⇤ pmax ( age
55 . 8 ,

316

}

12 Logistic Model Case Study 2: Survival of Titanic Passengers
0 ) ^ 3 ) + ( p c l a s s == ” 3 rd ” ) ⇤ ( 0.17513289 ⇤
age + 0 . 0 0 0 3 5 2 8 3 3 5 8 ⇤ pmax ( age
6 , 0 )^3
0 . 0 0 2 3 0 4 9 3 7 2 ⇤ pmax ( age
2 1 , 0 )^3 + 0 . 0 0 2 8 9 7 8 9 6 2 ⇤
pmax ( age
2 7 , 0 )^3
0 . 0 0 1 0 5 1 4 5 ⇤ pmax ( age
3 6 , 0 ) ^ 3 + 0 . 0 0 0 1 0 5 6 5 7 3 5 ⇤ pmax ( age
55 . 8 ,
0 ) ^ 3 ) + s i b s p ⇤ ( 0 . 0 4 0 8 3 0 7 7 3 ⇤ age
1 .5627772e 05 ⇤
pmax ( age
6 , 0 ) ^ 3 + 0 . 0 0 0 1 2 7 9 0 2 5 6 ⇤ pmax ( age
2 1 , 0 )^3
0 . 0 0 0 2 5 0 3 9 3 8 5 ⇤ pmax ( age
27 ,
0 ) ^ 3 + 0 . 0 0 0 1 7 8 7 1 7 0 1 ⇤ pmax ( age
3 6 , 0 )^3
4 .0597949e 05 ⇤ pmax ( age
55 . 8 , 0 ) ^ 3 )

# Run the newly created function
p l o g i s ( p r e d . l o g i t ( age=c ( 2 , 2 1 , 5 0 ) , s e x= ’ male ’ , p c l a s s= ’ 3 rd ’ ) )
[1] 0.914817 0.132640 0.056248

A nomogram could be used to obtain predicted values manually, but this is
not feasible when so many interaction terms are present.

Chapter 13

Ordinal Logistic Regression

13.1 Background
Many medical and epidemiologic studies incorporate an ordinal response variable. In some cases an ordinal response Y represents levels of a standard
measurement scale such as severity of pain (none, mild, moderate, severe).
In other cases, ordinal responses are constructed by specifying a hierarchy of
separate endpoints. For example, clinicians may specify an ordering of the
severity of several component events and assign patients to the worst event
present from among none, heart attack, disabling stroke, and death. Still
another use of ordinal response methods is the application of rank-based
methods to continuous responses so as to obtain robust inferences. For example, the proportional odds model described later allows for a continuous
Y and is really a generalization of the Wilcoxon–Mann–Whitney rank test.
Thus the semiparametric proportional odds model is a direct competitor of
ordinary linear models.
There are many variations of logistic models used for predicting an ordinal
response variable Y . All of them have the advantage that they do not assume
a spacing between levels of Y . In other words, the same regression coefficients
and P -values result from an analysis of a response variable having levels 0, 1, 2
when the levels are recoded 0, 1, 20. Thus ordinal models use only the rankordering of values of Y .
In this chapter we consider two of the most popular ordinal logistic models,
the proportional odds (PO) form of an ordinal logistic model639 and the
forward continuation ratio (CR) ordinal logistic model.186 Chapter 15 deals
with a wider variety of ordinal models with emphasis on analysis of continuous
Y.

317

1

318

13 Ordinal Logistic Regression

13.2 Ordinality Assumption
A basic assumption of all commonly used ordinal regression models is that
the response variable behaves in an ordinal fashion with respect to each
predictor. Assuming that a predictor X is linearly related to the log odds
of some appropriate event, a simple way to check for ordinality is to plot the
mean of X stratified by levels of Y . These means should be in a consistent
order. If for many of the Xs, two adjacent categories of Y do not distinguish
the means, that is evidence that those levels of Y should be pooled.
One can also estimate the mean or expected value of X|Y = j (E(X|Y =
j)) given that the ordinal model assumptions hold. This is a useful tool for
checking those assumptions, at least in an unadjusted fashion. For simplicity,
assume that X is discrete, and let Pjx = Pr(Y = j|X = x) be the probability
that Y = j given X = x that is dictated from the model being fitted, with
X being the only predictor in the model. Then
Pr(X = x|Y = j) = Pr(Y = j|X = x) Pr(X = x)/Pr(Y = j)
X
E(X|Y = j) =
xPjx Pr(X = x)/ Pr(Y = j),
(13.1)
x

and the expectation can be estimated by
X
Ê(X|Y = j) =
xP̂jx fx /gj ,

(13.2)

x

where P̂jx denotes the estimate of Pjx from the fitted one-predictor model
(for inner values of Y in the PO models, these probabilities are di↵erences
between terms given by Equation 13.4 below), fx is the frequency of X = x
in the sample of size n, and gj is the frequency of Y = j in the sample. This
estimate can be computed conveniently without grouping the data by X. For
n subjects let the n values of X be x1 , x2 , . . . , xn . Then
Ê(X|Y = j) =

n
X

xi P̂jxi /gj .

(13.3)

i=1

Note that if one were to compute di↵erences between conditional means of X
and the conditional means of X given PO, and if furthermore the means were
conditioned on Y
j instead of Y = j, the result would be proportional to
means of score residuals defined later in Equation 13.6.

13.3 Proportional Odds Model

319

13.3 Proportional Odds Model
13.3.1 Model
The most commonly used ordinal logistic model was described in Walker
and Duncan639 and later called the proportional odds (PO) model by McCullagh.442 The PO model is best stated as follows, for a response variable
having levels 0, 1, 2, . . . , k:
Pr[Y

j|X] =

1
,
1 + exp[ (↵j + X )]

(13.4)

where j = 1, 2, . . . , k. Some authors write the model in terms of Y  j. Our
formulation makes the model coefficients consistent with the binary logistic
model. There are k intercepts (↵s). For fixed j, the model is an ordinary
logistic model for the event Y
j. By using a common vector of regression
coefficients connecting probabilities for varying j, the PO model allows for
parsimonious modeling of the distribution of Y .
There is a nice connection between the PO model and the Wilcoxon–
Mann–Whitney two-sample test: when there is a single predictor X1 that is
binary, the numerator of the score test for testing H0 : 1 = 0 is proportional
to the two-sample test statistic [658, pp. 2258-2259].

13.3.2 Assumptions and Interpretation of Parameters
There is an implicit assumption in the PO model that the regression coefficients ( ) are independent of j, the cuto↵ level for Y . One could say that
there is no X ⇥ Y interaction if PO holds. For a specific Y -cuto↵ j, the model
has the same assumptions as the binary logistic model (Section 10.1.1). That
is, the model in its simplest form assumes the log odds that Y
j is linearly
related to each X and that there is no interaction between the Xs.
In designing clinical studies, one sometimes hears the statement that an
ordinal outcome should be avoided since statistical tests of patterns of those
outcomes are hard to interpret. In fact, one interprets e↵ects in the PO
model using ordinary odds ratios. The di↵erence is that a single odds ratio is
assumed to apply equally to all events Y
j, j = 1, 2, . . . , k. If linearity and
additivity hold, the Xm + 1 : Xm odds ratio for Y
j is exp( m ), whatever
the cuto↵ j.
The proportional hazards assumption is frequently violated, just as the assumptions of normality of residuals with equal variance in ordinary regression
are frequently violated, but the PO model can still be useful and powerful in
this situation. As stated by Senn and Julious559 ,

2

320

13 Ordinal Logistic Regression

Clearly, the dependence of the proportional odds model on the assumption
of proportionality can be over-stressed. Suppose that two di↵erent statisticians
would cut the same three-point scale at di↵erent cut points. It is hard to see how
anybody who could accept either dichotomy could object to the compromise
answer produced by the proportional odds model.

3

Sometimes it helps in interpreting the model to estimate the mean Y as
a function of one or more predictors, even though this assumes a spacing for
the Y -levels.

13.3.3 Estimation
The PO model is fitted using MLE on a somewhat complex likelihood function
that is dependent on di↵erences in logistic model probabilities. The estimation process forces the ↵s to be in descending order.

13.3.4 Residuals
Schoenfeld residuals552 are very e↵ective228 in checking the proportional hazards assumption in the Cox129 survival model. For the PO model one could
analogously compute each subject’s contribution to the first derivative of
the log likelihood function with respect to m , average them separately by
levels of Y , and examine trends in the residual plots as in Section 20.5.2.
A few examples have shown that such plots are usually hard to interpret.
Easily interpreted score residual plots for the PO model can be constructed,
however, by using the fitted PO model to predict a series of binary events
Y
j, j = 1, 2, . . . , k, using the corresponding predicted probabilities
P̂ij =

1
1 + exp[ (ˆ
↵j + Xi ˆ)]

,

(13.5)

where Xi stands for a vector of predictors for subject i. Then, after forming
an indicator variable for the event currently being predicted ([Yi
j]), one
computes the score (first derivative) components Uim from an ordinary binary
logistic model:
Uim = Xim ([Yi j] P̂ij ),
(13.6)
for the subject i and predictor m. Then, for each column of U , plot the mean
Ū·m and confidence limits, with Y (i.e., j) on the x-axis. For each predictor
the trend against j should be flat if PO holds. a In binary logistic regression,
partial residuals are very useful as they allow the analyst to fit linear e↵ects
a

If ˆ were derived from separate binary fits, all Ū·m ⌘ 0.

13.3 Proportional Odds Model

321

for all the predictors but then to nonparametrically estimate the true transformation that each predictor requires (Section 10.4). The partial residual is
defined as follows, for the ith subject and mth predictor variable.112, 366
rim = ˆm Xim +
where

Yi
P̂i (1

P̂i
P̂i )

,

(13.7)
written like this because it can then be the normal logistic when k=1

P̂i =

1
1 + exp[ (↵ + Xi ˆ)]

.

(13.8)

A smoothed plot (e.g., using the moving linear regression algorithm in
loess108 ) of Xim against rim provides a nonparametric estimate of how Xm
relates to the log relative odds that Y = 1|Xm . For ordinal Y , we just need
to compute binary model partial residuals for all cuto↵s j:
[Yi j] P̂ij
rim = ˆm Xim +
,
P̂ij (1 P̂ij )

(13.9)

then to make a plot for each m showing smoothed partial residual curves
for all j, looking for similar shapes and slopes for a given predictor for all j.
Each curve provides an estimate of how Xm relates to the relative log odds
that Y
j. Since partial residuals allow examination of predictor transformations (linearity) while simultaneously allowing examination of PO (parallelism), partial residual plots are generally preferred over score residual plots
for ordinal models.
Li and Shepherd395 have a residual for ordinal models that serves for the
entire range of Y without the need to consider cuto↵s. Their residual is
useful for checking functional form of predictors but not the proportional
odds assumption.

13.3.5 Assessment of Model Fit
Peterson and Harrell495 developed score and likelihood ratio tests for testing
the PO assumption. The score test is used in the SAS PROC LOGISTIC,535
but its extreme anti-conservatism in many cases can make it unreliable.495
For determining whether the PO assumption is likely to be satisfied for
each predictor separately, there are several graphics that are useful. One is the
graph comparing means of X|Y with and without assuming PO, as described
in Section 13.2 (see Figure 14.2 for an example). Another is the simple method
of stratifying on each predictor and computing the logits of all proportions of
the form Y
j, j = 1, 2, . . . , k. When proportional odds holds, the di↵erences
in logits between di↵erent values of j should be the same at all levels of X,

4

322

13 Ordinal Logistic Regression

because the model dictates that logit(Y
j|X) logit(Y
for any constant X. An example of this is in Figure 13.1.

i|X) = ↵j

↵i ,

r e q u i r e ( Hmisc )
getHdata ( s u p p o r t )
sfdm
a s . i n t e g e r ( s u p p o r t $ sfdm2 )
1
sf
function (y)
c ( ’Y 1 ’=q l o g i s ( mean ( y
1 ) ) , ’Y 2 ’=q l o g i s ( mean ( y
2)) ,
’Y 3 ’=q l o g i s ( mean ( y
3)))
s
summary ( sfdm ⇠ a d l s c + s e x + age + meanbp , f u n=s f ,
data=s u p p o r t )
p l o t ( s , which = 1 : 3 , pch = 1 : 3 , x l a b= ’ l o g i t ’ , vnames= ’ names ’ ,
main= ’ ’ , w i d t h . f a c t o r =1 . 5 )
# Figure 13.1

N

adlsc

0.000
[0.495,1.167)
[1.167,3.024)
[3.024,7.000]

●
●
●
●

sex

female
male

377
464

●
●

age

[19.8, 52.4)
[52.4, 65.3)
[65.3, 74.8)
[74.8,100.1]

211
210
210
210

●
●
●
●

meanbp

[ 0, 64)
[ 64, 78)
[ 78,108)
[108,180]

211
216
204
210

●
●
●
●

Overall

841

●

−0.5

0.0

282
150
199
210

0.5

1.0

1.5

logit

Fig. 13.1 Checking PO assumption separately for a series of predictors. The circle,
triangle, and plus sign correspond to Y
1, 2, 3, respectively. PO is checked by
examining the vertical constancy of distances between any two of these three symbols.
Response variable is the severe functional disability scale sfdm2 from the 1000-patient
SUPPORT dataset, with the last two categories combined because of low frequency
of coma/intubation.

When Y is continuous or almost continuous and X is discrete, the PO model
assumes that the logit of the cumulative distribution function of Y is parallel across categories of X.
The corresponding, more rigid, assumptions

13.3 Proportional Odds Model

323

of the ordinary linear model (here, parametric ANOVA) are parallelism and
linearity if the normal inverse cumulative distribution function across categories of X. As an example consider the web site’s diabetes dataset, where
we consider the distribution of log glycohemoglobin across subjects’ body
frames.
getHdata ( d i a b e t e s )
a
Ecdf (⇠ l o g ( g l y h b ) , group=frame , f u n=qnorm ,
x l a b= ’ l o g ( HbA1c ) ’ , l a b e l . c u r v e s=FALSE, data=d i a b e t e s ,
y l a b=e x p r e s s i o n ( p a s t e ( Phi ^ 1 , (F [ n ] ( x ) ) ) ) ) # Fig. 13.2
b
Ecdf (⇠ l o g ( g l y h b ) , group=frame , f u n=q l o g i s ,
x l a b= ’ l o g ( HbA1c ) ’ , l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) ,
data=d i a b e t e s , y l a b=e x p r e s s i o n ( l o g i t (F [ n ] ( x ) ) ) )
p r i n t ( a , more=TRUE, s p l i t =c ( 1 , 1 , 2 , 1 ) )
p r i n t ( b , s p l i t =c ( 2 , 1 , 2 , 1 ) )

5

logit(Fn(x))

Φ−1(Fn(x))

2

0

−2

0

small
medium
large

−5

1.0

1.5

2.0

log(HbA1c)

2.5

1.0

1.5

2.0

log(HbA1c)

Fig. 13.2 Transformed empirical cumulative distribution functions stratified by
body frame in the diabetes dataset. Left panel: checking all assumptions of the
parametric ANOVA. Right panel: checking all assumptions of the PO model (here,
Kruskal–Wallis test).

One could conclude the the right panel of Figure 13.2 displays more parallelism than the left panel displays linearity, so the assumptions of the PO
model are better satisfied than the assumptions of the ordinary linear model.
Chapter 14 has many examples of graphics for assessing fit of PO models.
Regarding assessment of linearity and additivity assumptions, splines, partial

2.5

324

13 Ordinal Logistic Regression

residual plots, and interaction tests are among the best tools. Fagerland and
Hosmer178 have a good review of goodness-of-fit tests for the PO model.

13.3.6 Quantifying Predictive Ability
2
The RN
coefficient is really computed from the model LR 2 ( 2 added to
a model containing only the k intercept parameters) to describe the model’s
predictive power. The Somers’ Dxy rank correlation between X ˆ and Y is
an easily interpreted measure of predictive discrimination. Since it is a rank
measure, it does not matter which intercept ↵ is used in the calculation.
The probability of concordance, c, is also a useful measure. Here one takes all
possible pairs of subjects having di↵ering Y values and computes the fraction
of such pairs for which the values of X ˆ are in the same direction as the two
Y values. c could be called a generalized ROC area in this setting. As before,
Dxy = 2(c 0.5). Note that Dxy , c, and the Brier score B can easily be
computed for various dichotomizations of Y , to investigate predictive ability
in more detail.

13.3.7 Describing the Fitted Model
As discussed in Section 5.1, models are best described by computing predicted
values or di↵erences in predicted values. For PO models there are four and
sometimes five types of relevant predictions:
1.
2.
3.
4.
5.

logit[Y
j|X], i.e., the linear predictor
Prob[Y
j|X]
Prob[Y = j|X]
Quantiles of Y |X (e.g., the medianb )
E(Y |X) if Y is interval scaled.

For the first two quantities above a good default choice for j is the middle
category. Partial e↵ect plots are as simple to draw for PO models as they are
for binary logistic models. Other useful graphics, as before, are odds ratio
charts and nomograms. For the latter, an axis displaying the predicted mean
makes the model more interpretable, under scaling assumptions on Y .
b

If Y does not have very many levels, the median will be a discontinuous function
of X and may not be satisfactory.

13.3 Proportional Odds Model

325

13.3.8 Validating the Fitted Model
The PO model is validated much the same way as the binary logistic model
(see Section 10.9). For estimating an overfitting-corrected calibration curve
(Section 10.11) one estimates Pr(Y
j|X) using one j at a time.

13.3.9 R Functions
The rms package’s lrm and orm functions fit the PO model directly, assuming
that the levels of the response variable (e.g., the levels of a factor variable)
are listed in the proper order. lrm is intended to be used for the case where
the number of unique values of Y are less than a few dozen whereas orm
handles the continuous Y case efficiently, as well as allowing for links other
than the logit. See Chapter 15 for more information.
If the response is numeric, lrm assumes the numeric codes properly order the responses. If it is a character vector and is not a factor, lrm assumes the correct ordering is alphabetic. Of course ordered variables in R
are appropriate response variables for ordinal regression. The predict function (predict.lrm) can compute all the quantities listed in Section 13.3.7
except for quantiles.
The R functions popower and posamsize (in the Hmisc package) compute
power and sample size estimates for ordinal responses using the proportional
odds model.
The function plot.xmean.ordinaly in rms computes and graphs the quantities described in Section 13.2. It plots simple Y -stratified means overlaid with Ê(X|Y = j), with j on the x-axis. The Ês are computed for
both PO and continuation ratio ordinal logistic models. The Hmisc package’s summary.formula function is also useful for assessing the PO assumption (Figure 13.1). Generic rms functions such as validate, calibrate, and
nomogram work with PO model fits from lrm as long as the analyst specifies which intercept(s) to use. rms has a special function generator Mean for
constructing an easy-to-use function for getting the predicted mean Y from
a PO model. This is handy with plot and nomogram. If the fit has been
run through the bootcov function, it is easy to use the Predict function to
estimate bootstrap confidence limits for predicted means.

326

13 Ordinal Logistic Regression

13.4 Continuation Ratio Model
13.4.1 Model
Unlike the PO model, which is based on cumulative probabilities, the continuation ratio (CR) model is based on conditional probabilities. The (forward)
CR model31, 51, 186 is stated as follows for Y = 0, . . . , k.

Pr(Y = j|Y
logit(Y = 0|Y

1
1 + exp[ (✓j + X )]
0, X) = logit(Y = 0|X)
j, X) =

= ✓0 + X
logit(Y = 1|Y

(13.10)

1, X) = ✓1 + X
...

logit(Y = k

1|Y

k

1, X) = ✓k

1

+X .

The CR model has been said to be likely to fit ordinal responses when subjects
have to “pass through” one category to get to the next. The CR model is a
discrete version of the Cox proportional hazards model. The discrete hazard
function is defined as Pr(Y = j|Y
j).

13.4.2 Assumptions and Interpretation of Parameters
The CR model assumes that the vector of regression coefficients, , is the
same regardless of which conditional probability is being computed.
One could say that there is no X⇥ condition interaction if the CR model
holds. For a specific condition Y
j, the model has the same assumptions as
the binary logistic model (Section 10.1.1). That is, the model in its simplest
form assumes that the log odds that Y = j conditional on Y
j is linearly
related to each X and that there is no interaction between the Xs.
A single odds ratio is assumed to apply equally to all conditions Y
j, j =
0, 1, 2, . . . , k 1. If linearity and additivity hold, the Xm + 1 : Xm odds ratio
for Y = j is exp( m ), whatever the conditioning event Y
j.
To compute Pr(Y > 0|X) from the CR model, one only needs to take
one minus Pr(Y = 0|X). To compute other unconditional probabilities from
the CR model, one must multiply the conditional probabilities. For example,
Pr(Y > 1|X) = Pr(Y > 1|X, Y
1) ⇥ Pr(Y
1|X) = [1 Pr(Y = 1|Y
1, X)][1 Pr(Y = 0|X)] = [1 1/(1 + exp[ (✓1 + X )])][1 1/(1 + exp[ (✓0 +
X )])].

13.4 Continuation Ratio Model

327

13.4.3 Estimation
Armstrong and Sloan31 and Berridge and Whitehead51 showed how the CR
model can be fitted using an ordinary binary logistic model likelihood function, after certain rows of the X matrix are duplicated and a new binary Y
vector is constructed. For each subject, one constructs separate records by
considering successive conditions Y
0, Y
1, . . . , Y
k 1 for a response
variable with values 0, 1, . . . , k. The binary response for each applicable condition or “cohort” is set to 1 if the subject failed at the current “cohort” or
“risk set,” that is, if Y = j where the cohort being considered is Y
j. The
constructed cohort variable is carried along with the new X and Y . This variable is considered to be categorical and its coefficients are fitted by adding
k 1 dummy variables to the binary logistic model. For ease of computation,
the CR model is restated as follows, with the first cohort used as the reference
cell.
1
Pr(Y = j|Y
j, X) =
.
(13.11)
1 + exp[ (↵ + ✓j + X )]
Here ↵ is an overall intercept, ✓0 ⌘ 0, and ✓1 , . . . , ✓k
↵.

1

are increments from

13.4.4 Residuals
To check CR model assumptions, binary logistic model partial residuals are
again valuable. We separately fit a sequence of binary logistic models using a
series of binary events and the corresponding applicable (increasingly small)
subsets of subjects, and plot smoothed partial residuals against X for all of
the binary events. Parallelism in these plots indicates that the CR model’s
constant assumptions are satisfied.

13.4.5 Assessment of Model Fit
The partial residual plots just described are very useful for checking the
constant slope assumption of the CR model. The next section shows how to
test this assumption formally. Linearity can be assessed visually using the
smoothed partial residual plot, and interactions between predictors can be
tested as usual.

328

13 Ordinal Logistic Regression

13.4.6 Extended CR Model

5

The PO model has been extended by Peterson and Harrell495 to allow for
unequal slopes for some or all of the Xs for some or all levels of Y . This partial
PO model requires specialized software. The CR model can be extended more
easily. In R notation, the ordinary CR model is specified as
y ⇠ c o h o r t + X1 + X2 + X3 + . . .

with cohort denoting a polytomous variable. The CR model can be extended
to allow for some or all of the s to change with the cohort or Y -cuto↵.31
Suppose that non-constant slope is allowed for X1 and X2. The R notation for
the extended model would be
y ⇠ c o h o r t ⇤ (X1 + X2) + X3

The extended CR model is a discrete version of the Cox survival model with
time-dependent covariables.
There is nothing about the CR model that makes it fit a given dataset
better than other ordinal models such as the PO model. The real benefit of
the CR model is that using standard binary logistic model software one can
flexibly specify how the equal-slopes assumption can be relaxed.

13.4.7 Role of Penalization in Extended CR Model
As demonstrated in the upcoming case study, penalized MLE is invaluable in
allowing the model to be extended into an unequal-slopes model insofar as the
information content in the data will support. Faraway182 has demonstrated
how all data-driven steps of the modeling process increase the real variance in
“final” parameter estimates, when one estimates variances without assuming
that the final model was prespecified. For ordinal regression modeling, the
most important modeling steps are (1) choice of predictor variables, (2) selecting or modeling predictor transformations, and (3) allowance for unequal
slopes across Y -cuto↵s (i.e., non-PO or non-CR). Regarding Steps (2) and (3)
one is tempted to rely on graphical methods such as residual plots to make
detours in the strategy, but it is very difficult to estimate variances or to
properly penalize assessments of predictive accuracy for subjective modeling
decisions. Regarding (1), shrinkage has been proven to work better than stepwise variable selection when one is attempting to build a main-e↵ects model.
Choosing a shrinkage factor is a well-defined, smooth, and often a unique process as opposed to binary decisions on whether variables are “in” or “out”
of the model. Likewise, instead of using arbitrary subjective (residual plots)
or objective ( 2 due to cohort ⇥ covariable interactions, i.e., non-constant
covariable e↵ects), shrinkage can systematically allow model enhancements
insofar as the information content in the data will support, through the use of

13.4 Continuation Ratio Model

329

di↵erential penalization. Shrinkage is a solution to the dilemma faced when
the analyst attempts to choose between a parsimonious model and a more
complex one that fits the data. Penalization does not require the analyst to
make a binary decision, and it is a process that can be validated using the
bootstrap.

13.4.8 Validating the Fitted Model
Validation of statistical indexes such as Dxy and model calibration is done
using techniques discussed previously, except that certain problems must be
addressed. First, when using the bootstrap, the resampling must take into
account the existence of multiple records per subject that were created to use
the binary logistic likelihood trick. That is, sampling should be done with replacement from subjects rather than records. Second, the analyst must isolate
which event to predict. This is because when observations are expanded in
order to use a binary logistic likelihood function to fit the CR model, several
di↵erent events are being predicted simultaneously. Somers’ Dxy could be
computed by relating X ˆ (ignoring intercepts) to the ordinal Y , but other
indexes are not defined so easily. The simplest approach here would be to
validate a single prediction for Pr(Y = j|Y
j, X), for example. The simplest event to predict is Pr(Y = 0|X), as this would just require subsetting
on all observations in the first cohort level in the validation sample. It would
also be easy to validate any one of the later conditional probabilities. The
validation functions described in the next section allow for such subsetting,
as well as handling the cluster sampling. Specialized calculations would be
needed to validate an unconditional probability such as Pr(Y
2|X).

13.4.9 R Functions
The cr.setup function in rms returns a list of vectors useful in constructing
a dataset used to trick a binary logistic function such as lrm into fitting
CR models. The subs vector in this list contains observation numbers in the
original data, some of which are repeated. Here is an example.
# Y=original ordinal response
# mydata is the original dataset
# mydata[i,] subscripts input data,
# using duplicate values of i for
# repeats
y
u$y
# constructed binary responses
cohort
u$ c o h o r t
# cohort or risk set categories
f
lrm ( y ⇠ c o h o r t ⇤ age + s e x )

u
c r . s e t u p (Y)
a t t a c h ( mydata [ u$ subs , ] )

330

6

13 Ordinal Logistic Regression

Since the lrm and pentrace functions have the capability to penalize different parts of the model by di↵erent amounts, they are valuable for fitting
extended CR models in which the cohort ⇥ predictor interactions are allowed
to be only as important as the information content in the data will support.
Simple main e↵ects can be unpenalized or slightly penalized as desired.
The validate and calibrate functions for lrm allow specification of subject identifiers when using the bootstrap, so the samples can be constructed
with replacement from the original subjects. In other words, cluster sampling is done from the expanded records. This is handled internally by the
predab.resample function. These functions also allow one to specify a subset of the records to use in the validation, which makes it especially easy to
validate the part of the model used to predict Pr(Y = 0|X).
The plot.xmean.ordinaly function is useful for checking the CR assumption for single predictors, as described earlier.

13.5 Further Reading
1

2

3
4

See5, 25, 26, 31, 32, 51, 62, 63, 110, 123, 235, 240, 272, 347, 442, 495, 556, 658, 674 for some
excellent background references, applications, and extensions to the ordinal
models.657 and421 demonstrate how to model ordinal outcomes with repeated
measurements within subject using random e↵ects in Bayesian models. The first
to develop an ordinal regression model were Aitchison and Silvey8 .
Some analysts feel that combining categories improves the performance of test
statistics when fitting PO models when sample sizes are small and cells are
sparse. Murad et al.462 demonstrated that this causes more problems, because
it results in overly conservative Wald tests.
Anderson and Philips [26, p. 29] proposed methods for constructing properly
spaced response values given a fitted PO model.
The simplest demonstration of this is to consider a model in which there is a
single predictor that is totally independent of a nine-level response Y , so PO
must hold. A PO model is fitted in SAS using:
DATA test;
DO i=1 to 50;
y=FLOOR(RANUNI(151)*9);
x=RANNOR(5);
OUTPUT;
END;
PROC LOGISTIC; MODEL y=x;

5
6

The score test for PO was 2 = 56 on 7 d.f., P < 0.0001. This problem results
from some small cell sizes in the distribution of Y .495 The P -value for testing
the regression e↵ect for X was 0.76.
The R glmnetcr package by Kellie Archer provides a di↵erent way to fit continuation ratio models.
Bender and Benner47 have some examples using the precursor of the rms package for fitting and assessing the goodness of fit of ordinal logistic regression
models.

13.6 Problems

331

13.6 Problems
Test for the association between disease group and total hospital cost in
SUPPORT, without imputing any missing costs (exclude the one patient
having zero cost).
1. Use the Kruskal–Wallis rank test.
2. Use the proportional odds ordinal logistic model generalization of the
Wilcoxon–Mann–Whitney Kruskal–Wallis Spearman test. Group total
cost into 20 quantile groups so that only 19 intercepts will need to be
in the model, not one less than the number of subjects (this would have
taken the program too long to fit the model). Use the likelihood ratio 2
for this and later steps.
3. Use a binary logistic model to test for association between disease group
and whether total cost exceeds the median of total cost. In other words,
group total cost into two quantile groups and use this binary variable as
the response. What is wrong with this approach?
4. Instead of using only two cost groups, group cost into 3, 4, 5, 6, 8, 10,
and 12 quantile groups. Describe the relationship between the number of
intervals used to approximate the continuous response variable and the
efficiency of the analysis. How many intervals of total cost, assuming that
the ordering of the di↵erent intervals is used in the analysis, are required
to avoid losing significant information in this continuous variable?
5. If you were selecting one of the rank-based tests for testing the association
between disease and cost, which of any of the tests considered would you
choose?
6. Why do all of the tests you did have the same number of degrees of freedom
for the hypothesis of no association between dzgroup and totcst?
7. What is the advantage of a rank-based test over a parametric test based
on log(cost)?
8. Show that for a two-sample problem, the numerator of the score test for
comparing the two groups using a proportional odds model is exactly the
numerator of the Wilcoxon-Mann-Whitney two-sample rank-sum test.

Chapter 14

Case Study in Ordinal Regression, Data
Reduction, and Penalization

This case study is taken from Harrell et al.267 which described a World
Health Organization study432 in which vital signs and a large number of
clinical signs and symptoms were used to develop a predictive model for an
ordinal response. This response consists of laboratory assessments of diagnosis and severity of illness related to pneumonia, meningitis, and sepsis.
Much of the modeling strategy given in Chapter 4 was used to develop the
model, with additional emphasis on penalized maximum likelihood estimation (Section 9.10). The following laboratory data are used in the response:
cerebrospinal fluid (CSF) culture from a lumbar puncture (LP), blood culture (BC), arterial oxygen saturation (SaO2 , a measure of lung dysfunction),
and chest X-ray (CXR). The sample consisted of 4552 infants aged 90 days
or less.
This case study covers these topics:
1. definition of the ordinal response (Section 14.1);
2. scoring and clustering of clinical signs (Section 14.2);
3. testing adequacy of weights specified by subject-matter specialists and
assessing the utility of various scoring schemes using a tentative ordinal
logistic model (Section 14.3);
4. assessing the basic ordinality assumptions and examining the proportional
odds and continuation ratio (PO and CR) assumptions separately for each
predictor (Section 14.4);
5. deriving a tentative PO model using cluster scores and regression splines
(Section 14.5);
6. using residual plots to check PO, CR, and linearity assumptions (Section 14.6);
7. examining the fit of a CR model (Section 14.7);
8. utilizing an extended CR model to allow some or all of the regression
coefficients to vary with cuto↵s of the response level as well as to provide
formal tests of constant slopes (Section 14.8);
9. using penalized maximum likelihood estimation to improve accuracy (Section 14.9);
333

334

14 Ordinal Regression, Data Reduction, and Penalization

Table 14.1 Ordinal Outcome Scale
Outcome
Level
Y
0
1
2
a
b

Definition

n

Fraction in Outcome Level
BC, CXR
Not
Random
Indicated
Indicated
Sample
(n = 2398) (n = 1979) (n = 175)
None of the below
3551
0.63
0.96
0.91
90%  SaO2 < 95% or CXR+
490
0.17
0.04a
0.05
BC+ or CSF+ or SaO2 < 90% 511
0.21
0.00b
0.03

SaO2 was measured but CXR was not done
Assumed zero since neither BC nor LP were done.

10. approximating the full model by a sub-model and drawing a nomogram
on the basis of the sub-model (Section 14.10); and
11. validating the ordinal model using the bootstrap (Section 14.11).

14.1 Response Variable
To be a candidate for BC and CXR, an infant had to have a clinical indication
for one of the three diseases, according to prespecified criteria in the study
protocol (n = 2398). Blood work-up (but not necessarily LP) and CXR was
also done on a random sample intended to be 10% of infants having no signs
or symptoms suggestive of infection (n = 175). Infants with signs suggestive
of meningitis had LP done. All 4552 infants received a full physical exam and
standardized pulse oximetry to measure SaO2 . The vast majority of infants
getting CXR had the X-rays interpreted by three independent radiologists.
The analyses that follow are not corrected for verification bias683 with
respect to BC, LP, and CXR, but Section 14.1 has some data describing the
extent of the problem, and the problem is reduced by conditioning on a large
number of covariates.
Patients were assigned to the worst qualifying outcome category. Table 14.1 shows the definition of the ordinal outcome variable Y and shows
the distribution of Y by the lab work-up strategy.
The e↵ect of verification bias is a false negative fraction of 0.03 for Y =
2, from comparing the detection fraction of zero for Y = 2 in the “Not
Indicated” group with the observed positive fraction of 0.03 in the random
sample that was fully worked up. The extent of verification bias in Y = 1 is
0.05 0.04 = 0.01. These biases are ignored in this analysis.

14.3 Developing Cluster Summary Scores

335

14.2 Variable Clustering
Forty-seven clinical signs were collected for each infant. Most questionnaire
items were scored as a single variable using equally spaced codes, with 0 to
3 representing, for example, sign not present, mild, moderate, severe. The
resulting list of clinical signs with their abbreviations is given in Table 14.2.
The signs are organized into clusters as discussed later.
Here, hx stands for history, ausc for auscultation, and hxprob for history
of problems. Two signs (qcr, hcm) were listed twice since they were later
placed into two clusters each.
Next, hierarchical clustering was done using the matrix of squared Spearman rank correlation coefficients as the similarity matrix. The varclus R
function was used as follows.
r e q u i r e ( rms )
getHdata ( a r i ) # defines ari,
vclust
v a r c l u s (⇠ i l l d + h l t + slpm
h f b + h f e + hap
f d e + c h i + twb
s t r + gru + coh
csa + aro + qcr
absu + s t u + deh
whz + hdb + smi2
data=a r i )
p l o t ( v c l u s t ) # Figure 14.1

Sc, Y, Y.death
+
+
+
+
+
+
+

slpl
hcl
ldy
ccy
con
dcp
abd

+
+
+
+
+
+
+

wake
hcm
apn
jau
att
crs
conj

+
+
+
+
+
+
+

convul
hcs
lcw
omph
mvm
abb
oto

+
+
+
+
+
+
+

hfa +
hdi +
nfl +
csd +
afe +
abk +
puskin ,

The output appears in Figure 14.1. This output served as a starting point
for clinicians to use in constructing more meaningful clinical clusters. The
clusters in Table 14.2 were the consensus of the clinicians who were the investigators in the WHO study. Prior subject matter knowledge plays a key
role at this stage in the analysis.

14.3 Developing Cluster Summary Scores
The clusters listed in Table 14.2 were first scored by the first principal component of transcan-transformed signs, denoted by P C1 . Knowing that the
resulting weights may be too complex for clinical use, the primary reasons for
analyzing the principal components were to see if some of the clusters could
be removed from consideration so that the clinicians would not spend time
developing scoring rules for them. Let us “peek” at Y to assist in scoring
clusters at this point, but to do so in a very structured way that does not
involve the examination of a large number of individual coefficients.
To judge any cluster scoring scheme, we must pick a tentative outcome
model. For this purpose we chose the PO model. By using the 14 P C1 s

336

14 Ordinal Regression, Data Reduction, and Penalization

Table 14.2 Clinical Signs
Cluster Name
bul.conv
hydration

drowsy

agitated

crying

re↵ort

stop.breath
ausc
hxprob
feeding

labor

abdominal
fever.ill
pustular

Sign
Abbreviation
abb
convul
abk
hdi
deh
stu
dcp
hcl
qcr
csd
slpm
wake
aro
mvm
hcm
slpl
con
csa
hcm
hcs
qcr
smi2
nfl
lcw
gru
ccy
hap
apn
whz
coh
crs
hfb
hdb
hlt
hfa
absu
afe
chi
fde
ldy
twb
adb
jau
omph
illd
hfe
conj
oto
puskin

Name
Values
of Sign
bulging fontanel
0-1
hx convulsion
0-1
sunken fontanel
0-1
hx diarrhoea
0-1
dehydrated
0-2
skin turgor
0-2
digital capillary refill
0-2
less activity
0-1
quality of crying
0-2
drowsy state
0-2
sleeping more
0-1
wakes less easily
0-1
arousal
0-2
amount of movement
0-2
crying more
0-1
sleeping less
0-1
consolability
0-2
agitated state
0-1
crying more
0-1
crying less
0-1
quality of crying
0-2
smiling ability ⇥ age > 42 days
0-2
nasal flaring
0-3
lower chest in-drawing
0-3
grunting
0-2
central cyanosis
0-1
hx stop breathing
0-1
apnea
0-1
wheezing
0-1
cough heard
0-1
crepitation
0-2
fast breathing
0-1
difficulty breathing
0-1
mother report resp. problems
none, chest, other
hx abnormal feeding
0-3
sucking ability
0-2
drinking ability
0-2
previous child died
0-1
fever at delivery
0-1
days in labor
1-9
water broke
0-1
abdominal distension
0-4
jaundice
0-1
omphalitis
0-1
age-adjusted no. days ill
hx fever
0-1
conjunctivitis
0-1
otoscopy impression
0-2
pustular skin rash
0-1

14.3 Developing Cluster Summary Scores

337

Spearman ρ2

0.2
0.4
0.6
0.8
1.0

twb
ldy
fde
chi
oto
att
jau
omph
illd
smi2
str
hltHy respir/chest
hfb
crs
lcw
nfl
coh
whz
hltHy respir/no chest
hdb
puskin
conj
hfe
slpl
hcm
hap
apn
ccy
dcp
aro
csd
qcr
mvm
hcs
hcl
hfa
afe
absu
slpm
wake
hdi
abk
stu
deh
convul
abb
abd
gru
csa
con

0.0

Fig. 14.1 Hierarchical variable clustering using Spearman ⇢2 as a similarity measure
for all pairs of variables. Note that since the hlt variable was nominal, it is represented
by two dummy variables here.

corresponding to the 14 clusters, the fitted PO model had a likelihood ratio
(LR) 2 of 1155 with 14 d.f., and the predictive discrimination of the clusters
was quantified by a Somers’ Dxy rank correlation between X ˆ and Y of
0.596. The following clusters were not statistically important predictors and
we assumed that the lack of importance of the P C1 s in predicting Y (adjusted
for the other P C1 s) justified a conclusion that no sign within that cluster
was clinically important in predicting Y : hydration, hxprob, pustular,
crying, fever.ill, stop.breath, labor. This list was identified using a
backward step-down procedure on the full model. The total Wald 2 for these
seven P C1 s was 22.4 (P = 0.002). The reduced model had LR 2 = 1133
with 7 d.f., Dxy = 0.591. The bootstrap validation in Section 14.11 penalizes
for examining all candidate predictors.
The clinicians were asked to rank the clinical severity of signs within each
potentially important cluster. During this step, the clinicians also ranked
severity levels of some of the component signs, and some cluster scores
were simplified, especially when the signs within a cluster occurred infrequently. The clinicians also assessed whether the severity points or weights
should be equally spaced, assigning unequally spaced weights for one cluster
(agitated). The resulting rankings and sign combinations are shown in Table 14.3. The signs or sign combinations separated by a comma are treated
as separate categories, whereas some signs were unioned (“or”–ed) when the
clinicians deemed them equally important. As an example, if an additive clus-

338

14 Ordinal Regression, Data Reduction, and Penalization

Table 14.3 Clinician Combinations, Rankings, and Scorings of Signs
Cluster
bul.conv
drowsy
agitated
re↵ort
ausc
feeding
abdominal

Combined/Ranked Signs in Order of Severity
Weights
abb [ convul
0–1
hcl, qcr>0, csd>0 [ slpm [ wake, aro>0, mvm>0
0–5
hcm, slpl, con=1, csa, con=2
0, 1, 2, 7, 8, 10
nfl>0, lcw>1, gru=1, gru=2, ccy
0–5
whz, coh, crs>0
0–3
hfa=1, hfa=2, hfa=3, absu=1 [ afe=1, absu=2 [ afe=2
0–5
jau [ abd>0 [ omph
0–1

ter score was to be used for drowsy, the scorings would be 0 = none present,
1 = hcl, 2 = qcr>0, 3 = csd>0 or slpm or wake, 4 = aro>0, 5 = mvm>0 and
the scores would be added.
This table reflects some data reduction already (unioning some signs and
selection of levels of ordinal signs) but more reduction is needed. Even after
signs are ranked within a cluster, there are various ways of assigning the cluster scores. We investigated six methods. We started with the purely statistical
approach of using P C1 to summarize each cluster. Second, all sign combinations within a cluster were unioned to represent a 0/1 cluster score. Third,
only sign combinations thought by the clinicians to be severe were unioned,
resulting in drowsy=aro>0 or mvm>0, agitated=csa or con=2, reffort=lcw>1
or gru>0 or ccy, ausc=crs>0, and feeding=absu>0 or afe>0. For clusters
that are not scored 0/1 in Table 14.3, the fourth summarization method was
a hierarchical one that used the weight of the worst applicable category as
the cluster score. For example, if aro=1 but mvm=0, drowsy would be scored as
4. The fifth method counted the number of positive signs in the cluster. The
sixth method summed the weights of all signs or sign combinations present.
Finally, the worst sign combination present was again used as in the second method, but the points assigned to the category were data-driven ones
obtained by using extra dummy variables. This provided an assessment of
the adequacy of the clinician-specified weights. By comparing rows 4 and 7
in Table 14.4 we see that response data-driven sign weights have a slightly
worse AIC, indicating that the number of extra parameters estimated was
not justified by the improvement in 2 . The hierarchical method, using the
clinicians’ weights, performed quite well. The only cluster with inadequate
clinician weights was ausc—see below. The P C1 method, without any guidance, performed well, as in263 . The only reasons not to use it are that it
requires a coefficient for every sign in the cluster and the coefficients are not
translatable into simple scores such as 0, 1, . . ..
Representation of clusters by a simple union of selected signs or of all signs
is inadequate, but otherwise the choice of methods is not very important in
terms of explaining variation in Y . We chose the fourth method, a hierarchical severity point assignment (using weights that were prespecified by the
clinicians), for its ease of use and of handling missing component variables

14.4 Assessing Ordinality of Y for each X, and Unadjusted Checking of PO and CR Assumptions
339
Table 14.4 Predictive information of various cluster scoring strategies. AIC is on
the likelihood ratio 2 scale.

Scoring Method
P C1 of each cluster
Union of all signs
Union of higher categories
Hierarchical (worst sign)
Additive, equal weights
Additive using clinician weights
Hierarchical, data-driven weights

LR 2
1133
1045
1123
1194
1155
1183
1227

d.f.
7
7
7
7
7
7
25

AIC
1119
1031
1109
1180
1141
1169
1177

(in most cases) and potential for speeding up the clinical exam (examining
to detect more important signs first). Because of what was learned regarding the relationship between ausc and Y , we modified the ausc cluster score
by redefining it as ausc=crs>0 (crepitations present). Note that neither the
“tweaking” of ausc nor the examination of the seven scoring methods displayed in Table 14.4 is taken into account in the model validation.

14.4 Assessing Ordinality of Y for each X, and
Unadjusted Checking of PO and CR Assumptions
Section 13.2 described a graphical method for assessing the ordinality assumption for Y separately with respect to each X, and for assessing PO
and CR assumptions individually. Figure 14.2 is an example of such displays.
For this dataset we expect strongly nonlinear e↵ects for temp, rr, and hrat,
so for those predictors we plot the mean absolute di↵erences from suitable
“normal” values as an approximate solution.
Sc

t r a n s f o r m ( Sc ,
a u s c = 1 ⇤ ( a u s c == 3 ) ,
b u l . c o n v = 1 ⇤ ( b u l . c o n v == ’TRUE ’ ) ,
abdominal = 1 ⇤ ( abdominal == ’TRUE ’ ) )
p l o t . x m e a n . o r d i n a l y (Y ⇠ age + abs ( temp 37 ) + abs ( rr 60 ) +
abs ( hrat 125 ) + waz + b u l . c o n v + drowsy +
a g i t a t e d + r e f f o r t + ausc + f e e d i n g +
abdominal , data=Sc , c r=TRUE,
subn=FALSE, c e x . p o i n t s=. 6 5 ) # Figure 14.2

The plot is shown in Figure 14.2. Y does not seem to operate in an ordinal
fashion with respect to age, |rr 60|, or ausc. For the other variables, ordinality holds, and PO holds reasonably well for the other variables. For heart
rate, the PO assumption appears to be satisfied perfectly. CR model assumptions appear to be more tenuous than PO assumptions, when one variable at
a time is fitted.

C
●

2

0

●

●

1

Y

2

40
36
32
3.5

●

C
●

0

1

2

Y

●

C
C
●

1

2

C
●

2.5

agitated
2

C
●

0

1

C

1.5
1

2.5

C

C

0

0

Y

C

C

abs(hrat − 125)

14.0
2.0

●
C

1.5

●

●

0.25

ausc

●

Y

2

feeding

0.40

C

1

●

C

Y

0.10

reffort

0.5 1.0 1.5 2.0

Y

0

drowsy

●

C

1

C
●

Y
C
●

●
C

0

2

C

2

C
●

0.24

0.16

2

1

C
●

1.0

●

C

1

●

0

●

Y

0.10

bul.conv

●

0.04

−0.6

C

0

2

Y

●
C

−1.0

waz

−0.2

Y

1

C
●

0.18

0

13.0

1.0
C
●

C

abdominal

2

0.5

1

C

0.12

●

0

abs(rr − 60)

C

●

C

●

●

C

12.0

39

C

C
●

0.8

●

0.6

C
●

abs(temp − 37)

14 Ordinal Regression, Data Reduction, and Penalization

37

age

41

340

Y

●

C
●
C

0

1

Y

Fig. 14.2 Examination of the ordinality of Y for each predictor by assessing how
varying Y relate to the mean X, and whether the trend is monotonic. Solid lines
connect the simple stratified means, and dashed lines connect the estimated expected
value of X|Y = j given that PO holds. Estimated expected values from the CR model
are marked with Cs.

14.5 A Tentative Full Proportional Odds Model
Based on what was determined in Section 14.3, the original list of 47 signs
was reduced to seven predictors: two unions of signs (bul.conv, abdominal),
one single sign (ausc), and four “worst category” point assignments (drowsy,
agitated, reffort, feeding). Seven clusters were dropped for the time
being because of weak associations with Y . Such a limited use of variable
selection reduces the severe problems inherent with that technique.
At this point in model development add to the model age and vital signs:
temp (temperature), rr (respiratory rate), hrat (heart rate), and waz, weightfor-age Z-score. Since age was expected to modify the interpretation of temp,
rr, and hrat, and interactions between continuous variables would be difficult

2

14.5 A Tentative Full Proportional Odds Model

341

to use in the field, we categorized age into three intervals: 0–6 days (n = 302),
7–59 days (n = 3042), and 60–90 days (n = 1208).a
Sc $ ageg

c u t 2 ( Sc $ age , c ( 7 , 6 0 ) )

The new variables temp, rr, hrat, waz were missing in, respectively,
n = 13, 11, 147, and 20 infants. Since the three vital sign variables are
somewhat correlated with each other, customized single imputation models
were developed to impute all the missing values without assuming linearity
or even monotonicity of any of the regressions.
vsign.trans

t r a n s c a n (⇠ temp + h r a t + r r , data=Sc ,
imputed=TRUE, p l=FALSE)

Convergence criterion:2.222 0.643 0.191 0.056 0.016
Convergence in 6 iterations
R2 achieved in predicting each variable:
temp hrat
rr
0.168 0.160 0.066
Adjusted R2:
temp hrat
rr
0.167 0.159 0.064
Sc

t r a n s f o r m ( Sc ,
temp = impute ( v s i g n . t r a n s , temp ) ,
h r a t = impute ( v s i g n . t r a n s , h r a t ) ,
rr
= impute ( v s i g n . t r a n s , r r ) )

After transcan estimated optimal restricted cubic spline transformations,
temp could be predicted with adjusted R2 = 0.17 from hrat and rr, hrat
could be predicted with adjusted R2 = 0.16 from temp and rr, and rr could
be predicted with adjusted R2 of only 0.06. The first two R2 , while not
large, mean that customized imputations are more efficient than imputing
with constants. Imputations on rr were closer to the median rr of 48/minute
as compared with the other two vital signs whose imputations have more
variation. In a similar manner, waz was imputed using age, birth weight, head
circumference, body length, and prematurity (adjusted R2 for predicting waz
from the others was 0.74). The continuous predictors temp, hrat, rr were
not assumed to linearly relate to the log odds that Y
j. Restricted cubic
spline functions with five knots for temp,rr and four knots for hrat,waz were
used to model the e↵ects of these variables:
f1

a

lrm (Y ⇠ ageg ⇤ ( r c s ( temp ,5)+ r c s ( r r ,5)+ r c s ( h r a t , 4 ) ) +
r c s ( waz , 4 ) + b u l . c o n v + drowsy + a g i t a t e d +
r e f f o r t + a u s c + f e e d i n g + abdominal ,
data=Sc , x=TRUE, y=TRUE)

These age intervals were also found to adequately capture most of the interaction
e↵ects.

342

14 Ordinal Regression, Data Reduction, and Penalization

# x=TRUE, y=TRUE used by resid() below
p r i n t ( f 1 , l a t e x=TRUE, c o e f s =5)

Logistic Regression Model
lrm(formula = Y ˜ ageg * (rcs(temp, 5) + rcs(rr, 5) + rcs(hrat,
4)) + rcs(waz, 4) + bul.conv + drowsy + agitated + reffort +
ausc + feeding + abdominal, data = Sc, x = TRUE, y = TRUE)
Model Likelihood
Ratio Test
Obs
4552 LR 2
1393.18
0
3551 d.f.
45
1
490 Pr(> 2 ) < 0.0001
2
511
L
6
max | @ log
|
2⇥10
@

Discrimination
Indexes
2
R
0.355
g
1.485
gr
4.414
gp
0.225
Brier
0.120

Rank Discrim.
Indexes
C
0.826
Dxy
0.653
0.654
⌧a
0.240

Coef
S.E. Wald Z Pr(> |Z|)
y 1
0.0653 7.6563
0.01
0.9932
y 2
-1.0646 7.6563
-0.14
0.8894
ageg=[ 7,60)
9.5590 9.9071
0.96
0.3346
ageg=[60,90] 29.1376 15.8915
1.83
0.0667
temp
-0.0694 0.2160
-0.32
0.7480
...
Wald tests of nonlinearity and interaction are shown in Table 14.6.
l a t e x ( anova ( f 1 ) , f i l e = ’ ’ , l a b e l= ’ o r d i n a l a n o v a . f 1 ’ ,
c a p t i o n= ’ Wald s t a t i s t i c s from t h e p r o p o r t i o n a l odds model ’ ,
s i z e= ’ s m a l l e r ’ )
# Table 14.6

The bottom four lines of the table are the most important. First, there
is strong evidence that some associations with Y exist (45 d.f. test) and
very strong evidence of nonlinearity in one of the vital signs or in waz (26 d.f.
test). There is moderately strong evidence for an interaction e↵ect somewhere
in the model (22 d.f. test). We see that the grouped age variable ageg is
predictive of Y , but mainly as an e↵ect modifier for rr, and hrat. temp is
extremely nonlinear, and rr is moderately so. hrat, a difficult variable to
measure reliably in young infants, is perhaps not important enough ( 2 =
19, 9 d.f.) to keep in the final model.

14.6 Residual Plots

343

Table 14.6 Wald statistics from the proportional odds model
2
d.f.
P
ageg (Factor+Higher Order Factors)
41.49 24 0.0147
All Interactions
40.48 22 0.0095
temp (Factor+Higher Order Factors)
37.08 12 0.0002
All Interactions
6.77 8 0.5617
Nonlinear (Factor+Higher Order Factors)
31.08 9 0.0003
rr (Factor+Higher Order Factors)
81.16 12 < 0.0001
All Interactions
27.37 8 0.0006
Nonlinear (Factor+Higher Order Factors)
27.36 9 0.0012
hrat (Factor+Higher Order Factors)
19.00 9 0.0252
All Interactions
8.83 6 0.1836
Nonlinear (Factor+Higher Order Factors)
7.35 6 0.2901
waz
35.82 3 < 0.0001
Nonlinear
13.21 2 0.0014
bul.conv
12.16 1 0.0005
drowsy
17.79 1 < 0.0001
agitated
8.25 1 0.0041
re↵ort
63.39 1 < 0.0001
ausc
105.82 1 < 0.0001
feeding
30.38 1 < 0.0001
abdominal
0.74 1 0.3895
ageg ⇥ temp (Factor+Higher Order Factors)
6.77 8 0.5617
Nonlinear
6.40 6 0.3801
Nonlinear Interaction : f(A,B) vs. AB
6.40 6 0.3801
ageg ⇥ rr (Factor+Higher Order Factors)
27.37 8 0.0006
Nonlinear
14.85 6 0.0214
Nonlinear Interaction : f(A,B) vs. AB
14.85 6 0.0214
ageg ⇥ hrat (Factor+Higher Order Factors)
8.83 6 0.1836
Nonlinear
2.42 4 0.6587
Nonlinear Interaction : f(A,B) vs. AB
2.42 4 0.6587
TOTAL NONLINEAR
78.20 26 < 0.0001
TOTAL INTERACTION
40.48 22 0.0095
TOTAL NONLINEAR + INTERACTION
96.31 32 < 0.0001
TOTAL
1073.78 45 < 0.0001

14.6 Residual Plots
Section 13.3.4 defined binary logistic score residuals for isolating the PO
assumption in an ordinal model. For the tentative PO model, score residuals
for four of the variables were plotted using
r e s i d ( f 1 , ’ s c o r e . b i n a r y ’ , p l=TRUE, which=c ( 1 7 , 1 8 , 2 0 , 2 1 ) )
## Figure 14.3

The result is shown in Figure 14.3. We see strong evidence of non-PO for
ausc and moderate evidence for drowsy and bul.conv, in agreement with
Figure 14.2.
Partial residuals computed separately for each Y -cuto↵ (Section 13.3.4) are
the most useful residuals for ordinal models as they simultaneously check lin-

−0.004

●

1

0.02

drowsy

●

−0.02

0.002

14 Ordinal Regression, Data Reduction, and Penalization

bul.conv

344

2

●

●

1

2

Y

0.005

ausc

●

−0.010

reffort

−0.02 0.00

●

1

2

Y

0.02

Y
●

●

1

2

Y

Fig. 14.3 Binary logistic model score residuals for binary events derived from two
cuto↵s of the ordinal response Y . Note that the mean residuals, marked with closed
circles, correspond closely to di↵erences between solid and dashed lines at Y = 1, 2
in Figure 14.2. Score residual assessments for spline-expanded variables such as rr
would have required one plot per d.f.

earity, find needed transformations, and check PO. In Figure 14.4, smoothed
partial residual plots were obtained for all predictors, after first fitting a simple model in which every predictor was assumed to operate linearly. Interactions were temporarily ignored and age was used as a continuous variable.
lrm (Y ⇠ age + temp + r r + h r a t + waz +
b u l . c o n v + drowsy + a g i t a t e d + r e f f o r t + a u s c +
f e e d i n g + abdominal , data=Sc , x=TRUE, y=TRUE)
r e s i d ( f 2 , ’ p a r t i a l ’ , p l=TRUE, l a b e l . c u r v e s=FALSE) # Figure 14.4

f2

The degree of non-parallelism generally agreed with the degree of non-flatness
in Figure 14.3 and with the other score residual plots that were not shown.
The partial residuals show that temp is highly nonlinear and that it is much
more useful in predicting Y = 2. For the cluster scores, the linearity assumption appears reasonable, except possibly for drowsy. Other nonlinear
e↵ects are taken into account using splines as before (except for age, which
is categorized).
A model can have significant lack of fit with respect to some of the predictors and still yield quite accurate predictions. To see if that is the case for this
PO model, we computed predicted probabilities of Y = 2 for all infants from
the model and compared these with predictions from a customized binary

3

4

5

0.4

ausc

3.0
2.0
1.0
0.0

1.5

2.5

Partial Residual

150

1

0.8

2

3

4

0.8
0.4
0.0
−0.4

Partial Residual

0.8
0.4
0.0
0

5

0

2

3

4

6

8

4

0.4
0.2
0.0

0.5

1

2

agitated

1.0

drowsy

0

250

hrat

0.0

1.0
0.5
0.0

50

−0.2

0.8

0.0

Partial Residual

1.5
0.5

2

reffort

0.4

120

Partial Residual

0.0

bul.conv

−0.5

Partial Residual

1

80

−0.4

0.4 0.8 1.2
−0.2

4

waz

0

40

rr
Partial Residual

2

Partial Residual

1.0
−0.5

0

0

temp

−2.0

Partial Residual

−4

345

0.5

4.0 5.0 6.0 7.0
32 34 36 38 40

age

Partial Residual

20 40 60 80

Partial Residual

0

Partial Residual

0.4
0.0
−0.6

Partial Residual

14.7 Graphical Assessment of Fit of CR Model

5

0.0

feeding

0.4

0.8

abdominal

Fig. 14.4 Smoothed partial residuals corresponding to two cuto↵s of Y , from a model
in which all predictors were assumed to operate linearly and additively. The smoothed
curves estimate the actual predictor transformations needed, and parallelism relates
to the PO assumption. Solid lines denote Y
1 while dashed lines denote Y
2.

logistic model derived to predict Pr(Y = 2). The mean absolute di↵erence
in predicted probabilities between the two models is only 0.02, but the 0.90
quantile of that di↵erence is 0.059. For high-risk infants, discrepancies of 0.2
were common. Therefore we elected to consider a di↵erent model.

14.7 Graphical Assessment of Fit of CR Model
In order to take a first look at the fit of a CR model, let us consider the
two binary events that need to be predicted, and assess linearity and parallelism over Y -cuto↵s. Here we fit a sequence of binary fits and then use the
plot.lrm.partial function, which assembles partial residuals for a sequence
of fits and constructs one graph per predictor.
cr0

lrm (Y==0 ⇠ age + temp + r r + h r a t + waz +

10

346

14 Ordinal Regression, Data Reduction, and Penalization

3

reffort

4

5

1

ausc

0.8

2

3

4

0.0
−1.0
−2.0
0.2
−0.1
−0.4

5

cr1

0

0.0
−0.4
0

1

2

3

2

4

6

8

10

agitated

−0.8

−0.4 0.0

cr0

0.4

Partial Residual

0.0
0.0 0.2
0

250

cr0

drowsy
cr1

0.0

150

hrat
Partial Residual

0.8

−1.0

Partial Residual

0.0
−0.4
−1.0

Partial Residual

cr0

2

0.4

50

4

feeding

cr1

cr1

−0.05

0.0

cr1

120

cr0

bul.conv

cr1

1

80

cr1

−0.4

0.0
−0.2

4

cr1
cr0

waz

0

40

cr0

−0.15

2

Partial Residual

cr1

0

0

cr0

rr

−0.4

Partial Residual

0.5

cr0

−4

cr0

temp

−0.5

Partial Residual

age

−0.8 −0.4

0.5
0.0
−0.5

32 34 36 38 40

cr1

Partial Residual

20 40 60 80

cr0

Partial Residual

0

cr1

Partial Residual

cr0

Partial Residual

−0.1 0.1

cr1

−0.4

Partial Residual

b u l . c o n v + drowsy + a g i t a t e d + r e f f o r t + a u s c +
f e e d i n g + abdominal , data=Sc , x=TRUE, y=TRUE)
# Use the update function to save repeating model right# hand side. An indicator variable for Y=1 is the
# response variable below
cr1
update ( cr0 , Y==1 ⇠ . , s u b s e t=Y 1 )
p l o t . l r m . p a r t i a l ( cr0 , cr1 , c e n t e r=TRUE) # Figure 14.5

5

cr0

0.0

0.4

0.8

abdominal

Fig. 14.5 loess smoothed partial residual plots for binary models that are components of an ordinal continuation ratio model. Solid lines correspond to a model for
Y = 0, and dotted lines correspond to a model for Y = 1|Y
1.

The output is in Figure 14.5. There is not much more parallelism here than
in Figure 14.4. For the two most important predictors, ausc and rr, there are
strongly di↵ering e↵ects for the di↵erent events being predicted (e.g., Y = 0
or Y = 1|Y
1). As is often the case, there is no one constant model that
satisfies assumptions with respect to all predictors simultaneously, especially
when there is evidence for non-ordinality for ausc in Figure 14.2. The CR
model will need to be generalized to adequately fit this dataset.

14.8 Extended Continuation Ratio Model

347

14.8 Extended Continuation Ratio Model
The CR model in its ordinary form has no advantage over the PO model for
this dataset. But Section 13.4.6 discussed how the CR model can easily be
extended to relax any of its assumptions. First we use the cr.setup function
to set up the data for fitting a CR model using the binary logistic trick.
u
c r . s e t u p (Y)
Sc.expanded
Sc [ u$ subs , ]
y
u$y
cohort
u$ c o h o r t

Here the cohort variable has values ’all’, ’Y>=1’ corresponding to the
conditioning events in Equation 13.10. Once the data frame is expanded to
include the di↵erent risk cohorts, vectors such as age are lengthened (to 5553
records). Now we fit a fully extended CR model that makes no equal slopes
assumptions; that is, the model has to fit Y assuming the covariables are
linear and additive. At this point, we omit hrat but add back all variables that
were deleted by examining their association with Y . Recall that most of these
seven cluster scores were summarized using P C1 . Adding back “insignificant”
variables will allow us to validate the model fairly using the bootstrap, as well
as to obtain confidence intervals that are not falsely narrow.16
full
lrm ( y ⇠ c o h o r t ⇤ ( ageg ⇤ ( r c s ( temp , 5 ) + r c s ( r r , 5 ) ) +
r c s ( waz , 4 ) + b u l . c o n v + drowsy + a g i t a t e d + r e f f o r t +
a u s c + f e e d i n g + abdominal + h y d r a t i o n + hxprob +
pustular + crying + f e v e r . i l l + stop.breath + labor ) ,
data=Sc.expanded , x=TRUE, y=TRUE)
# x=TRUE, y=TRUE are for pentrace, validate, calibrate below
perf
f u n c t i o n ( f i t ) { # model performance for Y=0
pr
p r e d i c t ( f i t , t y p e= ’ f i t t e d ’ ) [ c o h o r t == ’ a l l ’ ]
s
round ( somers2 ( pr , y [ c o h o r t == ’ a l l ’ ] ) , 3 )
pr
1
pr
# Predict Prob[Y > 0] instead of Prob[Y = 0]
f
round ( c ( mean ( pr < . 0 5 ) , mean ( pr > . 2 5 ) ,
mean ( pr > . 5 ) ) , 2 )
f
p a s t e ( f [ 1 ] , ’ , ’ , f [ 2 ] , ’ , and ’ , f [ 3 ] , ’ . ’ , s e p= ’ ’ )
l i s t ( somers=s , f r a c t i o n s=f )
}
perf.unpen
perf ( f u l l )
p r i n t ( f u l l , l a t e x=TRUE, c o e f s =5)

Logistic Regression Model
lrm(formula = y ˜ cohort * (ageg * (rcs(temp, 5) + rcs(rr, 5)) +
rcs(waz, 4) + bul.conv + drowsy + agitated + reffort + ausc +
feeding + abdominal + hydration + hxprob + pustular + crying +
fever.ill + stop.breath + labor), data = Sc.expanded, x = TRUE,
y = TRUE)

348

14 Ordinal Regression, Data Reduction, and Penalization

Table 14.8 Wald statistics for cohort in the CR model
2
d.f.
P
cohort (Factor+Higher Order Factors) 199.47 44 < 0.0001
All Interactions
172.12 43 < 0.0001
TOTAL
199.47 44 < 0.0001

Model Likelihood
Ratio Test
Obs
5553 LR 2
1824.33
0
1512 d.f.
87
1
4041 Pr(> 2 ) < 0.0001
L
max | @ log
| 8⇥10 7
@

Discrimination
Indexes
R2
0.406
g
1.677
gr
5.350
gp
0.269
Brier
0.135

Rank Discrim.
Indexes
C
0.843
Dxy
0.685
0.687
⌧a
0.272

Coef
S.E. Wald Z Pr(> |Z|)
Intercept
1.3966 9.0827
0.15
0.8778
cohort=Y 1
1.5077 14.6443
0.10
0.9180
ageg=[ 7,60)
-9.3715 11.4104
-0.82
0.4115
ageg=[60,90] -26.4502 17.2188
-1.54
0.1245
temp
-0.0049 0.2551
-0.02
0.9846
...
l a t e x ( anova ( f u l l , c o h o r t ) , f i l e = ’ ’ , l a b e l= ’ o r d i n a l a n o v a . c o h o r t ’ ,
c a p t i o n= ’ Wald s t a t i s t i c s f o r \\ co { c o h o r t } i n t h e CR model ’ ,
s i z e= ’ s m a l l e r [ 2 ] ’ )
# Table 14.8
an

anova ( f u l l , i n d i a=FALSE, i n d n l=FALSE)

l a t e x ( an , f i l e = ’ ’ , l a b e l= ’ o r d i n a l a n o v a . f u l l ’ ,
c a p t i o n= ’ Wald s t a t i s t i c s f o r t h e c o n t i n u a t i o n r a t i o m o d e l .
I n t e r a c t i o n s with \\ co { c o h o r t } a s s e s s n o n p r o p o r t i o n a l h a z a r d s ’ ,
c a p t i o n . l o t= ’ Wald s t a t i s t i c s f o r $Y$ i n t h e
c o n t i n u a t i o n r a t i o model ’ ,
s i z e= ’ s m a l l e r [ 2 ] ’ )
# Table 14.9

This model has LR 2 = 1824 with 87 d.f. Wald statistics are in Tables 14.8
and 14.9. The global test of the constant slopes assumption in the CR model
(test of all interactions involving cohort) has Wald 2 = 172 with 43 d.f.,
P < 0.0001. Consistent with Figure 14.5, the formal tests indicate that ausc
is the biggest violator, followed by waz and rr.

14.9 Penalized Estimation

349

Table 14.9 Wald statistics for the continuation ratio model. Interactions with
cohort assess non-proportional hazards
2

cohort
199.47
ageg
48.89
temp
59.37
rr
93.77
waz
39.69
bul.conv
10.80
drowsy
15.19
agitated
13.55
re↵ort
51.85
ausc
109.80
feeding
27.47
abdominal
1.78
hydration
4.47
hxprob
6.62
pustular
3.03
crying
1.55
fever.ill
3.63
stop.breath
5.34
labor
5.35
ageg ⇥ temp
8.18
ageg ⇥ rr
38.11
cohort ⇥ ageg
14.88
cohort ⇥ temp
8.77
cohort ⇥ rr
19.67
cohort ⇥ waz
9.04
cohort ⇥ bul.conv
0.33
cohort ⇥ drowsy
0.57
cohort ⇥ agitated
0.55
cohort ⇥ re↵ort
2.29
cohort ⇥ ausc
38.11
cohort ⇥ feeding
2.48
cohort ⇥ abdominal
0.09
cohort ⇥ hydration
0.53
cohort ⇥ hxprob
2.54
cohort ⇥ pustular
2.40
cohort ⇥ crying
0.39
cohort ⇥ fever.ill
3.17
cohort ⇥ stop.breath
2.99
cohort ⇥ labor
0.05
cohort ⇥ ageg ⇥ temp
2.22
cohort ⇥ ageg ⇥ rr
10.22
TOTAL NONLINEAR
93.36
TOTAL INTERACTION
203.10
TOTAL NONLINEAR + INTERACTION 257.70
TOTAL
1211.73

d.f.
44
36
24
24
6
2
2
2
2
2
2
2
2
2
2
2
2
2
2
16
16
18
12
12
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
8
8
40
59
67
87

P
< 0.0001
0.0742
0.0001
< 0.0001
< 0.0001
0.0045
0.0005
0.0011
< 0.0001
< 0.0001
< 0.0001
0.4106
0.1069
0.0364
0.2194
0.4604
0.1630
0.0693
0.0690
0.9432
0.0015
0.6701
0.7225
0.0736
0.0288
0.5658
0.4489
0.4593
0.1298
< 0.0001
0.1152
0.7696
0.4682
0.1109
0.1210
0.5310
0.0749
0.0839
0.8309
0.9736
0.2500
< 0.0001
< 0.0001
< 0.0001
< 0.0001

14.9 Penalized Estimation
We know that the CR model must be extended to fit these data adequately.
If the model is fully extended to allow for all cohort ⇥ predictor interactions,
we have not gained any precision or power in using an ordinal model over
using a polytomous logistic model. Therefore we seek some restrictions on the
model’s parameters. The lrm and pentrace functions allow for di↵ering for
shrinking di↵erent types of terms in the model. Here we do a grid search to

350

14 Ordinal Regression, Data Reduction, and Penalization

determine the optimum penalty for simple main e↵ect (non-interaction) terms
and the penalty for interaction terms, most of which are terms interacting
with cohort to allow for unequal slopes. The following code uses pentrace
on the full extended CR model fit to find the optimum penalty factors. All
combinations of the simple and interaction s for which the interaction
penalty the penalty for the simple parameters are examined.
d
o p t i o n s ( d i g i t s =4)
pentrace ( f u l l ,
l i s t ( s i m p l e=c ( 0 , . 0 2 5 , . 0 5 , . 0 7 5 , . 1 ) ,
i n t e r a c t i o n=c ( 0 , 1 0 , 5 0 , 1 0 0 , 1 2 5 , 1 5 0 ) ) )
Best penalty:
simple interaction
df
0.05
125 49.75
simple interaction
df aic bic aic.c
0.000
0 87.00 1650 1074 1648
0.000
10 60.63 1671 1269 1669
0.025
10 60.11 1672 1274 1670
0.050
10 59.80 1672 1276 1670
0.075
10 59.58 1671 1277 1670
0.100
10 59.42 1671 1278 1670
0.000
50 54.64 1671 1309 1670
0.025
50 54.14 1672 1313 1671
0.050
50 53.83 1672 1316 1671
0.075
50 53.62 1672 1317 1671
0.100
50 53.46 1672 1318 1671
0.000
100 51.61 1672 1330 1671
0.025
100 51.11 1673 1334 1672
0.050
100 50.81 1673 1336 1672
0.075
100 50.60 1672 1337 1671
0.100
100 50.44 1672 1338 1671
0.000
125 50.55 1672 1337 1671
0.025
125 50.05 1673 1341 1672
0.050
125 49.75 1673 1343 1672
0.075
125 49.54 1672 1344 1672
0.100
125 49.39 1672 1345 1671
0.000
150 49.65 1672 1343 1671
0.025
150 49.15 1672 1347 1672
0.050
150 48.85 1673 1349 1672
0.075
150 48.64 1672 1350 1671
0.100
150 48.49 1672 1351 1671

options (d)

We see that shrinkage from 87 d.f. down to 49.75 e↵ective d.f. results in an
improvement in 2 –scaled AIC of 23. The optimum penalty factors were 0.05
for simple terms and 125 for interaction terms.
Let us now store a penalized version of the full fit, find where the e↵ective
d.f. were reduced, and compute 2 for each factor in the model. We take
the e↵ective d.f. for a collection of model parameters to be the sum of the
diagonals of the matrix product defined underneath Gray’s Equation 2.9232
that correspond to those parameters.
full.pen
update ( f u l l ,

14.9 Penalized Estimation

351

p e n a l t y= l i s t ( s i m p l e=. 0 5 , i n t e r a c t i o n =125))
p r i n t ( f u l l . p e n , l a t e x=TRUE, c o e f s=FALSE)

Logistic Regression Model
lrm(formula = y ˜ cohort * (ageg * (rcs(temp, 5) + rcs(rr, 5)) +
rcs(waz, 4) + bul.conv + drowsy + agitated + reffort + ausc +
feeding + abdominal + hydration + hxprob + pustular + crying +
fever.ill + stop.breath + labor), data = Sc.expanded, x = TRUE,
y = TRUE, penalty = list(simple = 0.05, interaction = 125))
Penalty factors
simple nonlinear interaction nonlinear.interaction
0.05
0.05
125
125
Model Likelihood
Ratio Test
Obs
5553 LR 2
1772.11
0
1512 d.f.
49.75
1
4041 Pr(> 2 ) < 0.0001
L
max | @ log
| 1⇥10 7 Penalty
21.48
@

Discrimination
Indexes
2
R
0.392
g
1.594
gr
4.924
gp
0.263
Brier
0.136

Rank Discrim.
Indexes
C
0.840
Dxy
0.679
0.681
⌧a
0.269

effective.df ( full.pen )
Original and Effective Degrees of Freedom
All
Simple Terms
Interaction or Nonlinear
Nonlinear
Interaction
Nonlinear Interaction

Original Penalized
87
49.75
20
19.98
67
29.77
40
16.82
59
22.57
32
9.62

## Compute discrimination for Y=0 vs. Y>0
perf.pen
perf ( full.pen )
# Exclude interactions and cohort effects from plot
p l o t ( anova ( f u l l . p e n ) , c e x . l a b e l s =0. 7 5 , r m . i a=TRUE,
r m . o t h e r= ’ c o h o r t
( F a c t o r+H i g h e r Order F a c t o r s ) ’ )
# Figure 14.6

This will be the final model except for the model used in Section 14.10.
The model has LR 2 = 1772. The output of effective.df shows that
non-interaction terms have barely been penalized, and coefficients of interaction terms have been shrunken from 59 d.f. to e↵ectively 22.6 d.f. Predictive

352

14 Ordinal Regression, Data Reduction, and Penalization
ageg
fever.ill
crying
pustular
abdominal
hydration
stop.breath
labor
hxprob
bul.conv
agitated
drowsy
temp
feeding
waz
rr
reffort
ausc

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

−20

0

20

40

60

80

100

120

χ2 − df
Fig. 14.6 Importance of predictors in full penalized model, as judged by partial
Wald 2 minus the predictor d.f. The Wald 2 values for each line in the dot plot
include contributions from all higher-order e↵ects. Interaction e↵ects by themselves
have been removed as has the cohort e↵ect.

discrimination was assessed by computing the Somers’ Dxy rank correlation between X ˆ and whether Y = 0, in the subset of records for which
Y = 0 is what was being predicted. Here Dxy = 0.672, and the ROC area
is 0.838 (the unpenalized model had an apparent Dxy = 0.676). To summarize in another way the e↵ectiveness of this model in screening infants for
risks of any abnormality, the fraction of infants with predicted probabilities
that Y > 0 being < 0.05, > 0.25, and > 0.5 are, respectively, 0.1, 0.28, and
0.14. anova output is plotted in Figure 14.6 to give a snapshot of the importance of the various predictors. The Wald statistics used here are computed
on a variance–covariance matrix which is adjusted for penalization (using
Gray Equation 2.6232 before it was determined that the sandwich covariance
estimator performs less well than the inverse of the penalized information
matrix—see p. 213).
The full equation for the fitted model is below. Only the part of the equation used for predicting Pr(Y = 0) is shown, other than an intercept for
Y
1 that does not apply when Y = 0.
latex ( full.pen ,

which =1:21 ,

f i l e=’ ’ )

Xˆ=
1.337435[Y >= 1]
+0.1074525[ageg 2 [ 7, 60)] + 0.1971287[ageg 2 [60, 90]]
+0.1978706temp + 0.1091831(temp

3

36.19998)+

2.833442(temp

3

37)+

14.9 Penalized Estimation

353
3

+5.07114(temp

37.29999)+

+0.1606456(temp
+0.02090741rr

5

6.336873⇥10
5

+6.152416⇥10

(rr

3
49)+

(rr

37.69998)+

3

+0.1225752(waz

0.08185088 drowsy

3
59)+

0.0001018105(rr
3

3
0.28)+

(rr

3

42)+

+ 1.960063⇥10

5

(rr

3

76)+

3

0.1185068(waz + 0.75)+
3
1.73)+

0.02915754(waz

0.05327209 agitated

0.1599588 feeding

5

32)+ + 8.405441⇥10

0.07589699waz + 0.02508918(waz + 2.9)+

1.158604 ausc

3

2.507527(temp

3
39)+

0.4418073 bul.conv

0.2304409 re↵ort

0.1608684 abdominal

0.05409718 hydration + 0.08086387 hxprob + 0.007519746 pustular
+0.04712091 crying + 0.004298725 fever.ill

0.3519033 stop.breath

+0.06863879 labor
5

+[ageg 2 [ 7, 60)][6.499592⇥10
0.008691166(temp
+0.0259236(temp

3

37)+

temp

0.01640406(temp
+0.09142148(temp

3
37)+

3

+[ageg 2 [ 7, 60)][ 0.0009437598rr
1.670499⇥10

(rr

3
42)+

6.382087⇥10

6

(rr

76)+ ]

8.628392⇥10
1.98372⇥10

5

(rr

(rr

3

36.19998)+

3
37.29999)+
3

0.02558693(temp
1.044673⇥10

5.189082⇥10

6

6

39)+ ]
(rr

3
49)+

(rr

3

32)+
+ 1.428634⇥10

5

(rr

59)+

3

5

(rr

59)+

3

+[ageg 2 [60, 90]][ 0.001920811rr
6

3
39)+ ]

0.00182639(temp

0.0476041(temp

37.69998)+

6

37.29999)+

0.009444801(temp

+[ageg 2 [60, 90]][0.0001320368temp

36.19998)+
3

0.004987871(temp

3
37.69998)+

3

0.00279976(temp

3
42)+

6

5.52134⇥10

4.147347⇥10

6

(rr

(rr
3
49)+

3

32)+
+ 3.813427⇥10

3

3

76)+ ]

where [c] = 1 if subject is in group c, 0 otherwise; (x)+ = x if x > 0, 0 otherwise.

Now consider displays of the shapes of e↵ects of the predictors. For the
continuous variables temp and rr that interact with age group, we show
the e↵ects for all three age groups separately for each Y cuto↵. All e↵ects
have been centered so that the log odds at the median predictor value is
zero when cohort=’all’, so these plots actually show log odds relative to
reference values. The patterns in Figures 14.9 and 14.8 are in agreement with
those in Figure 14.5.
yl
#
#
#
#
#

c ( 3, 1)

# put all plots on common y-axis scale

Plot predictors that interact with another predictor
Vary ageg over all age groups, then vary temp over its
default range (10th smallest to 10th largest values in
data). Make a separate plot for each ’cohort’
ref.zero centers effects using median x

dd
d a t a d i s t ( S c . e x p a n d e d ) ; dd
o p t i o n s ( d a t a d i s t= ’ dd ’ )
p1

d a t a d i s t ( dd , c o h o r t )

P r e d i c t ( f u l l . p e n , temp , ageg , c o h o r t ,
r e f . z e r o=TRUE, c o n f . i n t=FALSE)

354

14 Ordinal Regression, Data Reduction, and Penalization

p2

Predict ( f u l l . p e n , rr ,
ageg , c o h o r t ,
r e f . z e r o=TRUE, c o n f . i n t=FALSE)
p
r b i n d ( temp=p1 , r r=p2 )
# Figure 14.7:
g g p l o t ( p , ⇠ c o h o r t , g r o u p s= ’ ageg ’ , v a r y p r e d=TRUE,
y l i m=y l , l a y o u t=c ( 2 , 1 ) , l e g e n d . p o s i t i o n=c ( . 8 5 , . 8 ) ,
a d d l a y e r=l t he me ( width =3 , h e i g h t =3 , t e x t =2. 5 , t i t l e =2 . 5 ) ,
a d j . s u b t i t l e=FALSE)

all

1

Y>=1
Age in days
[ 0, 7)
[ 7,60)
[60,90]

log odds

0
−1
−2
−3
34

36

38

40

34

36

38

40

Temperature
all

1

Y>=1

log odds

0
−1
−2
−3
30

60

90

30

60

90

Adjusted respiratory rate
Fig. 14.7 Centered e↵ects of predictors on the log odds, showing the e↵ects of two
predictors with interaction e↵ects for the age intervals noted. The title all refers
to the prediction of Y = 0|Y
0, that is, Y = 0. Y>=1 refers to predicting the
probability of Y = 1|Y
1.

# For each predictor that only interacts with cohort, show

14.10 Using Approximations to Simplify the Model

355

# the differing effects of the predictor for predicting
# Pr(Y=0) and Pr(Y=1 given Y exceeds 0) on the same graph
dd$ l i m i t s [ ’ A djust t o ’ , ’ c o h o r t ’ ]
’Y 1 ’
v
Cs ( waz , b u l . c o n v , drowsy , a g i t a t e d , r e f f o r t , ausc ,
f e e d i n g , abdominal , h y d r a t i o n , hxprob , p u s t u l a r ,
crying )
yeq1
P r e d i c t ( f u l l . p e n , name=v , r e f . z e r o=TRUE)
yl
c ( 1.5 , 1 . 5 )
g g p l o t ( yeq1 , y l i m=y l , s e p d i s c r e t e= ’ v e r t i c a l ’ )
# Figure 14.8
dd$ l i m i t s [ ’ A djust t o ’ , ’ c o h o r t ’ ]
’ a l l ’ # original default
all
P r e d i c t ( f u l l . p e n , name=v , r e f . z e r o=TRUE)
g g p l o t ( a l l , y l i m=y l , s e p d i s c r e t e= ’ v e r t i c a l ’ ) # Figure 14.9
1

14.10 Using Approximations to Simplify the Model
Parsimonious models can be developed by approximating predictions from
the model to any desired level of accuracy. Let L̂ = X ˆ denote the predicted
log odds from the full penalized ordinal model, including multiple records for
subjects with Y > 0. Then we can use a variety of techniques to approximate
L̂ from a subset of the predictors (in their raw form). With this approach
one can immediately see what is lost over the full model by computing, for
example, the mean absolute error in predicting L̂. Another advantage to full
model approximation is that shrinkage used in computing L̂ is inherited by
any model that predicts L̂. In contrast, the usual stepwise methods result in
ˆ that are too large since the final coefficients are estimated as if the model
structure were prespecified.
CART would be particularly useful as a model approximator as it would
result in a prediction tree that would be easy for health workers to use.
Unfortunately, a 50-node CART was required to predict L̂ with an R2 0.9,
and the mean absolute error in the predicted logit was still 0.4. This will
happen when the model contains many important continuous variables.
Let’s approximate the full model using its important components, by using
a step-down technique predicting L̂ from all of the component variables using
ordinary least squares. In using step-down with the least squares function ols
in rms there is a problem when the initial R2 = 1.0 as in that case the estimate of = 0. This can be circumvented by specifying an arbitrary nonzero
value of to ols (here 1.0), as we are not using the variance–covariance matrix from ols anyway. Since cohort interacts with the predictors, separate
approximations can be developed for each level of Y . For this example we
approximate the log odds that Y = 0 using the cohort of patients used for
determining Y = 0, that is, Y
0 or cohort=’all’.
plogit
predict ( full.pen )
f
o l s ( p l o g i t ⇠ ageg ⇤ ( r c s ( temp , 5 ) + r c s ( r r , 5 ) ) +

2

356

14 Ordinal Regression, Data Reduction, and Penalization

agitated

crying

drowsy

1
0
−1

log odds

0.0

2.5

5.0 7.5
feeding

10.0−5 −4 −3 −2 −1
hxprob

0

1 0

0

1

2
3
reffort

4

5

−2
−1
0
1
Weight−for−age zscore

0

1

2

4

5

−4

1

2
3
hydration

4

5

1
0
−1
2.5

5.0

7.5

1
0
−1
3

−2

0

2
ausc

abdominal
1
0

1
0

●
●

●
●

Any Pustular Condition

bul.conv
1
0

1
0

●
●

−1

0

1

●
●

−1

0

1

log odds
Fig. 14.8 Centered e↵ects of predictors on the log odds, for predicting Y = 1|Y

r c s ( waz , 4 ) + b u l . c o n v + drowsy + a g i t a t e d +
r e f f o r t + a u s c + f e e d i n g + abdominal + h y d r a t i o n +
hxprob + p u s t u l a r + c r y i n g + f e v e r . i l l +
stop.breath + labor ,
s u b s e t=c o h o r t== ’ a l l ’ , data=Sc.expanded , sigma =1)
# Do fast backward stepdown
w
o p t i o n s ( width =120)
f a s t b w ( f , a i c s =1e10 )

1

10.0

14.10 Using Approximations to Simplify the Model

agitated

357

crying

drowsy

1
0
−1

log odds

0.0

2.5

5.0 7.5
feeding

10.0−5 −4 −3 −2 −1
hxprob

0

1 0

0

1

2
3
reffort

4

5

−2
−1
0
1
Weight−for−age zscore

0

1

2

4

5

−4

1

2
3
hydration

4

5

1
0
−1
2.5

5.0

7.5

1
0
−1
3

−2

0

2
ausc

abdominal
1
0

1
0

●
●

●
●

Any Pustular Condition

bul.conv
1
0

1
0

●
●

−1

0

1

●
●

−1

0

1

log odds
Fig. 14.9 Centered e↵ects of predictors on the log odds, for predicting Y
plot was made for the fever.ill, stop.breath. or labor cluster scores.

Deleted
Chi-Sq
ageg * temp
1.87
ageg
0.05
pustular
0.02
fever.ill
0.08
crying
9.47
abdominal
12.66
rr
17.90
hydration
13.21
labor
23.48

d.f.
8
2
1
1
1
1
4
1
1

P
Residual d.f.
0.9848
1.87
8
0.9740
1.92 10
0.8778
1.94 11
0.7828
2.02 12
0.0021
11.49 13
0.0004
24.15 14
0.0013
42.05 18
0.0003
55.26 19
0.0000
78.74 20

P
AIC
R2
0.9848 -14.13 1.000
0.9969 -18.08 1.000
0.9987 -20.06 1.000
0.9994 -21.98 1.000
0.5698 -14.51 0.999
0.0440
-3.85 0.997
0.0011
6.05 0.995
0.0000
17.26 0.993
0.0000
38.74 0.990

1. No

10.0

358
stop.breath
33.40 1
bul.conv
51.53 1
agitated
63.66 1
hxprob
84.16 1
drowsy
109.86 1
temp
295.67 4
waz
368.86 3
reffort
449.83 1
ageg * rr
751.19 8
ausc
1906.82 1
feeding
3900.33 1

14 Ordinal Regression, Data Reduction, and Penalization
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000

112.14
163.67
227.33
311.49
421.35
717.01
1085.87
1535.70
2286.90
4193.72
8094.04

21
22
23
24
25
29
32
33
41
42
43

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000

70.14
119.67
181.33
263.49
371.35
659.01
1021.87
1469.70
2204.90
4109.72
8008.04

0.986
0.980
0.972
0.962
0.948
0.911
0.866
0.810
0.717
0.482
0.000

Approximate Estimates after Deleting Factors
Coef
S.E. Wald Z P
[1,] 1.617 0.01482 109.1 0
Factors in Final Model
None

o p t i o n s (w)
# 1e10 causes all variables to eventually be
# deleted so can see most important ones in order
# Fit an approximation to the full penalized model using
# most important variables
full.approx
o l s ( p l o g i t ⇠ r c s ( temp , 5 ) + ageg ⇤ r c s ( r r , 5 ) +
r c s ( waz , 4 ) + b u l . c o n v + drowsy + r e f f o r t +
ausc + feeding ,
s u b s e t=c o h o r t== ’ a l l ’ , data=S c . e x p a n d e d )
p
predict ( full.approx )
abserr
mean ( abs ( p
p l o g i t [ c o h o r t == ’ a l l ’ ] ) )
Dxy
somers2 ( p , y [ c o h o r t == ’ a l l ’ ] ) [ ’ Dxy ’ ]

The approximate model had R2 against the full penalized model of 0.972, and
the mean absolute error in predicting L̂ was 0.17. The Dxy rank correlation
between the approximate model’s predicted logit and the binary event Y = 0
is 0.665 as compared with the full model’s Dxy = 0.672. See Section 19.5 for
an example of computing correct estimates of variance of the parameters in
an approximate model.
Next turn to diagramming this model approximation so that all predicted
values can be computed without the use of a computer. We draw a type of
nomogram that converts each e↵ect in the model to a 0 to 100 scale which is
just proportional to the log odds. These points are added across predictors
to derive the “Total Points,” which are converted to L̂ and then to predicted
probabilities. For the interaction between rr and ageg, rms’s nomogram function automatically constructs three rr axes—only one is added into the total
point score for a given subject. Here we draw a nomogram for predicting the
probability that Y > 0, which is 1 Pr(Y = 0). This probability is derived
by negating ˆ and X ˆ in the model derived to predict Pr(Y = 0).
f
full.approx
f$coefficients

f$ c o e f f i c i e n t s

14.11 Validating the Model
f$linear.predictors

359
f$ l i n e a r . p r e d i c t o r s

n

nomogram ( f ,
temp = 3 2 : 4 1 , r r=s e q ( 2 0 , 1 2 0 , by =10) ,
waz=s e q ( 1.5 , 2 , by=. 5 ) ,
f u n=p l o g i s , f u n l a b e l= ’ Pr (Y>0) ’ ,
f u n . a t=c ( . 0 2 , . 0 5 , s e q ( . 1 , . 9 , by=. 1 ) , . 9 5 , . 9 8 ) )
# Print n to see point tables
p l o t ( n , lmgp=. 2 , c e x . a x i s=. 6 ) # Figure 14.10
newsubject
d a t a . f r a m e ( ageg= ’ [ 0 , 7 ) ’ , r r =30 , temp=39 , waz=0 , drowsy =5 ,
r e f f o r t =2 , b u l . c o n v =0 , a u s c =0 , f e e d i n g =0)
xb
p r e d i c t ( f , newsubject )

The nomogram is shown in Figure 14.10. As an example in using the nomogram, a six-day-old infant gets approximately 9 points for having a respiration
rate of 30/minute, 19 points for having a temperature of 39 C, 11 points for
waz=0, 14 points for drowsy=5, and 15 points for reffort=2. Assuming that
bul.conv=ausc=feeding=0, that infant gets 68 total points. This corresponds
to X ˆ = 0.68 and a probability of 0.34.

14.11 Validating the Model
For the full CR model that was fitted using penalized maximum likelihood
estimation (PMLE), we used 200 bootstrap replications to estimate and then
to correct for optimism in various statistical indexes: Dxy , generalized R2 ,
intercept and slope of a linear re-calibration equation for X ˆ, the maximum
calibration error for Pr(Y = 0) based on the linear-logistic re-calibration
(Emax), and the Brier quadratic probability score B. PMLE is used at each
of the 200 resamples. During the bootstrap simulations, we sample with replacement from the patients and not from the 5553 expanded records, hence
the specification cluster=u$subs, where u$subs is the vector of sequential
patient numbers computed from cr.setup above. To be able to assess predictive accuracy of a single predicted probability, the subset parameter is
specified so that Pr(Y = 0) is being assessed even though 5553 observations
are used to develop each of the 200 models.
set.seed (1)
# so can reproduce results
v
v a l i d a t e ( f u l l . p e n , B=200 , c l u s t e r=u$ subs ,
s u b s e t=c o h o r t== ’ a l l ’ )
l a t e x ( v , f i l e = ’ ’ , d i g i t s =2 , s i z e= ’ s m a l l e r ’ )

3

360

14 Ordinal Regression, Data Reduction, and Penalization
0

10

20

30

40

50

60

70

80

90

100

Points
38

Temperature
37

36

35

50

rr (ageg=[ 0, 7))
40

rr (ageg=[60,90])
Weight−for−age
zscore

30

30

2

1

41

33

32

70

20
60

30
50

40

34
60

40
50

rr (ageg=[ 7,60))

39

20

90 110

70

80

60

90

100

110

70

120

80

90

100

110

120

20

0

−0.5
1

bul.conv

0
1

drowsy

3

0

2

5
4

1

reffort

3

0

5

2

4
1

ausc
0
1

feeding

3

0

Total Points
Linear Predictor

2

0

4

20

−4

−3

0.02

0.05

5

40

60

−2

−1

80

100

0

120

1

140

2

160

3

180

200

4

220

240

5

Pr(Y>0)
0.1

0.2

0.3 0.4 0.5 0.6 0.7

0.8

0.9

0.95

0.98

Fig. 14.10 Nomogram for predicting Pr(Y > 0) from the penalized extended CR
model, using an approximate model fitted using ordinary least squares (R2 = 0.972
against the full model’s predicted logits).

6

260

14.12 Summary
Index

361
Original Training Test Optimism Corrected n
Sample

Dxy
R2
Intercept
Slope
Emax
D
U
Q
B
g
gp
v

0.67
0.38
0.03
1.03
0.00
0.28
0.00
0.28
0.12
1.47
0.22

Sample

0.68
0.38
0.03
1.03
0.00
0.29
0.00
0.29
0.12
1.50
0.23

Sample

0.67
0.37
0.00
1.00
0.00
0.28
0.00
0.28
0.12
1.45
0.22

Index

0.01
0.01
0.03
0.03
0.00
0.01
0.00
0.01
0.00
0.04
0.00

0.66
0.36
0.00
1.00
0.00
0.27
0.00
0.27
0.12
1.42
0.22

200
200
200
200
200
200
200
200
200
200
200

round ( v , 3 )

We see that for the apparent Dxy = 0.672 and that the optimism from overfitting was estimated to be 0.011 for the PMLE model, so the bias-corrected
estimate of predictive discrimination is 0.661. The intercept and slope needed
to re-calibrate X ˆ to a 45 line are very near (0, 1). The estimate of the
maximum calibration error in predicting Pr(Y = 0) is 0.001, which is quite
satisfactory. The corrected Brier score is 0.122.
The simple calibration statistics just listed do not address the issue of
whether predicted values from the model are miscalibrated in a nonlinear
way, so now we estimate an overfitting-corrected calibration curve nonparametrically.
cal
err

c a l i b r a t e ( f u l l . p e n , B=200 , c l u s t e r=u$ subs ,
s u b s e t=c o h o r t== ’ a l l ’ )
p l o t ( c a l ) # Figure 14.11

n=5553
Mean absolute error=0.017
Mean squared error=0.00043
0.9 Quantile of absolute error=0.038

The results are shown in Figure 14.11. One can see a slightly nonlinear calibration function estimate, but the overfitting-corrected calibration is excellent everywhere, being only slightly worse than the apparent calibration. The
estimated maximum calibration error is 0.044. The excellent validation for
both predictive discrimination and calibration are a result of the large sample
size, frequency distribution of Y , initial data reduction, and PMLE.

14.12 Summary
Clinically guided variable clustering and item weighting resulted in a great
reduction in the number of candidate predictor degrees of freedom and hence
increased the true predictive accuracy of the model. Scores summarizing clusters of clinical signs, along with temperature, respiration rate, and weightfor-age after suitable nonlinear transformation and allowance for interactions

362

14 Ordinal Regression, Data Reduction, and Penalization
1.0

Actual Probability

0.8

0.6

0.4

Apparent

0.2

Bias−corrected
Ideal

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Predicted Pr{y=1}
Fig. 14.11 Bootstrap calibration curve for the full penalized extended CR model.
200 bootstrap repetitions were used in conjunction with the loess smoother.108 Also
shown is a “rug plot” to demonstrate how e↵ective this model is in discriminating
patients into low- and high-risk groups for Pr(Y = 0) (which corresponds with the
derived variable value y = 1 when cohort=’all’).

with age, are powerful predictors of the ordinal response. Graphical methods
are e↵ective for detecting lack of fit in the PO and CR models and for diagramming the final model. Model approximation allowed development of parsimonious clinical prediction tools. Approximate models inherit the shrinkage
from the full model. For the ordinal model developed here, substantial shrinkage of the full model was needed.

14.13 Further Reading
1
2
3

See Moons et al.455 for another case study in penalized maximum likelihood
estimation.
The lasso method of Tibshirani601, 602 also incorporates shrinkage into variable
selection.
To see how this compares with predictions using the full model, the extra clinical
signs in that model that are not in the approximate model were predicted
individually on the basis of X ˆ from the reduced model along with the signs
that are in that model, using ordinary linear regression. The signs not specified
when evaluating the approximate model were then set to predicted values based
on the values given for the 6-day-old infant above. The resulting X ˆ for the full

14.14 Problems

363

model is 0.81 and the predicted probability is 0.31, as compared with -0.68
and 0.34 quoted above.

14.14 Problems
Develop a proportional odds ordinal logistic model predicting the severity of
functional disability (sfdm2) in SUPPORT. The highest level of this variable
corresponds to patients dying before the two-month follow-up interviews.
Consider this level as the most severe outcome. Consider the following predictors: age, sex, dzgroup, num.co, scoma, race (use all levels), meanbp,
hrt, temp, pafi, alb, adlsc. The last variable is the baseline level of functional disability from the “activities of daily living scale.”
1. For the variables adlsc, sex, age, meanbp, and others if you like, make
plots of means of predictors stratified by levels of the response, to check for
ordinality. On the same plot, show estimates of means assuming the proportional odds relationship between predictors and response holds. Comment on the evidence for ordinality and for proportional odds.
2. To allow for maximum adjustment of baseline functional status, treat this
predictor as nominal (after rounding it to the nearest whole number; fractional values are the result of imputation) in remaining steps, so that all
dummy variables will be generated. Make a single chart showing proportions of various outcomes stratified (individually) by adlsc, sex, age,
meanbp. For continuous predictors use quartiles. You can pass the following function to the summary (summary.formula) function to obtain the
proportions of patients having sfdm2 at or worse than each of its possible
levels (other than the first level). An easy way to do this is to use the
cumcategory function with the Hmisc package’s summary.formula function. cumcategorysummary.formula Print estimates to only two significant
digits of precision. Manually check the calculations for the sex variable using table(sex, sfdm2). Then plot all estimates on a single graph using
plot(object, which=1:4), where object was created by summary (actually summary.formula). Note: for printing tables you may want to convert
sfdm2 to a 0–4 variable so that column headers are short and so that later
calculations are simpler. You can use for example:
sfdm

a s . i n t e g e r ( sfdm2 )

1

3. Use an R function such as the following to compute the logits of the cumulative proportions.
sf
c ( ’Y
’Y
’Y
’Y

function (y)
1 ’=q l o g i s ( mean ( y
2 ’=q l o g i s ( mean ( y
3 ’=q l o g i s ( mean ( y
4 ’=q l o g i s ( mean ( y

1)) ,
2)) ,
3)) ,
4)))

364

14 Ordinal Regression, Data Reduction, and Penalization

As the Y = 3 category is rare, it may be even better to omit the Y
4
column above, as was done in Section 13.3.9 and Figure 13.1. For each
predictor pick two rows of the summary table having reasonable sample
sizes, and take the di↵erence between the two rows. Comment on the
validity of the proportional odds assumption by assessing how constant
the row di↵erences are across columns. Note: constant di↵erences in log
odds (logits) mean constant ratios of odds or constant relative e↵ects of
the predictor across outcome levels.
4. Make two plots nonparametrically relating age to all of the cumulative
proportions or their logits. You can use commands such as the following
(to use the R Hmisc package). plsmo
for ( i in 1:4)
plsmo ( age , sfdm
i , add=i >1 ,
y l i m=c ( . 2 , . 8 ) , y l a b= ’ P r o p o r t i o n Y j ’ )
for ( i in 1:4)
plsmo ( age , sfdm
i , add=i >1 , f u n=q l o g i s ,
y l i m=q l o g i s ( c ( . 2 , . 8 ) ) , y l a b= ’ l o g i t ’ )

5.
6.

7.

8.

9.

Comment on the linearity of the age e↵ect (which of the two plots do
you use?) and on the proportional odds assumption for age, by assessing
parallelism in the second plot.
Impute race using the most frequent category and pafi and alb using
“normal” values.
Fit a model to predict the ordinal response using all predictors. For continuous ones assume a smooth relationship but allow it to be nonlinear.
Quantify the ability of the model to discriminate patients in the five outcomes. Do an overall likelihood ratio test for whether any variables are
associated with the level of functional disability.
Compute partial tests of association for each predictor and a test of nonlinearity for continuous ones. Compute a global test of nonlinearity. Graphically display the ranking of importance of the predictors.
Display the shape of how each predictor relates to the log odds of exceeding
any level of sfdm2 you choose, setting other predictors to typical values
(one value per predictor). By default, Predict will make predictions for
the second response category, which is a satisfactory choice here.
Use resampling to validate the Somers’ Dxy rank correlation between predicted logit and the ordinal outcome. Also validate the generalized R2 ,
and slope shrinkage coefficient, all using a single R command. Comment
on the quality (potential “export-ability”) of the model.

Chapter 15

Regression Models for Continuous Y and
Case Study in Ordinal Regression

This chapter concerns univariate continuous Y . There are many multivariable
models for predicting such response variables, such as
• linear models with assumed normal residuals, fitted with ordinary least
squares
• generalized linear models and other parametric models based on special
distributions such as the gamma
• generalized additive models (GAMs)273
• generalization of GAMs to also nonparametrically transform Y (see Chapter 16)
• quantile regression (see Section 15.2)
• other robust regression models that, like quantile regression, use an objective di↵erent from minimizing the sum of squared errors627
• semiparametric models based on the ranks of Y , such as the Cox proportional hazards model (Chapter 20) and the proportional odds ordinal
logistic model (Chapters 13 and 14)
• cumulative probability models (often called cumulative link models) which
are semiparametric models from a wider class of families than the logistic.
Semiparametric models that treat Y as ordinal but not interval-scaled have
many advantages including robustness and freedom from all distributional
assumptions for Y conditional on any given set of predictors.Advantages are
demonstrated in a case study of a cumulative probability ordinal model.
Some of the results are compared to quantile regression and OLS. Many of
the methods used in the case study also apply to ordinary linear models.

365

366 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

15.1 The Linear Model
The most popular multivariable model for analyzing a univariate continuous
Y is the the linear model
E(Y |X) = X ,
(15.1)

where isP
estimated using ordinary least squares, that is, by solving for ˆ to
minimize (Yi X ˆ)2 .
To compute P -values and confidence limits using parametric methods we
would have to assume that Y |X is normal with mean X and constant variance 2a . One could estimate conditional means of Y without any distributional assumptions, but least squares estimators are not robust to outliers
or high-leverage points, and the model would be inaccurate in estimating
conditional quantiles of Y |X or the probability that Prob[Y
c|X] unless
normality of residuals holds. To be accurate in estimating all quantities, the
linear model assumes that the Gaussian distribution of Y |X1 is a simple shift
from the distribution of Y |X2 .

15.2 Quantile Regression
Quantile regression348, 350 is a di↵erent approach to modeling Y . It makes
no distributional assumptions other than continuity of Y , while having all
the usual right hand side assumptions. Quantile regression provides essentially the same estimates as sample quantiles if there is only an intercept or
a categorical predictor in the model. Quantile regression is transformation
invariant — pre-transforming Y is not important.
Quantile regression is a natural generalization of sample quantiles. Let
⇢
(y)
= y(⌧
[y < 0]). The ⌧ th sample quantile is the minimizer q of
⌧
P
n
⇢
(y
q).
For a conditional ⌧ th quantile of Y |X the corresponding
i 1 ⌧ i
Pn
quantile regression estimator ˆ⌧ minimizes i=1 ⇢⌧ (Yi X ).
In non-large samples, quantile regression is not as efficient at estimating
quantiles as is ordinary least squares at estimating the mean, if the latter’s
assumptions hold.
Koenker’s quantreg package in R349 implements quantile regression, and
the rms package’s Rq function provides a front-end that gives rise to various
graphics and inference tools.
We directly model the median gh as a function of covariates so that only
the X structure need be correct. Other quantiles (e.g., 90th percentile) can
be modeled but standard errors will be much larger as it is more difficult to
precisely estimate outer quantiles.
a

The latter assumption may be dispensed with if we use a robust Huber–White or
bootstrap covariance matrix estimate. Normality may sometimes be dispensed with
by using the bootstrap nonparametric percentile confidence interval.

15.3 Ordinal Regression Models for Continuous Y

367

15.3 Ordinal Regression Models for Continuous Y
A di↵erent robust semiparametric regression approach than quantile regression is the cumulative probability ordinal model. Semiparametric models
have several advantages over parametric models such as OLS. While quantile
regression has no restriction in the parameters when modeling one quantile
versus anotherb , ordinal cumulative probability models assume a connection
between distributions of Y for di↵erent X. Ordinal regression even makes
one less assumption than quantile regression about the distribution of Y for
a specific X: the distribution need not be continuous.
Applying an increasing 1–1 transformation to Y results in no change to
regression coefficient estimates with ordinal regressionc . Regression coefficient
estimates are completely robust to extreme Y valuesd . Estimates of quantiles
of Y from ordinal regression are exactly transformation-preserving, e.g., the
estimate of the median of log Y is exactly the log of the estimate of the
median Y .
For a general continuous distribution function F (y), an ordinal regression
model based on cumulative probabilities may be stated as followse . Let the
ordered unique values of Y be denoted by y1 , y2 , . . . , yk and let the intercepts
associated with y1 , . . . , yk be ↵1 , ↵2 , . . . , ↵k , where ↵1 = 1 because Prob[Y
y1 ] = 1. Let ↵y = ↵i , i : yi = y. Then
Prob[Y

yi |X] = F (↵i + X ) = F (↵yi + X )

(15.2)

For the OLS fully parametric case, the model may be restated
Prob[Y

y|X] = Prob[
=1

(

y

Y

X

X
)= (

y
y

X
+

X

]

(15.3)

)

(15.4)

so that to within an additive constantf ↵y = y (intercepts ↵ are linear in
y whereas they are arbitrarily descending in the ordinal model), and
is
absorbed in to put the OLS model into the new notation.
b

Quantile regression allows the estimated value of the 0.5 quantile to be higher than
the estimated value of the 0.6 quantile for some values of X. Composite quantile
regression687 removes this possibility by forcing all the X coefficients to be the same
across multiple quantiles, a restriction not unlike what cumulative probability ordinal
models make.
c
For symmetric distributions applying a decreasing transformation will negate the
coefficients. For asymmetric distributions (e.g., Gumbel), reversing the order of Y
will do more than change signs.
d
Only an estimate of mean Y from these ˆs is non-robust.
e
It is more traditional to state the model in terms of Prob[Y  y|X] but we use
Prob[Y
y|X] so that higher predicted values are associated with higher Y .
f
↵ˆy are unchanged if a constant is added to all y.

368 15 Regression Models for Continuous Y and Case Study in Ordinal Regression
Table 15.1 Distribution families used in ordinal cumulative probability models.
denotes the Gaussian cumulative distribution function. For the Connection column,
P1 = Prob[Y
y|X1 ], P2 = Prob[Y
y|X2 ],
= (X2
X1 ) . The connection
specifies the only distributional assumption if the model is fitted semiparametrically,
i.e, contains an intercept for every unique Y value less one. For parametric models, P1
must be specified absolutely instead of just requiring a relationship between P1 and
P2 . For example, the traditional Gaussian parametric model specifies that Prob[Y
y|X] = 1
( y X ) = ( y+X ).
Distribution

F

Inverse
(Link Function)
log( 1 y y )
1
(y)
log( log(y))

Logistic
[1 + exp( y)] 1
Gaussian
(y)
Gumbel maximum exp( exp( y))
value
Gumbel minimum 1 exp( exp(y)) log( log(1 y))
value
1
1
1
Cauchy
(y) + 12
tan[⇡(y
⇡ tan
2 )]

Link Name
logit
probit
log log

Connection
P2
1 P2

P

= 1 1P exp( )
1
1
P2 = (
(P1 ) + )
exp( )
P2 = P1

complementary 1
log log
cauchit

P2 = (1

P1 )exp(

)

The general ordinal regression model assumes that for fixed X1 , X2 ,
F

1

(Prob[Y

y|X2 ])

F

1

(Prob[Y
= (X2

y|X1 ])

(15.5)

X1 )

(15.6)

independent of the ↵s (parallelism assumption). If F = [1 + exp( y)] 1 , this
is the proportional odds assumption.
Common choices of F , implemented in the R rms orm function, are shown
in Table 15.1.
The Gumbel maximum value distribution is also called
the extreme value type I distribution. This distribution (log log link) also
represents a continuous time proportional hazards model. The hazard ratio
when X changes from X1 to X2 is exp( (X2 X1 ) ).
The mean of Y |X is easily estimated from a fitted cumulative probability
ordinal model by computing
n
X
i=1

d
yi Prob[Y
= yi |X]

(15.7)

and the q th quantile of Y |X is y such that F 1 (1 q) X ˆ = ↵
ˆ y .g
The orm function in the rms package takes advantage of the information
matrix being of a sparse tri-band diagonal form for the intercept parameters.
This makes the computations efficient even for hundreds of intercepts (i.e.,
unique values of Y ). orm is made to handle continuous Y .
g

The intercepts have to be shifted to the left one position in solving this equation
because the quantile is such that Prob[Y  y] = q whereas the model is stated in
terms of Prob[Y
y].

<- in the case of
proportional odds, F-1
would be the logit

15.4 Comparison of Assumptions of Various Models

369

Ordinal regression has nice properties in addition to those listed above,
allowing for
• estimation of quantiles as efficiently as quantile regression if the parallel
slopes assumptions hold
• efficient estimation of mean Y
• direct estimation of Prob[Y
y|X]
• arbitrary clumping of values of Y , while still estimating and mean Y
efficientlyh
• solutions for ˆ using ordinary Newton-Raphson or other popular optimization techniques
• being based on a standard likelihood function, penalized estimation can
be straightforward
• Wald, score, and likelihood ratio 2 tests that are more powerful than tests
from quantile regression.
On the last point, if there is a single predictor in the model and it is binary,
the score test from the proportional odds model is essentially the Wilcoxon
test, and the score test from the Gumbel log-log cumulative probability
model is essentially the log-rank test.

15.4 Comparison of Assumptions of Various Models
Quantile regression makes the fewest left-hand-side model assumptions except for the assumption that Y be continuous, but can have less estimator
precision than other models and has lower power. To summarize how assumptions of parametric models compare to assumptions of semiparametric
ordinal models, consider the ordinary linear model or its special case the
equal variance two-sample t-test, vs. the probit or logit (proportional odds)
ordinal model or their special cases the Van der Waerden (normal-scores)
two-sample rank test or the Wilcoxon two-sample test . All the assumptions
of the linear model other than independence of residuals are captured in the
following, using the more standard Y  y notation:
F (y|X) = Prob[Y  y|X] = P hi(
1

(F (y|X)) =

y

y

X

)

X

(15.8)
(15.9)

On the other hand, ordinal models assume the following:
Prob[Y  y|X] = F (g(y)
h

X ),

(15.10)

But it is not sensible to estimate quantiles of Y when there are heavy ties in Y in
the area containing the quantile.

370 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

Φ−1(F(y|X))

Φ−1(F(y|X))

− ∆Xβ σ

− ∆Xβ

logit(F(y|X))

y

y

Fig. 15.1 Assumptions of the linear model (left panel) and semiparametric ordinal probit or logit (proportional odds) models (right panel). Ordinal models do not
assume any shape for the distribution of Y for a given X; they only assume parallelism. The linear model can relax the parallelism assumption if is allowed to vary,
but in practice it is difficult to know how to vary it except for the unequal variance
two-sample t-test.

where g is unknown and may be discontinuous. This translates to the parallelism assumption in the right panel of Figure 15.1, whereas the linear model
makes the additional strong assumption of linearity of normal inverse cumulative distribution function, which arises from the Gaussian distribution
assumption.

15.5 Dataset and Descriptive Statistics
Diabetes Mellitus (DM) type II (adult onset diabetes) is strongly associated with obesity. The currently best laboratory test for diabetes measures
gylcosylated hemoglobin (HbA1c ), also called glycated hemoglobin, glycohemoglobin, or hemoglobin A1c . HbA1c reflects average blood glucose for the
preceding 60 to 90 days. HbA1c > 7.0 is sometimes taken as a positive diagnosis of diabetes even though there are no data to support the use of a
threshold.

15.5 Dataset and Descriptive Statistics

371

The goals of this analyses are to better understand e↵ects of body size
measurements on risk of DM and to enhance screening for DM. The best way
to develop a model for DM screening is not to fit a binary logistic model
with HbA1c > 7 as the response variable. There are at least two reasons for
this. First, when the relationship between a measurement and its ultimate
clinical impact is smooth, all cutpoints are arbitrary. There is no justification
for any putative cut on HbA1c . Second, such an analysis loses information by
treating HbA1c =2 the same as HbA1c =6.9, and by treating HbA1c =7.1 as
equal to HbA1c =10. Failure to use all available information results in larger
standard errors of ˆ, lower power, and wider confidence bands. It is better to
predict continuous HbA1c using a continuous response model, then use that
model to estimate the probability that HbA1c exceeds any cuto↵, or estimate
the 0.9 quantile of HbA1c .
The data used here are from the National Health and Nutrition Examination Survey (NHANES) 2009–2010 from the U.S. National Center for Health
Statistics/Centers for Disease Control. The original data may be obtained
from http://www.cdc.gov/nchs/nhanes.htm91 ; the analysis file used
here, called nhgh, may be obtained from the DataSets wiki page, along with
R code used to download and create the file. Note that CDC coded age
80
as 80. We use the subset of subjects with age
21 who have neither been
diagnosed nor treated for DM. Descriptive statistics are shown below.
r e q u i r e ( rms )
getHdata ( nhgh )
w
s u b s e t ( nhgh , age
21 & dx==0 & t x ==0, s e l e c t= c ( dx , t x ) )
l a t e x ( d e s c r i b e (w) , f i l e = ’ ’ )

18 Variables

w
4629 Observations

seqn : Respondent sequence number
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
4629
0 4629
1 56902 52136 52633 54284 56930 59495 61079 61641
lowest : 51624 51629 51630 51645 51647
highest: 62152 62153 62155 62157 62158

sex

n missing unique
4629
0
2

male (2259, 49%), female (2370, 51%)

age : Age [years]
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
4629
0
703
1 48.57 23.33 26.08 33.92 46.83 61.83 74.83 80.00
lowest : 21.00 21.08 21.17 21.25 21.33
highest: 79.67 79.75 79.83 79.92 80.00

372 15 Regression Models for Continuous Y and Case Study in Ordinal Regression
re : Race/Ethnicity
n missing unique
4629
0
5
Mexican American (832, 18%), Other Hispanic (474, 10%)
Non-Hispanic White (2318, 50%), Non-Hispanic Black (756, 16%)
Other Race Including Multi-Racial (249, 5%)

income : Family Income
n missing unique
4389
240
14

[0,5000) (162, 4%), [5000,10000) (216, 5%), [10000,15000) (371, 8%)
[15000,20000) (300, 7%), [20000,25000) (374, 9%)
[25000,35000) (535, 12%), [35000,45000) (421, 10%)
[45000,55000) (346, 8%), [55000,65000) (257, 6%), [65000,75000) (188, 4%)
> 20000 (149, 3%), < 20000 (52, 1%), [75000,100000) (399, 9%)
>= 100000 (619, 14%)

wt : Weight [kg]
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
4629
0
890
1 80.49 52.44 57.18 66.10 77.70 91.40 106.52 118.00
lowest : 33.2 36.1 37.9 38.5 38.7
highest: 184.3 186.9 195.3 196.6 203.0

ht : Standing Height [cm]
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
4629
0
512
1 167.5 151.1 154.4 160.1 167.2 175.0 181.0 184.8
lowest : 123.3 135.4 137.5 139.4 139.8
highest: 199.2 199.3 199.6 201.7 202.7

bmi : Body Mass Index [kg/m2 ]
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
4629
0 1994
1 28.59 20.02 21.35 24.12 27.60 31.88 36.75 40.68
lowest : 13.18 14.59 15.02 15.40 15.49
highest: 61.20 62.81 65.62 71.30 84.87

leg : Upper Leg Length [cm]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4474
155
216
1 38.39 32.0 33.5 36.0 38.4 41.0 43.3 44.6
lowest : 20.4 24.9 25.0 25.1 26.4, highest: 49.0 49.5 49.8 50.0 50.3

arml : Upper Arm Length [cm]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4502
127
156
1 37.01 32.6 33.5 35.0 37.0 39.0 40.6 41.7
lowest : 24.8 27.0 27.5 29.2 29.5, highest: 45.2 45.5 45.6 46.0 47.0

armc : Arm Circumference [cm]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4499
130
290
1 32.87 25.4 26.9 29.5 32.5 35.8 39.1 41.4
lowest : 17.9 19.0 19.3 19.5 19.9, highest: 54.2 54.9 55.3 56.0 61.0

waist : Waist Circumference [cm]
n missing unique Info Mean .05 .10 .25 .50
.75
.90
.95
4465
164
716
1 97.62 74.8 78.6 86.9 96.3 107.0 117.8 125.0
lowest : 59.7 60.0 61.5 62.0 62.4
highest: 160.0 160.6 162.2 162.7 168.7

tri : Triceps Skinfold [mm]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4295
334
342
1 18.94 7.2 8.8 12.0 18.0 25.2 31.0 33.8
lowest :

2.6

3.1

3.2

3.3

3.4, highest: 39.6 39.8 40.0 40.2 40.6

15.5 Dataset and Descriptive Statistics

373

sub : Subscapular Skinfold [mm]
n missing unique Info Mean .05
.10
.25
.50
.75
.90
.95
3974
655
329
1 20.8 8.60 10.30 14.40 20.30 26.58 32.00 35.00
lowest :

3.8

4.2

4.6

4.8

4.9, highest: 40.0 40.1 40.2 40.3 40.4

gh : Glycohemoglobin [%]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4629
0
63 0.99 5.533 4.8 5.0 5.2 5.5 5.8 6.0 6.3
lowest :

4.0

4.1

4.2

4.3

4.4, highest: 11.9 12.0 12.1 12.3 14.5

albumin : Albumin [g/dL]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4576
53
26 0.99 4.261 3.7 3.9 4.1 4.3 4.5 4.7 4.8
lowest : 2.6 2.7 3.0 3.1 3.2, highest: 4.9 5.0 5.1 5.2 5.3

bun : Blood urea nitrogen [mg/dL]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4576
53
50 0.99 13.03 7 8 10 12 15 19 22
lowest :

1

2

3

4

5, highest: 49 53 55 56 63

SCr : Creatinine [mg/dL]
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
4576
53
167
1 0.8887 0.58 0.62 0.72 0.84 0.99 1.14 1.25
lowest :
highest:

dd

0.34
5.98

0.38
6.34

0.39 0.40 0.41
9.13 10.98 15.66

d a t a d i s t (w ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )

15.5.1 Checking Assumptions of OLS and Other
Models
First let’s see if gh would make a Gaussian residuals model fit. Use ordinary
regression on four key variables to collapse these into one variable (predicted
mean from the OLS model). Stratify the predicted means into six quantile
1
groups. Apply the normal inverse cumulative distribution function
to the
empirical cumulative distribution functions (ECDF) of gh using these strata,
and check for normality and constant 2 . The ECDF estimates Prob[Y 
y|X] but for ordinal modeling we want to state models in terms of Prob[Y
y|X] so take one minus the ECDF before inverse transforming.
f
pgh
p

f u n c t i o n ( fun , row , c o l ) {
s u b s t i t u t e ( fun ) ; g
f u n c t i o n (F) e v a l ( f )
Ecdf (⇠ gh , g r o u p s=c u t 2 ( pgh , g =6) ,
f u n=f u n c t i o n (F) g ( 1
F) ,
y l a b=a s . e x p r e s s i o n ( f ) , x l i m=c ( 4 . 5 , 7 . 7 5 ) , data=w,
l a b e l . c u r v e=FALSE)
p r i n t ( z , s p l i t =c ( c o l , row , 2 , 2 ) , more=row < 2 | c o l < 2 )

f
z

}

o l s ( gh ⇠ r c s ( age , 5 ) + s e x + r e + r c s ( bmi , 3 ) , data=w)
fitted ( f )

374 15 Regression Models for Continuous Y and Case Study in Ordinal Regression
p ( l o g (F/ ( 1 F) ) ,
1 , 1)
p ( qnorm (F ) ,
1 , 2)
p ( log ( log (F ) ) ,
2 , 1)
p ( l o g ( log ( 1 F) ) , 2 , 2 )
# Get slopes of pgh for some cutoffs of Y
# Use glm complementary log-log link on Prob(Y < cutoff) to
# get log-log link on Prob(Y
cutoff)
r
NULL
for ( link in c ( ’ l o g i t ’ , ’ probit ’ , ’ cloglog ’ ))
f o r ( k in c (5 , 5 .5 , 6)) {
co
c o e f ( glm ( gh < k ⇠ pgh , data=w, f a m i l y=b i n o m i a l ( l i n k ) ) )
r
r b i n d ( r , d a t a . f r a m e ( l i n k=l i n k , c u t o f f=k ,
s l o p e=round ( co [ 2 ] , 2 ) ) )
}
p r i n t ( r , row.names=FALSE)
link cutoff slope
logit
5.0 -3.39
logit
5.5 -4.33
logit
6.0 -5.62
probit
5.0 -1.69
probit
5.5 -2.61
probit
6.0 -3.07
cloglog
5.0 -3.18
cloglog
5.5 -2.97
cloglog
6.0 -2.51

The upper right curves in Figure 15.2 are not linear, implying that a normal
conditional distribution cannot work for ghi There is non-parallelism for the
logit model. The other graphs will be used to guide selection of an ordinal
model below.

15.6 Ordinal Regression Applied to HbA1c
In the upper left panel of Figure 15.2, logit inverse curves are not parallel
so the proportional odds assumption does not hold when predicting HbA1c .
The log-log link yields the highest degree of parallelism and most constant
regression coefficients across cuto↵s of gh, so we use this link in an ordinal
regression model (linearity of the curves is not required).

15.6.1 Checking Fit for Various Models Using Age
Another way to examine model fit is to flexibly fit the single most important
predictor (age) using a variety of methods, and compare predictions to sample
quantiles and means based on subsets on age. We use overlapping subsets
to gain resolution, with each subset composed of those subjects having age
i

They are not parallel either.

5

qnorm(F)

log(F (1 − F))

15.6 Ordinal Regression Applied to HbA1c

0

375

2
0
−2

−5
5.0 5.5 6.0 6.5 7.0 7.5

5.0 5.5 6.0 6.5 7.0 7.5

Glycohemoglobin, %

Glycohemoglobin, %

log(− log(1 − F))

− log(− log(F))

2
6
4
2
0

0
−2
−4
−6

−2
5.0 5.5 6.0 6.5 7.0 7.5

5.0 5.5 6.0 6.5 7.0 7.5

Glycohemoglobin, %

Glycohemoglobin, %

Fig. 15.2 Examination of normality and constant variance assumption, and assumptions for various ordinal models

within five years of the point being predicted by the models. Here we predict
the 0.5, 0.75, and 0.9 quantiles and the mean. For quantiles we can compare
to quantile regression (discussed below) and for means we compare to OLS.
ag
25:75
lag
l e n g t h ( ag )
q2
q3
p90
means
n u m e ric ( l a g )
for ( i in 1: lag ) {
s
which ( a bs (w$ a g e
ag [ i ] ) < 5 )
y
w$ gh [ s ]
a
q u a n t i l e ( y , p r o b s=c ( . 5 , . 7 5 , . 9 ) )

376 15 Regression Models for Continuous Y and Case Study in Ordinal Regression
q2 [ i ]
q3 [ i ]
p90 [ i ]
means [ i ]

a[1]
a[2]
a[3]
mean ( y )

}
fams
c ( ’ l o g i s t i c ’ , ’ probit ’ , ’ loglog ’ , ’ cloglog ’ )
fe
f u n c t i o n ( pred , t a r g e t ) mean ( ab s ( p r e d $ yhat
target ))
mod
gh ⇠ r c s ( age , 6 )
P
Er
l i s t ()
f o r ( e s t i n c ( ’ q2 ’ , ’ q3 ’ , ’ p90 ’ , ’ mean ’ ) ) {
meth
i f ( e s t == ’ mean ’ ) ’ o l s ’ e l s e ’QR ’
p
l i s t ()
er
r e p (NA, 5 )
names ( e r )
c ( fams , meth )
f o r ( f a m i l y i n fams ) {
h
orm ( mod , f a m i l y=f a m i l y , d a t a=w)
fun
i f ( e s t == ’ mean ’ ) Mean ( h )
else {
qu
Quantile (h)
s w i t c h ( e s t , q2 = f u n c t i o n ( x ) qu ( . 5 ,
x) ,
q3 = f u n c t i o n ( x ) qu ( . 7 5 , x ) ,
p90 = f u n c t i o n ( x ) qu ( . 9 ,
x))
}
p [ [ family ] ]
z
P r e d i c t ( h , a g e=ag , f u n=fun , c o n f . i n t=FALSE)
er [ family ]
f e ( z , s w i t c h ( e s t , mean=means , q2=q2 , q3=q3 , p90=p90 ) )
}
h
switch ( est ,
mean= o l s ( mod , d a t a=w) ,
q2 = Rq ( mod , d a t a=w) ,
q3 = Rq ( mod , t a u=0. 7 5 , d a t a=w) ,
p90 = Rq ( mod , ta u=0. 9 0 , d a t a=w) )
p [ [ meth ] ]
z
P r e d i c t ( h , a g e=ag , c o n f . i n t=FALSE)
e r [ meth ]
f e ( z , s w i t c h ( e s t , mean=means , q2=q2 , q3=q3 , p90=p90 ) )
Er [ [ e s t ] ]
er
pr
d o . c a l l ( ’ rbind ’ , p)
pr $ e s t
est
P
r b i n d . d a t a . f r a m e (P , pr )
}
x y p l o t ( yhat ⇠ a g e | e s t , g r o u p s=. s e t . , d a t a=P , t y p e= ’ l ’ , # Figure 15.3
a u t o . k e y= l i s t ( x=. 7 5 , y=. 2 , p o i n t s=FALSE , l i n e s=TRUE) ,
p a n e l=f u n c t i o n ( . . . , s u b s c r i p t s ) {
p a n e l . x y p l o t ( . . . , s u b s c r i p t s=s u b s c r i p t s )
est
P$ e s t [ s u b s c r i p t s [ 1 ] ]
l p o i n t s ( ag , s w i t c h ( e s t , mean=means , q2=q2 , q3=q3 , p90=p90 ) ,
c o l=g r a y ( . 7 ) )
er
f o r m a t ( round ( Er [ [ e s t ] ] , 3 ) , n s m a l l =3)
l t e x t ( 2 6 , 6 . 1 5 , p a s t e ( names ( e r ) , c o l l a p s e= ’ \n ’ ) ,
c e x=. 7 , a d j =0)
l t e x t ( 4 0 , 6 . 1 5 , p a s t e ( e r , c o l l a p s e= ’ \n ’ ) ,
c e x=. 7 , a d j =1)})

It can be seen in Figure 15.3 that models dedicated to a specific task
(quantile regression for quantiles and OLS for means) were best for those
tasks. Although the log-log ordinal cumulative probability model did not
estimate the median as accurately as some other methods, it does well for
the 0.75 and 0.9 quantiles and is the best compromise overall because of
its ability to also directly predict the mean as well as quantities such as
Prob[HbA1c > 7|X].
From here on we focus on the log-log ordinal model. Returning to the
bottom left of Figure 15.2, let’s look at quantile groups of predicted HbA1c

15.6 Ordinal Regression Applied to HbA1c

377

30

40

50

q2

60

70

q3

logistic 0.048

logistic 0.050

probit

probit

0.052

0.045

loglog 0.058

loglog 0.037

cloglog 0.072

cloglog 0.077

QR

QR

0.024

6.2
●●●●●●●

0.027

●●●●●●●●●

5.8

●●●●●●●●●
●
●●●●●●●

●●●●●●●●●●●●●●
● ●●●●●●●●●

5.6

●●●●●●

●●●●●● ●

6.0

●●

●●●●●●●●

●●●●●●●●●

5.4

●●

●●●●●●●

5.2

yhat

●●●●

mean
6.2
6.0

p90

logistic 0.021

logistic 0.053

probit

probit

0.025

loglog 0.026

loglog 0.041

cloglog 0.033

cloglog 0.101

ols

QR

0.013

●●●●●●●●●●
●
●● ●●
●

0.047

● ●●●●●●
●●●●●●●● ●●

0.030 ●
●●●●

5.8
5.6
5.4
5.2

●●
● ●●● ●●●●●●
● ●●
●●
●
●●●●
●●●
●●●●
●
●●●●
●●
●●
●
●
●
●●
●
●
●
●●
●●●

30

40

50

60

●●●●●
●●●●●●●
●

logistic
probit
loglog
cloglog
QR
ols

70

age
Fig. 15.3 Three estimated quantiles and estimated mean using 6 methods, compared
against caliper-matched sample quantiles/means (circles). Numbers are mean absolute di↵erences between predicted and sample quantities using overlapping intervals
of age and caliper matching. QR:quantile regression.

loglog is the best! It is great at estimating quantiles
and it is capable of estimating the mean.

378 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

by OLS and plot predicted distributions of actual HbA1c against empirical
distributions.
w$ pghg
c u t 2 ( pgh , g=6)
f
orm ( gh ⇠ pghg , data=w)
lp
p r e d i c t ( f , newdata=d a t a . f r a m e ( pghg=l e v e l s (w$ pghg ) ) )
ep
ExProb ( f ) # Exceedance prob. functn. generator in rms
z
ep ( l p )
j
o r d e r (w$ pghg ) # puts in order of lp (levels of pghg)
p l o t ( z , x l i m=c ( 4 , 7 . 5 ) , data=w [ j , c ( ’ pghg ’ , ’ gh ’ ) ] ) # Fig. 15.4

1.0

Prob(Y ≥ y)

0.8

●●
●●
●●
●
●
●
●
●
●
●●
●
●
● ●
●
●●
●
●
●●
● ●
●
●●
●●
● ●
● ● ●
●
● ●●
●●
●●
●●●
●
●
● ●
●
●●
●●
●
● ●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●

0.6

●
●

●
●
●
●

●
●

0.4

[4.88,5.29)
[5.29,5.44)
[5.44,5.56)
[5.56,5.66)
[5.66,5.76)
[5.76,6.48]

●
●
●
●
●
●
●
●

●
●

●
●
●●
●
●●
●

●
●

0.2

●

●

0.0
4.0

4.5

5.0

●
●

●
●

● ●●
●● ●
●
●

●
●

●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●●●
●
●
●●
●●
●
●●
●
●●
● ●
●
●●
●●
●
●
●
●
●●●
● ●
●●
●●●●
●●
● ● ● ●●
●●
●
●
●●
●
●●
● ● ●●
●
●●●
●
●
●
●
●
●
●●
● ●
●●
●
●
●
●
● ●
●●
● ●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●●
● ●
● ●
● ●
● ●
●●
● ●
● ●
● ●
●●
●●
●●
●●
●●
●●
●

●

5.5

6.0

6.5

7.0

7.5

gh
Fig. 15.4 Observed (dashed lines, open circles) and predicted (solid lines, closed circles) exceedance probability distributions from a model using 6-tiles of OLS-predicted
HbA1c . Key shows quantile group intervals of predicted mean HbA1c .

Agreement between predicted and observed exceedance probability distributions is excellent in Figure 15.4.
To return to the initial look at a linear model with assumed Gaussian
residuals, fit a probit ordinal model and compare the estimated intercepts to
the linear relationship with gh that is assumed by the normal distribution.
f

orm ( gh ⇠ r c s ( age , 6 ) , f a m i l y=p r o b i t , data=w)

15.6 Ordinal Regression Applied to HbA1c

379

g
o l s ( gh ⇠ r c s ( age , 6 ) , data=w)
s
g $ s t a t s [ ’ Sigma ’ ]
yu
f $ y u niq ue [ 1 ]
r
q u a n t i l e (w$gh , c ( . 0 0 5 , . 9 9 5 ) )
alphas
coef ( f ) [ 1 : num.intercepts ( f ) ]
p l o t ( yu / s , a l p h a s , t y p e= ’ l ’ , x l i m=r e v ( r / s ) , # Fig. 15.5
x l a b=e x p r e s s i o n ( y/ hat ( sigma ) ) , y l a b=e x p r e s s i o n ( a l p h a [ y ] ) )

2
1
0

αy

−1
−2
−3
−4
−5
−14

−12

−10

−8

^
−y σ
Fig. 15.5 Estimated intercepts from probit model. Linearity would have indicated
Gaussian residuals.

Figure 15.5 depicts a significant departure from the linear form implied by
Gaussian residuals (Eq. 15.4).

15.6.2 Examination of BMI
Body mass index (BMI, weight divided by height2 ) is commonly used as an
obesity measure because it is well correlated with abdominal visceral fat.
But it is not obvious that BMI is the correct summary of height and weight
for predicting pre-clinical diabetes, and it may be the case that body size
measures other than height and weight are better predictors.
Use the log-log ordinal model to check the adequacy of BMI, adjusting for
age (without assuming linearity). This can be done by examining the ratio
of coefficients of log height and log weight, and also by using AIC to judge
whether BMI is an adequate summary of height and weight when compared
to nonlinear functions of the logs, and to a tensor spline interaction surface.
orm ( gh ⇠ r c s ( age , 5 ) + l o g ( ht ) + l o g ( wt ) ,
f a m i l y=l o g l o g , d a t a=w)
p r i n t ( f , l a t e x=TRUE)
f

380 15 Regression Models for Continuous Y and Case Study in Ordinal Regression
-log-log Ordinal Regression Model
orm(formula = gh ˜ rcs(age, 5) + log(ht) + log(wt), data = w,
family = loglog)
Model Likelihood
Ratio Test
Obs
4629 LR 2
1126.94
Unique Y
63 d.f.
6
Y0.5
5.5 Pr(> 2 ) < 0.0001
L
max | @ log
| 1⇥10 6 Score 2 1262.81
@
Pr(> 2 ) < 0.0001

Discrimination
Rank Discrim.
Indexes
Indexes
R2
0.217 ⇢
0.486
g
0.627
gr
1.872
|Pr(Y
Y0.5 ) 12 | 0.153

Coef
S.E. Wald Z
age
0.0398 0.0055
7.29
age’ -0.0158 0.0275
-0.57
age” -0.0072 0.0866
-0.08
age”’ 0.0309 0.1135
0.27
ht
-3.0680 0.2789 -11.00
wt
1.2748 0.0704 18.10

aic
NULL
f o r (mod i n l i s t ( gh ⇠ r c s ( age
gh ⇠ r c s ( age
gh ⇠ r c s ( age
aic
c ( a i c , AIC ( orm ( mod ,
print ( aic )

Pr(> |Z|)
< 0.0001
0.5657
0.9333
0.7853
< 0.0001
< 0.0001

, 5 ) + r c s ( l o g ( bmi ) , 5 ) ,
, 5 ) + r c s ( l o g ( ht ) , 5 ) + r c s ( l o g ( wt ) , 5 ) ,
, 5 ) + r c s ( l o g ( ht ) , 4 ) ⇤ r c s ( l o g ( wt ) , 4 ) ) )
f a m i l y=l o g l o g , d a t a=w ) ) )

[1] 25910.77 25910.17 25906.03

The ratio of the coefficient of log height to the coefficient of log weight is 2.4, which is between what BMI uses and the more dimensionally reasonable
weight / height3 . By AIC, a spline interaction surface between height and
weight does slightly better than BMI in predicting HbA1c , but a nonlinear
function of BMI is barely worse. It will require other body size measures to
displace BMI as a predictor.
As an aside, compare this model fit to that from the Cox proportional
hazards model. The Cox model uses a conditioning argument to obtain
a partial likelihood free of the intercepts ↵ (and requires a second step to
estimate these log discrete hazard components) whereas we are using a full
marginal likelihood of the ranks of Y 323 .
p r i n t ( cph ( Surv ( gh ) ⇠ r c s ( age , 5 ) + l o g ( ht ) + l o g ( wt ) , d a t a=w) ,
l a t e x=TRUE)

Cox Proportional Hazards Model
cph(formula = Surv(gh) ˜ rcs(age, 5) + log(ht) + log(wt), data = w)

15.6 Ordinal Regression Applied to HbA1c
Model Tests
Obs
4629 LR 2 1120.20
Events 4629 d.f.
6
Center 8.3792 Pr(> 2 ) 0.0000
Score 2 1258.07
Pr(> 2 ) 0.0000

381
Discrimination
Indexes
R2
0.215
Dxy
0.359
g
0.622
gr
1.863

Coef
S.E. Wald Z
age
-0.0392 0.0054
-7.24
age’
0.0148 0.0274
0.54
age” 0.0093 0.0862
0.11
age”’ -0.0321 0.1131
-0.28
ht
3.0477 0.2779 10.97
wt
-1.2653 0.0701 -18.04

Pr(> |Z|)
< 0.0001
0.5888
0.9144
0.7767
< 0.0001
< 0.0001

Close agreement of the two is seen, as expected.

15.6.3 Consideration of All Body Size Measurements
Next we examine all body size measures, and check their redundancies.
v

v a r c l u s (⇠ wt + ht +
t r i + sub +
plot (v)
# Omit wt so it won’t be
redun (⇠ ht + bmi + l e g +
data=w, r 2=. 7 5 )

bmi + l e g + arml + armc + w a i s t +
age + s e x + re , data=w)
removed before bmi
arml + armc + w a i s t + t r i + sub ,

Redundancy Analysis
redun(formula = ⇠ht + bmi + leg + arml + armc + waist + tri +
sub, data = w, r2 = 0.75)
n: 3853

p: 8

nk: 3

Number of NAs:
776
Frequencies of Missing Values Due to Each Variable
ht
bmi
leg arml armc waist
tri
sub
0
0
155
127
130
164
334
655
Transformation of target variables forced to be linear
R2 cutoff: 0.75

Type: ordinary

R2 with which each variable can be predicted from all other variables:
ht
bmi
leg arml armc waist
tri
sub
0.829 0.924 0.682 0.748 0.843 0.864 0.531 0.594
Rendundant variables:
bmi ht

382 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

Predicted from variables:
leg arml armc waist tri sub
1
2

Variable Deleted
R2 R2 after later deletions
bmi 0.924
0.909
ht 0.792

tri
sub
sexfemale
leg

0.4

ht
arml

0.6
0.8
1.0

bmi
waist
wt
armc

Spearman ρ2

0.2

age
reOther Race Including Multi−Racial
reOther Hispanic
reNon−Hispanic White
reNon−Hispanic Black

0.0

Fig. 15.6 Variable clustering for all potential predictors

Six size measures adequately capture the entire set. Height and BMI are
removed. An advantage of removing height is that it is age-dependent due to
vertebral compression in the elderly:
f
orm ( ht ⇠ r c s ( age , 4 ) ⇤ sex , data=w) # Prop. odds model
qu
Q u a n t i l e ( f ) ; med
f u n c t i o n ( x ) qu ( . 5 , x )
g g p l o t ( P r e d i c t ( f , age , sex , f u n=med , c o n f . i n t=FALSE) ,
y l a b= ’ P r e d i c t e d Median Height , cm ’ )

In preparing to create a multivariable model, degrees of freedom are allocated according to the generalized Spearman ⇢2j .
spearman2 ( gh ⇠ age + s e x + r e + wt + l e g + arml + armc +
w a i s t + t r i + sub , data=w, p=2)
plot ( s )

s

j

Competition between collinear size measures hurts interpretation of partial tests of
association in a saturated additive model.

when you are normalizing, show that they thing you are normalizing
for doesn't depend on things it shouldn't depend on. So when we
normalize by height (as in the BMI case) you want to show that the
height doesn't relate to things other than weight... especially not
age once you control for the sex of the person. One good
normalizer could be the femur. So maybe leg length would be
better because it doesn't change with age.

Predicted Median Height, cm

15.6 Ordinal Regression Applied to HbA1c

383

180
175
sex

170

male
female

165
160
155
20

40

60

80

Age, years
Fig. 15.7 Estimated median height as a smooth function of age, allowing age to
interact with sex, from a proportional odds model

Spearman ρ2 Response : gh
age
waist
leg
sub
armc
wt
re
tri
arml
sex

●
●
●
●
●
●
●
●
●
●

0.00

0.05

0.10

Adjusted ρ
Fig. 15.8 Generalized squared rank correlations

0.15
2

0.20

N df
4629 2
4465 2
4474 2
3974 2
4499 2
4629 2
4629 4
4295 2
4502 2
4629 1

384 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

Parameters will be allocated in descending order of ⇢2 . But note that
subscapular skinfold has a large number of NAs and other predictors also
have NAs. Suboptimal casewise deletion will be used until the final model is
fitted.
Because there are many competing body measures, we use backwards stepdown to arrive at a set of predictors. The bootstrap will be used to penalize
predictive ability for variable selection. First the full model is fit using casewise deletion, then we do a composite test to assess whether any of the
frequently–missing predictors is important.
orm ( gh ⇠ r c s ( age , 5 ) + s e x + r e + r c s ( wt , 3 ) + r c s ( l e g , 3 ) + arml +
r c s ( armc , 3 ) + r c s ( w a i s t , 4 ) + t r i + r c s ( sub , 3 ) ,
f a m i l y= ’ l o g l o g ’ , d a t a=w, x=TRUE, y=TRUE)
p r i n t ( f , l a t e x=TRUE, c o e f s=FALSE)
f

-log-log Ordinal Regression Model
orm(formula = gh ˜ rcs(age, 5) + sex + re + rcs(wt, 3) + rcs(leg,
3) + arml + rcs(armc, 3) + rcs(waist, 4) + tri + rcs(sub,
3), data = w, x = TRUE, y = TRUE, family = "loglog")
Frequencies of Missing Values Due to Each Variable
N
sub
tri
waist
leg
armc
arml
gh
age
sex
re
wt

t
t
t
t
t

0

t
t

100

t
t

200

t

t

300

Model Likelihood
Ratio Test
Obs
3853 LR 2
1180.13
Unique Y
60 d.f.
22
Y0.5
5.5 Pr(> 2 ) < 0.0001
L
max | @ log
| 3⇥10 5 Score 2 1298.88
@
Pr(> 2 ) < 0.0001

400

500

600

655
334
164
155
130
127
0
0
0
0
0

700

Discrimination
Rank Discrim.
Indexes
Indexes
R2
0.265 ⇢
0.520
g
0.732
gr
2.080
|Pr(Y
Y0.5 ) 12 | 0.172

## Composite test :
lan
f u n c t i o n ( a ) l a t e x ( a , t a b l e . e n v=FALSE , f i l e = ’ ’ )
l a n ( anova ( f , l e g , arml , armc , w a i s t , t r i , sub ) )

15.6 Ordinal Regression Applied to HbA1c

385
2

leg
Nonlinear
arml
armc
Nonlinear
waist
Nonlinear
tri
sub
Nonlinear
TOTAL NONLINEAR
TOTAL

d.f.
8.30 2
3.32 1
0.16 1
6.66 2
3.29 1
29.40 3
4.29 2
16.62 1
40.75 2
4.50 1
14.95 5
128.29 11

P
0.0158
0.0685
0.6924
0.0358
0.0695
< 0.0001
0.1171
< 0.0001
< 0.0001
0.0340
0.0106
< 0.0001

The model achieves Spearman ⇢ = 0.52, the rank correlation between
predicted and observed HbA1c .
We show the predicted mean and median HbA1c as a function of age,
adjusting other variables to their median or mode. Compare the estimate of
the median and 90th percentile with that from quantile regression.
M
qu
med
p90
fq
fq90

Mean ( f )
Quantile ( f )
f u n c t i o n ( x ) qu ( . 5 , x )
f u n c t i o n ( x ) qu ( . 9 , x )
Rq( f o r m u l a ( f ) , data=w)
Rq( f o r m u l a ( f ) , data=w, tau=. 9 )

pmean
Predict ( f ,
age , f u n=M,
c o n f . i n t=FALSE)
pmed
Predict ( f ,
age , f u n=med , c o n f . i n t=FALSE)
p90
Predict ( f ,
age , f u n=p90 , c o n f . i n t=FALSE)
pmedqr
P r e d i c t ( fq ,
age , c o n f . i n t=FALSE)
p90qr
P r e d i c t ( f q 9 0 , age , c o n f . i n t=FALSE)
z
r b i n d ( ’ orm mean ’=pmean , ’ orm median ’=pmed , ’ orm P90 ’=p90 ,
’QR median ’=pmedqr , ’QR P90 ’=p90qr )
g g p l o t ( z , g r o u p s= ’ . s e t . ’ ,
a d j . s u b t i t l e=FALSE, l e g e n d . l a b e l=FALSE)
p r i n t ( f a s t b w ( f , r u l e= ’ p ’ ) , e s t i m a t e s=FALSE)
Deleted
arml
sex
wt
armc

Chi-Sq
0.16
0.45
5.72
3.32

d.f.
1
1
2
2

P
0.6924
0.5019
0.0572
0.1897

Residual
0.16
0.61
6.33
9.65

d.f.
1
2
4
6

P
0.6924
0.7381
0.1759
0.1400

AIC
-1.84
-3.39
-1.67
-2.35

Factors in Final Model
[1] age

re

leg

waist tri

sub

s e t . s e e d ( 1 3 ) # so can reproduce results
v
v a l i d a t e ( f , B=100 , bw=TRUE, e s t i m a t e s=FALSE, r u l e= ’ p ’ )
Backwards Step-down - Original Model

386 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

6.00

5.75

orm mean
orm median
orm P90

5.50

QR median
QR P90

5.25

5.00
20

40

60

80

Age, years
Fig. 15.9 Estimated mean and 0.5 and 0.9 quantiles from the log-log ordinal model
using casewise deletion, along with predictions of 0.5 and 0.9 quantiles from quantile
regression (QR). Age is varied and other predictors are held constant to medians/modes.

Deleted
arml
sex
wt
armc

Chi-Sq
0.16
0.45
5.72
3.32

d.f.
1
1
2
2

P
0.6924
0.5019
0.0572
0.1897

Residual
0.16
0.61
6.33
9.65

d.f.
1
2
4
6

P
0.6924
0.7381
0.1759
0.1400

AIC
-1.84
-3.39
-1.67
-2.35

Factors in Final Model
[1] age

re

leg

waist tri

sub

# Show number of variables selected in first 30 boots
l a t e x ( v , B=30 , f i l e = ’ ’ , s i z e= ’ s m a l l ’ )

15.6 Ordinal Regression Applied to HbA1c
Index
⇢
R2
Slope
g
|Pr(Y

Y0.5 )

387

Original Training Test Optimism Corrected n
Sample Sample Sample
Index
0.5225 0.5290 0.5208
0.0083
0.5142 100
0.2712 0.2788 0.2692
0.0095
0.2617 100
1.0000 1.0000 0.9761
0.0239
0.9761 100
1.2276 1.2505 1.2207
0.0298
1.1978 100
1
| 0.2007 0.2050 0.1987
0.0064
0.1943 100
2

Factors Retained in Backwards Elimination
First 30 Resamples
age sex re wt leg arml armc waist tri sub
• • • • •
•
•
•
•
•
•
•
•
• •
•
•
•
•
•
• •
• • • • •
•
•
•
• • • • •
•
•
•
•
•
•
•
•
• •
• • • •
•
•
•
• • • • •
•
•
•
• • •
•
•
• •
• • • • •
•
•
•
•
• •
•
•
• •
•
•
• •
•
•
• •
•
• • •
•
•
• •
•
• • •
•
•
• •
•
•
•
•
•
• •
•
• • •
•
•
• •
• • • • •
•
•
• •
•
•
•
•
• •
•
• • •
•
•
• •
•
• • •
•
•
• •
•
• • •
•
•
• •
•
• •
•
•
• •
•
• • •
•
•
• •
• • •
•
•
•
•
•
•
•
• •
•
• • •
•
•
• •
•
• •
•
•
• •
•
• • •
•
•
• •
•
• • •
•
•
• •
•
•
•
•
• •
Frequencies of Numbers of Factors Retained
5 6 7 8 9 10
1 19 29 46 4 1

Next we fit the reduced model, using multiple imputation to impute missing predictors.
a
g

a r e g I m p u t e (⇠ gh + wt + ht + bmi + l e g + arml + armc + w a i s t +
t r i + sub + a g e +r e , d a t a=w, n . i m p u t e =5 , pr=FALSE)
f i t . m u l t . i m p u t e ( gh ⇠ r c s ( age , 5 ) + r e + r c s ( l e g , 3 ) +
r c s ( w a i s t , 4 ) + t r i + r c s ( sub , 4 ) ,
orm , a , f a m i l y=l o g l o g , d a t a=w, pr=FALSE)

388 15 Regression Models for Continuous Y and Case Study in Ordinal Regression
print (g ,

l a t e x=TRUE, n e e d s p a c e= ’ 1 . 5 i n ’ )

-log-log Ordinal Regression Model
fit.mult.impute(formula = gh ˜ rcs(age, 5) + re + rcs(leg, 3) +
rcs(waist, 4) + tri + rcs(sub, 4), fitter = orm, xtrans = a,
data = w, pr = FALSE, family = loglog)
Model Likelihood
Ratio Test
Obs
4629 LR 2
1448.42
Unique Y
63 d.f.
17
Y0.5
5.5 Pr(> 2 ) < 0.0001
L
max | @ log
| 1⇥10 5 Score 2 1569.21
@
Pr(> 2 ) < 0.0001

Discrimination
Rank Discrim.
Indexes
Indexes
R2
0.269 ⇢
0.513
g
0.743
gr
2.102
|Pr(Y
Y0.5 ) 12 | 0.173

Coef
S.E. Wald Z
age
0.0404 0.0055
7.29
age’
-0.0228 0.0279
-0.82
age”
0.0126 0.0876
0.14
age”’
0.0424 0.1148
0.37
re=Other Hispanic
-0.0766 0.0597
-1.28
re=Non-Hispanic White
-0.4121 0.0449
-9.17
re=Non-Hispanic Black
0.0645 0.0566
1.14
re=Other Race Including Multi-Racial -0.0555 0.0750
-0.74
leg
-0.0339 0.0091
-3.73
leg’
0.0153 0.0105
1.46
waist
0.0073 0.0050
1.47
waist’
0.0304 0.0158
1.93
waist”
-0.0910 0.0508
-1.79
tri
-0.0163 0.0026
-6.28
sub
-0.0027 0.0097
-0.28
sub’
0.0674 0.0289
2.33
sub”
-0.1895 0.0922
-2.06
an
anova ( g )
l a n ( an )
2

age
Nonlinear
re
leg
Nonlinear
waist
Nonlinear
tri
sub
Nonlinear
TOTAL NONLINEAR
TOTAL

d.f.
692.50 4
28.47 3
168.91 4
24.37 2
2.14 1
128.31 3
4.05 2
39.44 1
39.30 3
6.63 2
46.80 8
1464.24 17

P
< 0.0001
< 0.0001
< 0.0001
< 0.0001
0.1434
< 0.0001
0.1318
< 0.0001
< 0.0001
0.0363
< 0.0001
< 0.0001

Pr(> |Z|)
< 0.0001
0.4137
0.8857
0.7116
0.1992
< 0.0001
0.2543
0.4593
0.0002
0.1434
0.1428
0.0536
0.0732
< 0.0001
0.7817
0.0198
0.0398

15.6 Ordinal Regression Applied to HbA1c

389

b
anova ( g , l e g , w a i s t , t r i , sub )
# Add new lines to the plot with combined effect of 4 size var.
s
r b i n d ( an , s i z e=b [ ’TOTAL ’ , ] )
class (s)
’ anova.rms ’
plot ( s )

leg

●

sub

●

tri

●

waist

●

re

●

size

●

age

●

0

100

200

300

400

500

600

700

2

χ − df
Fig. 15.10 ANOVA for reduced model, after multiple imputation, with addition of
a combined e↵ect for four size variables

g g p l o t ( P r e d i c t ( g ) , a b b r e v=TRUE, y l a b=NULL)

Compare the estimated age partial e↵ects and confidence intervals with
those from a model using casewise deletion, and with bootstrap nonparametric confidence intervals (also with casewise deletion).
gc

gb

orm ( gh ⇠ r c s ( age , 5 ) + r e + r c s ( l e g , 3 ) +
r c s ( w a i s t , 4 ) + t r i + r c s ( sub , 4 ) ,
f a m i l y=l o g l o g , data=w, x=TRUE, y=TRUE)
b o o t c o v ( gc , B=300)

bootclb

P r e d i c t ( gb , age , b o o t . t y p e= ’ b a s i c ’ )

bootclp
P r e d i c t ( gb , age , b o o t . t y p e= ’ p e r c e n t i l e ’ )
multimp
Predict (g ,
age )
p l o t ( P r e d i c t ( gc , age ) , a d d p a n e l=f u n c t i o n ( . . . ) {
with ( b o o t c l b , { l l i n e s ( age , l o w e r , c o l= ’ b l u e ’ )
l l i n e s ( age , upper , c o l= ’ b l u e ’ ) } )
with ( b o o t c l p , { l l i n e s ( age , l o w e r , c o l= ’ b l u e ’ , l t y =2)
l l i n e s ( age , upper , c o l= ’ b l u e ’ , l t y =2)})
with ( multimp , { l l i n e s ( age , l o w e r , c o l= ’ r e d ’ )

390 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

1.5

1.0

1.0

0.5

0.5

0.0

0.0

−0.5

−0.5

−1.0

ORIM−R

Race Ethnicity

1.5

−1.0
20

40

60

80

Age, years

1.5

30
1.5

35

40

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

−0.5

−0.5

−0.5

−1.0
10

20

30

40

Subscapular Skinfold, mm

10

20

30

Triceps Skinfold, mm

●

Nn−HsW

●

OthrHs

●

●

−1.0−0.50.0 0.5 1.0 1.5

1.5

1.0

−1.0

Nn−HsB

MxcnAm

45

Upper Leg Length, cm

●

−1.0
40

80

100

Fig. 15.11 Partial e↵ects (log hazard or log-log cumulative probability scale) of all
predictors in reduced model, after multiple imputation

l l i n e s ( age , upper , c o l= ’ r e d ’ )
l l i n e s ( age , yhat , c o l= ’ r e d ’ ) } ) } ,
c o l . f i l l =g r a y ( . 9 ) , a d j . s u b t i t l e=FALSE)
# Figure 15.12

Figure 15.13 depicts the relationship between various predicted quantities,
demonstrating that the ordinal model makes fewer model assumptions that
dictate their connections. A Gaussian or log-Gaussian model would have a
straight-line relationship between the predicted mean and median.
M
qu
med
q90
lp
lpr
lps
pmn

120

140

Waist Circumference, cm

Mean ( g )
Quantile ( g )
f u n c t i o n ( l p ) qu ( . 5 , l p )
f u n c t i o n ( l p ) qu ( . 9 , l p )
predict (g)
q u a n t i l e ( p r e d i c t ( g ) , c ( . 0 0 2 , . 9 9 8 ) , na.rm=TRUE)
s e q ( l p r [ 1 ] , l p r [ 2 ] , l e n g t h =200)
M( l p s )

15.6 Ordinal Regression Applied to HbA1c

391

log hazard

1.0

0.5

0.0

−0.5

20

30

40

50

60

70

80

Age, years
Fig. 15.12 Partial e↵ect for age from multiple imputation (center red line) and
casewise deletion (center blue line) with symmetric Wald 0.95 confidence bands using
casewise deletion (gray shaded area), basic bootstrap confidence bands using casewise
deletion (blue lines), percentile bootstrap confidence bands using casewise deletion
(dashed blue lines), and symmetric Wald confidence bands accounting for multiple
imputation (red lines).

pme
med ( l p s )
p90
q90 ( l p s )
p l o t (pmn , pme ,
# Figure 15.13
x l a b=e x p r e s s i o n ( p a s t e ( ’ P r e d i c t e d Mean ’ , HbA [ ” 1 c ” ] ) ) ,
y l a b= ’ Median and 0 . 9 Q u a n t i l e ’ , t y p e= ’ l ’ ,
x l i m=c ( 4 . 7 5 , 8 . 0 ) , y l i m=c ( 4 . 7 5 , 8 . 0 ) , bty= ’ n ’ )
box ( c o l=g r a y ( . 8 ) )
l i n e s (pmn , p90 , c o l= ’ b l u e ’ )
a b l i n e ( a =0 , b=1 , c o l=g r a y ( . 8 ) )
t e x t ( 6 . 5 , 5 . 5 , ’ Median ’ )
t e x t ( 5 . 5 , 6 . 3 , ’ 0 . 9 ’ , c o l= ’ b l u e ’ )
nint
350
s c a t 1 d (M( l p ) ,
n i n t=n i n t )
s c a t 1 d ( med ( l p ) , s i d e =2 , n i n t=n i n t )
s c a t 1 d ( q90 ( l p ) , s i d e =4 , c o l= ’ b l u e ’ , n i n t=n i n t )

392 15 Regression Models for Continuous Y and Case Study in Ordinal Regression

Median and 0.9 Quantile

8.0

7.5

7.0

6.5

0.9
6.0

Median

5.5

5.0

5.0

5.5

6.0

6.5

7.0

7.5

8.0

Predicted Mean HbA1c
Fig. 15.13 Predicted mean HbA1c vs. predicted median and 0.9 quantile along with
their marginal distributions

Finally, let us draw a nomogram that shows the full power of ordinal
models, by predicting five quantities of interest.
g
N e w l e v e l s ( g , l i s t ( r e=a b b r e v i a t e ( l e v e l s (w$ r e ) ) ) )
exprob
ExProb ( g )
nom
nomogram ( g , f u n= l i s t ( Mean=M,
’ Median Glycohemoglobin ’ = med ,
’ 0 .9 Quantile ’
= q90 ,
’ Prob ( HbA1c
6 . 5 ) ’=
f u n c t i o n ( x ) exprob ( x , y=6 . 5 ) ,
’ Prob ( HbA1c
7 . 0 ) ’=
f u n c t i o n ( x ) exprob ( x , y =7) ,
’ Prob ( HbA1c
7 . 5 ) ’=
f u n c t i o n ( x ) exprob ( x , y=7 . 5 ) ) ,
f u n . a t= l i s t ( s e q ( 5 , 8 , by=. 5 ) ,
c (5 ,5 .25 , 5 .5 , 5 .75 , 6 , 6 .25 ) ,
c (5 .5 , 6 , 6 .5 , 7 , 8 , 1 0 , 1 2 , 1 4 ) ,
c ( .01 , .05 , .1 , .2 , .3 , . 4 ) ,
c ( .01 , .05 , .1 , .2 , .3 , . 4 ) ,
c ( .01 , .05 , .1 , .2 , .3 , . 4 ) ) )
p l o t (nom , lmgp=. 2 8 )
# Figure 15.14

15.6 Ordinal Regression Applied to HbA1c
0

10

20

393

30

40

50

60

70

80

90

100

Points
Age
20

25

30
OthH

35

40

45

50

55

60

65 70 75

80

Race/Ethnicity
N−HW

ORIM

Upper Leg Length
55

45

35

30

25

20

Waist Circumference
50

70

90

100

110

120

130

140

150

160

170

Triceps Skinfold
45
35
25
15 20
25
30

15
5
35 40 45

0

Subscapular Skinfold
10

Total Points
0

20

40

60

80

100

120

140

160

180

200

220

240

260

Linear Predictor
−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

Mean
5

5.5

6

6.5

7 7.5 8

Median Glycohemoglobin
5

5.25

5.5

5.75

6

6.25

0.9 Quantile
5.5

6

6.5 7 8 10 12

14

Prob(HbA1c >= 6.5)
0.01

0.05 0.1

0.2

0.3 0.4

Prob(HbA1c >= 7.0)
0.01

0.05 0.1

0.2

0.3 0.4

Prob(HbA1c >= 7.5)
0.01

0.05 0.1

0.2

Fig. 15.14 Nomogram for predicting median, mean, and 0.9 quantile of glycohemoglobin, along with the estimated probability that HbA1c 6.5, 7, or 7.5, all from
the log-log ordinal model

0.3

280

Chapter 16

Transform-Both-Sides Regression

16.1 Background
Fitting multiple regression models by the method of least squares is one of the
most commonly used methods in statistics. There are a number of challenges
to the use of least squares, even when it is only used for estimation and not
inference, including the following.
1. How should continuous predictors be transformed so as to get a good fit?
2. Is it better to transform the response variable? How does one find a good
transformation that simplifies the right-hand side of the equation?
3. What if Y needs to be transformed non-monotonically (e.g., |Y
100|)
before it will have any correlation with X?
When one is trying to draw an inference about population e↵ects using confidence limits or hypothesis tests, the most common approach is to assume
that the residuals have a normal distribution. This is equivalent to assuming
that the conditional distribution of the response Y given the set of predictors
X is normal with mean depending on X and variance that is (one hopes)
a constant independent of X. The need for a distributional assumption to
enable us to draw inferences creates a number of other challenges such as the
following.
1. If for the untransformed original scale of the response Y the distribution of
the residuals is not normal with constant spread, ordinary methods will not
yield correct inferences (e.g., confidence intervals will not have the desired
coverage probability and the intervals will need to be asymmetric).
2. Quite often there is a transformation of Y that will yield well-behaving
residuals. How do you find this transformation? Can you find a transformation for the Xs at the same time?
3. All classical statistical inferential methods assume that the full model was
pre-specified, that is, the model was not modified after examining the data.

395

396

16 Transform-Both-Sides Regression

How does one correct confidence limits, for example, for data-based model
and transformation selection?

16.2 Generalized Additive Models
Hastie and Tibshirani270 have developed generalized additive models (GAMs)
for a variety of distributions for Y . There are semiparametric GAMs, but
most GAMs for continuous Y assume that the conditional distribution of Y is
from a specific distribution family. GAMs nicely estimate the transformation
each continuous X requires so as to optimize a fitting criterion such as sum
of squared errors or log likelihood, subject to the degrees of freedom the
analyst desires to spend on each predictor. However, GAMs assume that Y
has already been transformed to fit the specified distribution family.
There is excellent software available for fitting a wide variety of GAMs,
such as the R packages gam, mgcv, and robustgam. .

16.3 Nonparametric Estimation of Y -Transformation
When the model’s left-hand side also needs transformation, either to improve R2 or to achieve constant variance of the residuals (which increases the
chances of satisfying a normality assumption), there are a few approaches
available. One approach is Breiman and Friedman’s alternating conditional
expectation (ACE) method.67 ACE simultaneously transforms both Y and
each of the Xs so as to maximize the multiple R2 between the transformed
Y and the transformed Xs. The model is given by
g(Y ) = f1 (X1 ) + f2 (X2 ) + . . . + fp (Xp ).

(16.1)

ACE allows the analyst to impose restrictions on the transformations such as
monotonicity. It allows for categorical predictors, whose categories will automatically be given numeric scores. The transformation for Y is allowed to
be non-monotonic. One feature of ACE is its ability to estimate the maximal
correlation between an X and the response Y . Unlike the ordinary correlation
coefficient (which assumes linearity) or Spearman’s rank correlation (which
assumes monotonicity), the maximal correlation has the property that it is
zero if and only if X and Y are statistically independent. This property holds
because ACE allows for non-monotonic transformations of all variables. The
“super smoother” (see the S supsmu function) is the basis for the nonparametric estimation of transformations for continuous Xs.
Tibshirani developed a di↵erent algorithm for nonparametric additive regression based on least squares, additivity and variance stabilization (AVAS).600

16.4 Obtaining Estimates on the Original Scale

397

Unlike ACE, AVAS forces g(Y ) to be monotonic. AVAS’s fitting criterion is
to maximize R2 while forcing the transformation for Y to result in nearly
constant variance of residuals. The model specification is the same as for
ACE (Equation 16.3).
ACE and AVAS are powerful fitting algorithms, but they can result in overfitting (R2 can be greatly inflated when one fits many predictors), and they
provide no statistical inferential measures. As discussed earlier, the process of
estimating transformations (especially those for Y ) can result in significant
variance under-estimation, especially for small sample sizes. The bootstrap
2
can be used to correct the apparent R2 (Rapp
) for overfitting. As before,
2
it estimates the optimism (bias) in Rapp and subtracts this optimism from
2
Rapp
to get a more trustworthy estimate. The bootstrap can also be used to
compute confidence limits for all estimated transformations, and confidence
limits for estimated predictor e↵ects that take fully into account the uncertainty associated with the transformations. To do this, all steps involved in
fitting the additive models must be repeated fresh for each re-sample.
Limited testing has shown that the sample size needs to exceed 100 for
ACE and AVAS to provide stable estimates. In small sample sizes the bootstrap bias-corrected estimate of R2 will be zero because the sample information did not support simultaneous estimation of all transformations.

16.4 Obtaining Estimates on the Original Scale
A common practice in least squares fitting is to attempt to rectify lack of fit
by taking parametric transformations of Y before fitting; the logarithm is the
most common transformation. If after transformation the model’s residuals
have a population median of zero, the inverse transformation of a predicted
transformed value estimates the population median of Y given X. This is because unlike means, quantiles are transformation-preserving. Many analysts
make the mistake of not reporting which population parameter is being estimated when inverse transforming X ˆ, and sometimes they even report that
the mean is being estimated.
How would one go about estimating the population mean or other parameter on the untransformed scale? If the residuals are assumed to be normally
distributed and if log(Y ) is the transformation, the mean of the log-normal
distribution, a function of both the mean and the variance of the residuals,
can be used to derive the desired quantity. However, if the residuals are not
normally distributed, this procedure will not result in the correct estimator.
Duan162 developed a “smearing” estimator for more nonparametrically obtaining estimates of parameters on the original scale. In the
one-sample
Psimple
n
case without predictors in which one has computed ✓ˆ = i=1 log(Yi )/n, the
ˆ The smearing
residuals from this fitted value are given by ei = log(Yi ) ✓.

398

16 Transform-Both-Sides Regression

P
estimator of the population mean is
exp[✓ˆ + ei ]/n. In this simple case the
result is the ordinary sample mean Y .
The worth of Duan’s smearing estimator is in regression modeling. Suppose that the regression was run on g(Y ) from which estimated values
ĝ(Yi ) = Xi ˆ and residuals on the transformed scale ei = ĝ(Yi ) Xi ˆ were obtained. Instead of restricting ourselves to estimating the population mean, let
W (y1 , y2 , . . . , yn ) denote any function of a vector of untransformed response
values. To estimate the population mean in the homogeneous one-sample
case, W is the simple average of all of its arguments. To estimate the population 0.25 quantile, W is the sample 0.25 quantile of y1 , . . . , yn . Then the
smearing estimator of the population parameter estimated by W given X is
W (g 1 (a + e1 ), g 1 (a + e2 ), . . . , g 1 (a + en )), where g 1 is the inverse of the
g transformation and a = X ˆ.
When using the AVAS algorithm, the monotonic transformation g is estimated from the data, and the predicted value of ĝ(Y ) is given by Equation 16.3. So we extend the smearing estimator as W (ĝ 1 (a+e1 ), . . . , ĝ 1 (a+
en )), where a is the predicted transformed response given X. As ĝ is nonparametric (i.e., a table look-up), the areg.boot function described below
computes ĝ 1 using reverse linear interpolation.
If residuals from ĝ(Y ) are assumed to be symmetrically distributed, their
population median is zero and we can estimate the median on the untransformed scale by computing ĝ 1 (X ˆ). To be safe, areg.boot adds the median
residual to X ˆ when estimating the population median (the median residual
can be ignored by specifying statistic=’fitted’ to functions that operate
on objects created by areg.boot).
When quantiles of Y are of major interest, a more direct way to obtain
estimates is through the use of quantile regression350 . An excellent case study
including comparisons with other methods such as Cox regression can be
found in Austin et al.37 .

16.5 R Functions
The R acepack package’s ace function implements all the features of the ACE
algorithm, and its avas function does likewise for AVAS. The bootstrap and
smearing capabilities mentioned above are o↵ered for these estimation functions by the areg.boot (“additive regression using the bootstrap”) function
in the Hmisc package. Unlike the ace and avas functions, areg.boot uses the
R modeling language, making it easier for the analyst to specify the predictor
variables and what is assumed about their relationships with the transformed
Y . areg.boot also implements a parametric transform-both-sides approach
using restricted cubic splines and canonical variates, and o↵ers various estimation options with and without smearing. It can estimate the e↵ect of
changing one predictor, holding others constant, using the ordinary boot-

16.6 Case Study

399

strap to estimate the standard deviation of di↵erence in two possibly transformed estimates (for two values of X), assuming normality of such di↵erences. Normality is assumed to avoid generating a large number of bootstrap
replications of time-consuming model fits. It would not be very difficult to
add nonparametric bootstrap confidence limit capabilities to the software.
areg.boot re-samples every aspect of the modeling process it uses, just as
Faraway182 did for parametric least squares modeling.
areg.boot implements a variety of methods as shown in the simple example below. The monotone function restricts a variable’s transformation to be
monotonic, while the I function restricts it to be linear.
f

a r e g . b o o t (Y ⇠ monotone ( age ) +
sex + weight + I ( b l o o d . p r e s s u r e ) )

#show transformations, CLs
#generate S functions
#defining transformations
predict ( f )
#get predictions, smearing estimates
summary ( f )
#compute CLs on effects of each X
s m e a r i n g E s t ( ) #generalized smearing estimators
Mean ( f )
#derive S function to
#compute smearing mean Y
Quantile ( f )
#derive function to compute smearing quantile
plot ( f )
Function ( f )

The methods are best described in a case study.

16.6 Case Study
Consider simulated data where the conditional distribution of Y is log-normal
given X, but where transform-both-sides regression methods use unlogged
Y . Predictor X1 is linearly related to log Y , X2 is related by |X2 12 |, and
categorical X3 has reference group a e↵ect of zero, group b e↵ect of 0.3, and
group c e↵ect of 0.5.
r e q u i r e ( rms )
set.seed (7)
n
400
x1
runif (n)
x2
runif (n)
x3
f a c t o r ( sample ( c ( ’ a ’ , ’ b ’ , ’ c ’ ) , n , TRUE) )
y
exp ( x1 + 2 ⇤ abs ( x2
. 5 ) + . 3 ⇤ ( x3== ’ b ’ ) + . 5 ⇤ ( x3== ’ c ’ ) +
. 5 ⇤ rnorm ( n ) )
# For reference fit appropriate OLS model
p r i n t ( o l s ( l o g ( y ) ⇠ x1 + r c s ( x2 , 5 ) + x3 ) , c o e f s=FALSE,
l a t e x=TRUE)

Linear Regression Model

400

16 Transform-Both-Sides Regression

ols(formula = log(y) ˜ x1 + rcs(x2, 5) + x3)
Model Likelihood Discrimination
Ratio Test
Indexes
Obs 400 LR 2
236.87 R2
0.447
2
0.4722 d.f.
7 Radj
0.437
2
d.f. 392 Pr(> ) 0.0000 g
0.482
Residuals
1Q
Median 3Q Max
0.3075 0.0134 0.327 1.527

Min
1.346

Now fit the avas model. We use 300 bootstrap repetitions but only plot
the first 20 estimates to see clearly how the bootstrap re-estimates of transformations vary. Had we wanted to restrict transformations to be linear, we
would have specified the identity function, for example, I(x1).
a r e g . b o o t ( y ⇠ x1 + x2 + x3 , method= ’ a v a s ’ , B=300)

f
f

avas Additive Regression Model
areg.boot(x = y ⇠ x1 + x2 + x3, B = 300, method = "avas")
Predictor Types
x1
x2
x3

type
s
s
c

y type: s
n= 400

p= 3

Apparent R2 on transformed Y scale: 0.444
Bootstrap validated R2
: 0.42
Coefficients of standardized transformations:
Intercept
-3.443111e-16

x1
9.702960e-01

x2
1.224320e+00

x3
9.881150e-01

Residuals on transformed scale:
Min
1Q
Median
-1.877152e+00 -5.252194e-01 -3.732200e-02
Max
Mean
S.D.
2.172680e+00 8.673617e-19 7.420788e-01

3Q
5.339122e-01

Note that the coefficients above do not mean very much as the scale of the
transformations is arbitrary. We see that the model was very slightly overfitted (R2 dropped from 0.44 to 0.42), and the R2 are in agreement with the
OLS model fit above.

16.6 Case Study

401

Next we plot the transformations, 0.95 confidence bands, and a sample of
the bootstrap estimates.
p l o t ( f , boot =20) # Figure 16.1

Transformed x1

Transformed y

2
1
0
−1

0.6
0.4
0.2
0.0

−0.2
−0.4

−2

−0.6
0

5

10

15

20

0.0

0.4

0.6

0.8

1.0

x1
0.4

0.6

Transformed x3

Transformed x2

y

0.2

0.4
0.2
0.0

−0.2
−0.4
0.0

0.2

0.4

0.6

x2

0.8

1.0

0.2
0.0
−0.2
−0.4

a

b

x3

Fig. 16.1 avas transformations: overall estimates, pointwise 0.95 confidence bands,
and 20 bootstrap estimates (red lines).

The plot is shown in Figure 16.1. The nonparametrically estimated transformation of x1 is almost linear, and the transformation of x2 is close to
|x2 0.5|. We know that the true transformation of y is log(y), so variance
stabilization and normality of residuals will be achieved if the estimated ytransformation is close to log(y).
ys
s e q ( . 8 , 2 0 , l e n g t h =200)
ytrans
F u n c t i o n ( f ) $y
# Function outputs all transforms
p l o t ( l o g ( ys ) , y t r a n s ( ys ) , t y p e= ’ l ’ )
# Figure 16.2
a b l i n e ( lm ( y t r a n s ( ys ) ⇠ l o g ( ys ) ) , c o l=g r a y ( . 8 ) )

c

402

16 Transform-Both-Sides Regression

ytrans(ys)

2
1
0
−1
−2
0.0 0.5 1.0 1.5 2.0 2.5 3.0

log(ys)
Fig. 16.2 Checking estimated against optimal transformation

Approximate linearity indicates that the estimated transformation is very
log-like.a
Now let us obtain approximate tests of e↵ects of each predictor. summary
does this by setting all other predictors to reference values (e.g., medians),
and comparing predicted responses for a given level of the predictor X with
predictions for the lowest setting of X. The default predicted response for
summary is the median, which is used here. Therefore tests are for di↵erences
in medians.
summary ( f , v a l u e s= l i s t ( x1=c ( . 2 , . 8 ) , x2=c ( . 1 , . 5 ) ) )
summary.areg.boot(object = f, values = list(x1 = c(0.2, 0.8),
x2 = c(0.1, 0.5)))
Estimates based on 300 resamples

Values to which predictors are set when estimating
effects of other predictors:
y
x1
x2
x3
3.728843 0.500000 0.300000 2.000000
Estimates of differences of effects on Median Y (from first X
value), and bootstrap standard errors of these differences.
Settings for X are shown as row headings.
Predictor: x1
x
0.2
0.8

a

Differences
S.E Lower 0.95 Upper 0.95
Z
0.000000
NA
NA
NA
NA
1.546992 0.2099959
1.135408
1.958577 7.366773

Beware that use of a data–derived transformation in an ordinary model, as this will
will result in standard errors that are too small. This is because model selection is
not taken into account.182

16.6 Case Study
x

403

Pr(|Z|)
0.2
NA
0.8 1.747491e-13

Predictor: x2
x
0.1
0.5
x

Differences
S.E Lower 0.95 Upper 0.95
Z
0.000000
NA
NA
NA
NA
-1.658961 0.3163361 -2.278968 -1.038953 -5.244298

Pr(|Z|)
0.1
NA
0.5 1.568786e-07

Predictor: x3
x
a
b
c
x

Differences
S.E Lower 0.95 Upper 0.95
Z
0.0000000
NA
NA
NA
NA
0.8447422 0.1768244 0.4981728
1.191312 4.777295
1.3526151 0.2206395 0.9201697
1.785061 6.130431

Pr(|Z|)
a
NA
b 1.776692e-06
c 8.764127e-10

For example, when x1 increases from 0.2 to 0.8 we predict an increase in
median y by 1.55 with bootstrap standard error 0.21, when all other predictors are held to constants. Setting them to other constants will yield di↵erent
estimates of the x1 e↵ect, as the transformation of y is nonlinear.
Next depict the fitted model by plotting predicted values, with x2 varying
on the x-axis, and three curves corresponding to three values of x3. x1 is set
to 0.5. Figure 16.3 shows estimates of both the median and the mean y.
newdat

yhat

e x p a n d . g r i d ( x2=s e q ( . 0 5 , . 9 5 , l e n g t h =200) ,
x3=c ( ’ a ’ , ’ b ’ , ’ c ’ ) , x1=. 5 ,
s t a t i s t i c =c ( ’ median ’ , ’ mean ’ ) )
c ( p r e d i c t ( f , s u b s e t ( newdat , s t a t i s t i c== ’ median ’ ) ,
s t a t i s t i c = ’ median ’ ) ,
p r e d i c t ( f , s u b s e t ( newdat , s t a t i s t i c== ’ mean ’ ) ,
s t a t i s t i c = ’ mean ’ ) )

newdat
upData ( newdat ,
l p = x1 + 2 ⇤ abs ( x2
. 5 ) + . 3 ⇤ ( x3== ’ b ’ ) +
. 5 ⇤ ( x3== ’ c ’ ) ,
y t r u e = i f e l s e ( s t a t i s t i c== ’ median ’ , exp ( l p ) ,
exp ( l p + 0 . 5 ⇤ ( 0 . 5 ^ 2 ) ) ) , pr=FALSE)
Input
Added
Added
Added

object size:
variable
variable
variable

New object size:

45472 bytes;
lp
ytrue
pr

4 variables

69800 bytes;

7 variables

# Use Hmisc function xYplot to produce Figure 16.3
xYplot ( yhat ⇠ x2 | s t a t i s t i c , g r o u p s=x3 ,

404

16 Transform-Both-Sides Regression

)

data=newdat , t y p e= ’ l ’ , c o l =1 ,
y l a b=e x p r e s s i o n ( hat ( y ) ) ,
p a n e l=f u n c t i o n ( . . . ) {
panel.xYplot ( . . . )
dat
s u b s e t ( newdat ,
s t a t i s t i c==c ( ’ median ’ , ’ mean ’ ) [ c u r r e n t . c o l u m n ( ) ] )
f o r (w i n c ( ’ a ’ , ’ b ’ , ’ c ’ ) )
with ( s u b s e t ( dat , x3==w) ,
l l i n e s ( x2 , y t r u e , c o l=g r a y ( . 7 ) , lwd=1 . 5 ) )
}

0.2

median

0.6

0.8

mean
c

7
6

y^

0.4

c

5

b
b

4
a

a

3
2
0.2

0.4

0.6

0.8

x2
Fig. 16.3 Predicted median (left panel) and mean (right panel) y as a function of
x2 and x3. True population values are shown in gray.
keep lowering the sample size until you find a breaking point

e^(Xbetahat+Q3(e))
add the upper quartile of the residuals to Xbetahat and anti-log it. It is simple for quantiles because it is preserving order. This is pointing out the simple thing that we make use of in
simulation a lot - we are not assuming these residuals have a normal distribution. residuals are independent of X - they are exchangeable - to get this, we are assuming the transformation of y is
appropriate

Chapter 17

Introduction to Survival Analysis

17.1 Background
Suppose that one wished to study the occurrence of some event in a population of subjects. If the time until the occurrence of the event were unimportant, the event could be analyzed as a binary outcome using the logistic
regression model. For example, in analyzing mortality associated with open
heart surgery, it may not matter whether a patient dies during the procedure or he dies after being in a coma for two months. For other outcomes,
especially those concerned with chronic conditions, the time until the event
is important. In a study of emphysema, death at eight years after onset of
symptoms is di↵erent from death at six months. An analysis that simply
counted the number of deaths would be discarding valuable information and
sacrificing statistical power.
Survival analysis is used to analyze data in which the time until the event
is of interest. The response variable is the time until that event and is often
called a failure time, survival time, or event time. Examples of responses
of interest include the time until cardiovascular death, time until death or
myocardial infarction, time until failure of a light bulb, time until pregnancy,
or time until occurrence of an ECG abnormality during exercise. Bull and
Spiegelhalter82 have an excellent overview of survival analysis.
The response, event time, is usually continuous, but survival analysis allows the response to be incompletely determined for some subjects. For example, suppose that after a five-year follow-up study of survival after myocardial
infarction a patient is still alive. That patient’s survival time is censored on
the right at five years; that is, her survival time is known only to exceed five
years. The response value to be used in the analysis is 5+. Censoring can also
occur when a subject is lost to follow-up.
If no responses are censored, standard regression models for continuous
responses could be used to analyze the failure times by writing the expected
failure time as a function of one or more predictors, assuming that the dis-

405

1

2

406

17 Introduction to Survival Analysis

tribution of failure time is properly specified. However, there are still several
reasons for studying failure time using the specialized methods of survival
analysis.
1. Time to failure can have an unusual distribution. Failure time is restricted
to be positive so it has a skewed distribution and will never be normally
distributed.
2. The probability of surviving past a certain time is often more relevant than
the expected survival time (and expected survival time may be difficult to
estimate if the amount of censoring is large).
3. A function used in survival analysis, the hazard function, helps one to
understand the mechanism of failure.301
Survival analysis is used often in industrial life-testing experiments, and
it is heavily used in clinical and epidemiologic follow-up studies. Examples
include a randomized trial comparing a new drug with placebo for its ability
to maintain remission in patients with leukemia, and an observational study
of prognostic factors in coronary heart disease. In the latter example subjects
may well be followed for varying lengths of time, as they may enter the study
over a period of many years.
When regression models are used for survival analysis, all the advantages
of these models can be brought to bear in analyzing failure times. Multiple,
independent prognostic factors can be analyzed simultaneously and treatment
di↵erences can be assessed while adjusting for heterogeneity and imbalances
in baseline characteristics. Also, patterns in outcome over time can be predicted for individual subjects.
Even in a simple well-designed experiment, survival modeling can allow
one to do the following in addition to making simple comparisons.
1. Test for and describe interactions with treatment. Subgroup analyses can
easily generate spurious results and they do not consider interacting factors in a dose-response manner. Once interactions are modeled, relative
treatment benefits can be estimated (e.g., hazard ratios), and analyses
can be done to determine if some patients are too sick or too well to have
even a relative benefit.
2. Understand prognostic factors (strength and shape).
3. Model absolute e↵ect of treatment. First, a model for the probability of
surviving past time t is developed. Then di↵erences in survival probabilities
for patients on treatments A and B can be estimated. The di↵erences will
be due primarily to sickness (overall risk) of the patient and to treatment
interactions.
4. Understand time course of treatment e↵ect. The period of maximum e↵ect
or period of any substantial e↵ect can be estimated from a plot of relative
e↵ects of treatment over time.
5. Gain power for testing treatment e↵ects.
6. Adjust for imbalances in treatment allocation in non-randomized studies.

17.2 Censoring, Delayed Entry, and Truncation

407

17.2 Censoring, Delayed Entry, and Truncation
Responses may be left–censored and interval–censored besides being right–
censored. Interval–censoring is present, for example, when a measuring device functions only for a certain range of the response; measurements outside that range are censored at an end of the scale of the device. Interval–
censoring also occurs when the presence of a medical condition is assessed
during periodic exams. When the condition is present, the time until the condition developed is only known to be between the current and the previous
exam. Left–censoring means that an event is known to have occurred before
a certain time. In addition, left–truncation and delayed entry are common.
Nomenclature is confusing as many authors refer to delayed entry as left–
truncation. Left–truncation really means that an unknown subset of subjects
failed before a certain time and the subjects didn’t get into the study. For
example, one might study the survival patterns of patients who were admitted to a tertiary care hospital. Patients who didn’t survive long enough to
be referred to the hospital compose the left-truncated group, and interesting
questions such as the optimum timing of admission to the hospital cannot be
answered from the data set.
Delayed entry occurs in follow-up studies when subjects are exposed to the
risk of interest only after varying periods of survival. For example, in a study
of occupational exposure to a toxic compound, researchers may be interested
in comparing life length of employees with life expectancy in the general
population. A subject must live until the beginning of employment before
exposure is possible; that is, death cannot be observed before employment.
The start of follow-up is delayed until the start of employment and it may be
right–censored when follow-up ends. In some studies, a researcher may want
to assume that for the purpose of modeling the shape of the hazard function,
time zero is the day of diagnosis of disease, while patients enter the study
at various times since diagnosis. Delayed entry occurs for patients who don’t
enter the study until some time after their diagnosis. Patients who die before
study entry are left-truncated. Note that the choice of time origin is very
important.52, 82, 109, 130
Heart transplant studies have been analyzed by considering time zero to be
the time of enrollment in the study. Pre-transplant survival is right–censored
at the time of transplant. Transplant survival experience is based on delayed
entry into the “risk set” to recognize that a transplant patient is not at risk
of dying from transplant failure until after a donor heart is found. In other
words, survival experience is not credited to transplant surgery until the day
of transplant. Comparisons of transplant experience with medical treatment
su↵er from “waiting time bias” if transplant survival begins on the day of
transplant instead of using delayed entry.205, 431, 566
There are several planned mechanisms by which a response is right–
censored. Fixed type I censoring occurs when a study is planned to end after
two years of follow-up, or when a measuring device will only measure re-

3

408

4

17 Introduction to Survival Analysis

sponses up to a certain limit. There the responses are observed only if they
fall below a fixed value C. In type II censoring, a study ends when there is
a pre-specified number of events. If, for example, 100 mice are followed until
50 die, the censoring time is not known in advance.
We are concerned primarily with random type I right-censoring in which
each subject’s event time is observed only if the event occurs before a certain time, but the censoring time can vary between subjects. Whatever the
cause of censoring, we assume that the censoring is non-informative about the
event; that is, the censoring is caused by something that is independent of the
impending failure. Censoring is non-informative when it is caused by planned
termination of follow-up or by a subject moving out of town for reasons unrelated to the risk of the event. If subjects are removed from follow-up because
of a worsening condition, the informative censoring will result in biased estimates and inaccurate statistical inference about the survival experience. For
example, if a patient’s response is censored because of an adverse e↵ect of
a drug or noncompliance to the drug, a serious bias can result if patients
with adverse experiences or noncompliance are also at higher risk of su↵ering
the outcome. In such studies, efficacy can only be assessed fairly using the
intention to treat principle: all events should be attributed to the treatment
assigned even if the subject is later removed from that treatment.

17.3 Notation, Survival, and Hazard Functions
In survival analysis we use T to denote the response variable, as the response
is usually the time until an event. Instead of defining the statistical model
for the response T in terms of the expected failure time, it is advantageous
to define it in terms of the survival function, S(t), given by
S(t) = Prob{T > t} = 1

F (t),

(17.1)

where F (t) is the cumulative distribution function for T . If the event is death,
S(t) is the probability that death occurs after time t, that is, the probability
that the subject will survive at least until time t. S(t) is always 1 at t = 0;
all subjects survive at least to time zero. The survival function must be
non-increasing as t increases. An example of a survival function is shown in
Figure 17.1. In that example subjects are at very high risk of the event in the
early period so that the S(t) drops sharply. The risk is low for 0.1  t  0.6, so
S(t) is somewhat flat. After t = .6 the risk again increases, so S(t) drops more
quickly.
Figure 17.2 depicts the cumulative hazard function corresponding
to the survival function in Figure 17.1. This function is denoted by ⇤(t).
It describes the accumulated risk up until time t, and as is shown later,
is the negative of the log of the survival function. ⇤(t) is non-decreasing
as t increases; that is, the accumulated risk increases or remains the same.

17.3 Notation, Survival, and Hazard Functions

409

1.0

S(t)

0.8

0.6

0.4

0.2

0.0

0.2

0.4

0.6

0.8

1.0

0.6

0.8

1.0

t
Fig. 17.1 Survival function

3.0
2.5

Λ(t)

2.0
1.5
1.0
0.5
0.0
0.0

0.2

0.4

t
Fig. 17.2 Cumulative hazard function

410

17 Introduction to Survival Analysis

14
12

λ(t)

10
8
6
4
2

0.0

0.2

0.4

0.6

0.8

1.0

t
Fig. 17.3 Hazard function

Another important function is the hazard function, (t), also called the force
of mortality, or instantaneous event (death, failure) rate. The hazard at time
t is related to the probability that the event will occur in a small interval
around t, given that the event has not occurred before time t. By studying
the event rate at a given time conditional on the event not having occurred by
that time, one can learn about the mechanisms and forces of risk over time.
Figure 17.3 depicts the hazard function corresponding to S(t) in Figure 17.1
and to ⇤(t) in Figure 17.2 . Notice that the hazard function allows one to
more easily determine the phases of increased risk than looking for sudden
drops in S(t) or ⇤(t).
The hazard function is defined formally by
(t) = lim

u!0

Prob{t < T  t + u|T > t}
,
u

(17.2)

which using the law of conditional probability becomes
Prob{t < T  t + u}/Prob{T > t}
u
[F (t + u) F (t)]/u
= lim
u!0
S(t)
@F (t)/@t
=
S(t)

(t) = lim

u!0

(17.3)

17.3 Notation, Survival, and Hazard Functions

=

411

f (t)
,
S(t)

where f (t) is the probability density function of T evaluated at t, the derivative or slope of the cumulative distribution function 1 S(t). Since
@ log S(t)
@S(t)/@t
=
=
@t
S(t)

f (t)
,
S(t)

(17.4)

the hazard function can also be expressed as
(t) =

@ log S(t)
,
@t

(17.5)

the negative of the slope of the log of the survival function. Working backwards, the integral of (t) is:
Z

t

(v)dv =

log S(t).

(17.6)

0

The integral or area under (t) is defined to be ⇤(t), the cumulative hazard
function. Therefore
⇤(t) = log S(t),
(17.7)
or
S(t) = exp[ ⇤(t)].

(17.8)

So knowing any one of the functions S(t), ⇤(t), or (t) allows one to derive
the other two functions. The three functions are di↵erent ways of describing
the same distribution.
One property of ⇤(t) is that the expected value of ⇤(T ) is unity, since if
T ⇠ S(t), the density of T is (t)S(t) and
Z 1
E[⇤(T )] =
⇤(t) (t) exp( ⇤(t))dt
0
Z 1
=
u exp( u)du
(17.9)
0

= 1.

Now consider properties of the distribution of T . The population qth quantile (100qth percentile), Tq , is the time by which a fraction q of the subjects
will fail. It is the value t such that S(t) = 1 q; that is
Tq = S

1

(1

q).

(17.10)

The median life length is the time by which half the subjects will fail, obtained
by setting S(t) = 0.5:
T0.5 = S 1 (0.5).
(17.11)

412

17 Introduction to Survival Analysis

The qth quantile of T can also be computed by setting exp[ ⇤(t)] = 1
giving
Tq = ⇤

1

[ log(1

T.5 = ⇤

1

(log 2).

q,

q)] and as a special case,
(17.12)

The mean or expected value of T (the expected failure time) is the area under
the survival function for t ranging from 0 to 1:
Z 1
µ=
S(v)dv.
(17.13)
0

Irwin has defined mean restricted life (see [327, 328]), which is the area under
S(t) up to a fixed time (usually chosen to be a point at which there is still
adequate follow-up information).
The random variable T denotes a random failure time from the survival
distribution S(t). We need additional notation for the response and censoring
information for the ith subject. Let Ti denote the response for the ith subject.
This response is the time until the event of interest, and it may be censored
if the subject is not followed long enough for the event to be observed. Let Ci
denote the censoring time for the ith subject, and define the event indicator
as
ei = 1 if the event was observed (Ti  Ci ),

= 0 if the response was censored (Ti > Ci ).

(17.14)

The observed response is
Yi = min(Ti , Ci ),

(17.15)

which is the time that occurred first: the failure time or the censoring time.
The pair of values (Yi , ei ) contains all the response information for most
purposes (i.e., the potential censoring time Ci is not usually of interest if the
event occurred before Ci ).
Figure 17.4 demonstrates this notation. The line segments start at study
entry (survival time t = 0).
A useful property of the cumulative hazard function can be derived as follows. Let z be any cuto↵ time and consider the expected value of ⇤ evaluated
at the earlier of the cuto↵ time or the actual failure time.
E[⇤(min(T, z))] = E[⇤(T )[T  z] + ⇤(z)[T > z]]
= E[⇤(T )[T  z]] + ⇤(z)S(z).

The first term in the right–hand side is
Z 1
⇤(t)[t  z] (t) exp( ⇤(t))dt
0

(17.16)

17.4 Homogeneous Failure Time Distributions

413

t

t
t

Ti

Ci

Yi

ei

75

81

75

1

7

76

7

1

68+ 68

68

0

52+ 52

52

0

20

20

1

56

Termination of Study

Fig. 17.4 Some censored data. Circles denote events.

=

Z

=

z

⇤(t) (t) exp( ⇤(t))dt

(17.17)

0
⇤(z)

[u exp( u) + exp( u)]|0

=1

S(z)[⇤(z) + 1].

Adding ⇤(z)S(z) results in
E[⇤(min(T, z))] = 1

S(z) = F (z).

(17.18)

Pn

It follows that i=1 ⇤(min(Ti , z)) estimates the expected number of failures
occurring before time z among the n subjects.

17.4 Homogeneous Failure Time Distributions
In this section we assume that each subject in the sample has the same distribution of the random variable T that represents the time until the event.
In particular, there are no covariables that describe di↵erences between subjects in the distribution of T . As before we use S(t), (t), and ⇤(t) to denote,
respectively, the survival, hazard, and cumulative hazard functions.
The form of the true population survival distribution function S(t) is almost always unknown, and many distributional forms have been used for
describing failure time data. We consider first the two most popular parametric survival distributions: the exponential and Weibull distributions. The
exponential distribution is a very simple one in which the hazard function is
constant; that is, (t) = . The cumulative hazard and survival functions
are then
⇤(t) = t

and

5

414

17 Introduction to Survival Analysis

S(t) = exp( ⇤(t)) = exp(
The median life length is ⇤

1

t).

(17.19)

(log 2) or
T0.5 = log(2)/ .

(17.20)

The time by which 1/2 of the subjects will have failed is then proportional to
the reciprocal of the constant hazard rate . This is true also of the expected
or mean life length, which is 1/ .
The exponential distribution is one of the few distributions for which a
closed-form solution exists for the estimator of its parameter when censoring
is present. This estimator is a function of the number of events and the total
person-years of exposure. Methods based on person-years in fact implicitly
assume an exponential distribution. The exponential distribution is often
used to model events that occur “at random in time.”316 It has the property
that the future lifetime of a subject is the same, no matter how “old” it is,
or
Prob{T > t0 + t|T > t0 } = Prob{T > t}.
(17.21)
This “ageless” property also makes the exponential distribution a poor choice
for modeling human survival except over short time periods.
The Weibull distribution is a generalization of the exponential distribution.
Its hazard, cumulative hazard, and survival functions are given by
(t) = ↵ t

1

⇤(t) = ↵t

(17.22)

S(t) = exp( ↵t ).
The Weibull distribution with
= 1 is an exponential distribution (with
constant hazard). When > 1, its hazard is increasing with t, and when
< 1 its hazard is decreasing. Figure 17.5 depicts some of the shapes of
the hazard function that are possible. If T has a Weibull distribution, the
median of T is
T0.5 = [(log 2)/↵]1/ .
(17.23)

6

There are many other traditional parametric survival distributions, some of
which have hazards that are “bathtub shaped” as in Figure 17.3.238, 316 The
restricted cubic spline function described in Section 2.4.5 is an alternative
basis for (t).280, 281 This function family allows for any shape of smooth
(t) since the number of knots can be increased as needed, subject to the
number of events in the sample. Nonlinear terms in the spline function can
be tested to assess linearity of hazard (Rayleigh-ness) or constant hazard
(exponentiality).
The restricted cubic spline hazard model with k knots is

17.4 Homogeneous Failure Time Distributions

415

7
6

.5
1
2
4

5
4
3
2
1
0
0.0

0.2

0.4

0.6

0.8

1.0

1.2

t
Fig. 17.5 Some Weibull hazard functions with ↵ = 1 and various values of .

k (t) = a + bt +

k
X2

j wj (t),

(17.24)

j=1

where the wj (t) are the restricted cubic spline terms of Equation 2.25. There
terms are cubic terms in t. A set of knots v1 , . . . , vk is selected from the
quantiles of the uncensored failure times (see Section 2.4.5 and [280]).
The cumulative hazard function for this model is
1
1
⇤(t) = at + t2 + ⇥ quartic terms in t.
2
4

(17.25)

Standard maximum likelihood theory is used to obtain estimates of the k
unknown parameters to derive, for example, smooth estimates of (t) with
confidence bands. The flexible estimates of S(t) using this method are as
efficient as Kaplan–Meier estimates, but they are smooth and can be used as a
basis for modeling predictor variables. The spline hazard model is particularly
useful for fitting steeply falling and gently rising hazard functions that are
characteristic of high-risk medical procedures.

416

17 Introduction to Survival Analysis

Table 17.1

Day No. Subjects Deaths Censored
Cumulative
At Risk
Survival
12
100
1
0
99/100 = .99
30
99
2
1
97/99 ⇥ 99/100 = .97
60
96
0
3
96/96 ⇥ .97 = .97
72
93
3
0
90/93 ⇥ .97 = .94
.
.
.
.
.
.
.
.
.
.

17.5 Nonparametric Estimation of S and ⇤
17.5.1 Kaplan–Meier Estimator
As the true form of the survival distribution is seldom known, it is useful to
estimate the distribution without making any assumptions. For many analyses, this may be the last step, while in others this step helps one select a
statistical model for more in-depth analyses. When no event times are censored, a nonparametric estimator of S(t) is 1 Fn (t) where Fn (t) is the
usual empirical cumulative distribution function based on the observed failure times T1 , . . . , Tn . Let Sn (t) denote this empirical survival function. Sn (t)
is given by the fraction of observed failure times that exceed t:
Sn (t) = [number of Ti > t]/n.

(17.26)

When censoring is present, S(t) can be estimated (at least for t up until
the end of follow-up) by the Kaplan–Meier326 product-limit estimator. This
method is based on conditional probabilities. For example, suppose that every subject has been followed for 39 days or has died within 39 days so that
the proportion of subjects surviving at least 39 days can be computed. After
39 days, some subjects may be lost to follow-up besides those removed from
follow-up because of death within 39 days. The proportion of those still followed 39 days who survive day 40 is computed. The probability of surviving
40 days from study entry equals the probability of surviving day 40 after
living 39 days, multiplied by the chance of surviving 39 days.
The life table in Table 17.1 demonstrates the method in more detail. We
suppose that 100 subjects enter the study and none die or are lost before day
12.
Times in a life table should be measured as precisely as possible. If the
event being analyzed is death, the failure time should usually be specified
to the nearest day. We assume that deaths occur on the day indicated and
that being censored on a certain day implies the subject survived through the

17.5 Nonparametric Estimation of S and ⇤

417

Table 17.2

i
1
2
3

ti
1
3
9

ni
7
6
2

di (ni di )/ni
1
6/7
2
4/6
1
1/2

end of that day. The data used in computing Kaplan–Meier estimates consist
of (Yi , ei ), i = 1, 2, . . . , n using notation defined previously. Primary data
collected to derive (Yi , ei ) usually consist of entry date, event date (if subject
failed), and censoring date (if subject did not fail). Instead, the entry date,
date of event/censoring, and event/censoring indicator ei may be specified.
The Kaplan–Meier estimator is called the product-limit estimator because
it is the limiting case of actuarial survival estimates as the time periods
shrink so that an entry is made for each failure time. An entry need not
be in the table for censoring times (when no failures occur at that time) as
long as the number of subjects censored is subtracted from the next number
at risk. Kaplan–Meier estimates are preferred to actuarial estimates because
they provide more resolution and make fewer assumptions. In constructing
a yearly actuarial life table, for example, it is traditionally assumed that
subjects censored between two years were followed 0.5 years.
The product-limit estimator is a nonparametric maximum likelihood estimator [324, pp. 10–13]. The formula for the Kaplan–Meier product-limit
estimator of S(t) is as follows. Let k denote the number of failures in the
sample and let t1 , t2 , . . . , tk denote the unique event times (ordered for ease
of calculation). Let di denote the number of failures at ti and ni be the number of subjects at risk at time ti ; that is, ni = number of failure/censoring
times ti . The estimator is then
Y
SKM (t) =
(1 di /ni ).
(17.27)
i:ti t

The Kaplan–Meier estimator of ⇤(t) is ⇤KM (t) = log SKM (t). An estimate
1
of quantile q of failure time is SKM
(1 q), if follow-up is long enough so that
SKM (t) drops as low as 1 q. If the last subject followed failed so that SKM (t)
drops to zero, the expected failure time can be estimated by computing the
area under the Kaplan–Meier curve.
To demonstrate computation of SKM (t), imagine a sample of failure times
given by
1 3 3 6+ 8+ 9 10+ ,
where + denotes a censored time. The quantities needed to compute SKM
are in Table 17.2. Thus

418

17 Introduction to Survival Analysis

0t<1

SKM (t) = 1,

= 6/7 = .85, 1  t < 3

= (6/7)(4/6) = .57, 3  t < 9

(17.28)

= (6/7)(4/6)(1/2) = .29, 9  t < 10.
Note that the estimate of S(t) is undefined for t > 10 since not all subjects
have failed by t = 10 but no follow-up extends beyond t = 10. A graph of
the Kaplan–Meier estimate is found in Figure 17.6.
r e q u i r e ( rms )
tt
c (1 ,3 ,3 ,6 ,8 ,9 ,10)
stat
c (1 ,1 ,1 ,0 ,0 ,1 ,0)
S
Surv ( t t , s t a t )
s u r v p l o t ( npsurv ( S ⇠ 1 ) , c o n f=” bands ” , n . r i s k=TRUE,
x l a b=e x p r e s s i o n ( t ) )
s u r v p l o t ( npsurv ( S ⇠ 1 , t y p e=” f l e m i n g h a r r i n g t o n ” ,
c o n f . i n t=FALSE) , add=TRUE, l t y =3)

Survival Probability

1.0

0.8

0.6

0.4

0.2

0.0

7

7

6

6

4

4

4

3

3

2

1

0

1

2

3

4

5

6

7

8

9

10

t
Fig. 17.6 Kaplan–Meier product–limit estimator with 0.95 confidence bands. The
Altschuler–Nelson–Aalen–Fleming–Harrington estimator is depicted with the dotted
lines.

The variance of SKM (t) can be estimated using Greenwood’s formula [324,
p. 14], and using normality of SKM (t) in large samples this variance can
be used to derive a confidence interval for S(t). A better method is to derive an asymmetric confidence interval for S(t) based on a symmetric in-

17.5 Nonparametric Estimation of S and ⇤

419

terval for log ⇤(t). This latter method ensures that a confidence limit does
not exceed one or fall below zero, and is more accurate since log ⇤KM (t) is
more normally distributed than SKM (t). Once a confidence interval, say [a, b]
is determined for log ⇤(t), the confidence interval for S(t) is computed by
[exp{ exp(b)}, exp{ exp(a)}]. The formula for an estimate of the variance
of interest is [324, p. 15]:
P
di )]
i:t t di /[ni (ni
Var{log ⇤KM (t)} = P i
.
(17.29)
{ i:ti t log[(ni di )/ni ]}2

Letting s denote the square root of this variance estimate, an approximate
1 ↵ confidence interval for log ⇤(t) is given by log ⇤KM (t) ± zs , where z is
the 1 ↵/2 standard normal critical value. After simplification, the confidence
interval for S(t) becomes
SKM (t)exp(±zs) .
(17.30)
Even though the log ⇤ basis for confidence limits has theoretical advantages, on the log log scale the estimate of S(t) has the greatest instability
where much information is available: when S(t) falls just below 1.0. For that
reason, the recommended default confidence limits are on the ⇤(t) scale using
Var{⇤KM (t)} =

X

i:ti t

di
.
[ni (ni di )]

Letting s denote its square root, an approximate 1
S(t) is given by
exp(±zs)SKM (t),

(17.31)

↵ confidence interval for
(17.32)

truncated to [0, 1].

7

17.5.2 Altschuler–Nelson Estimator
Altschuler19 , Nelson465 , Aalen1 and Fleming and Harrington192 proposed estimators of ⇤(t) or of S(t) based on an estimator of ⇤(t):
ˆ =
⇤(t)

X di
ni

i:ti t

ˆ
S⇤ (t) = exp( ⇤(t)).
(17.33)
Pn ˆ
Pn
S⇤ (t) has advantages over SKM (t). First, i=1 ⇤(Yi ) = i=1 ei [598, Appendix 3]. In other words, the estimator gives the correct expected number of events. Second, there is a wealth of asymptotic theory based on the
Altschuler–Nelson estimator.192

420

8

17 Introduction to Survival Analysis

See Figure 17.6 for an example of the S⇤ (t) estimator. This estimator has
the same variance as SKM (t) for large enough samples.

17.6 Analysis of Multiple Endpoints
Clinical studies frequently assess multiple endpoints. A cancer clinical trial
may, for example, involve recurrence of disease and death, whereas a cardiovascular trial may involve nonfatal myocardial infarction and death. Endpoints may be combined, and the new event (e.g., time until infarction or
death) may be analyzed with any of the tools of survival analysis because
only the usual censoring mechanism is used. Sometimes the various endpoints may need separate study, however, because they may have di↵erent
risk factors.
When the multiple endpoints represent multiple causes of a terminating
event (e.g., death), Prentice et al. have developed standard methods for analyzing cause-specific hazards506 [324, pp. 163–178]. Their methods allow
each cause of failure to be analyzed separately, censoring on the other causes.
They do not assume any mechanism for cause removal nor make any assumptions regarding the interrelation among causes of failure. However, analyses
of competing events using data where some causes of failure are removed in
a di↵erent way from the original dataset will give rise to di↵erent inferences.
When the multiple endpoints represent a mixture of fatal and nonfatal
outcomes, the analysis may be more complex. The same is true when one
wishes to jointly study an event-time endpoint and a repeated measurement.
9

17.6.1 Competing Risks

10

When events are independent, each event may also be analyzed separately by
censoring on all other events as well as censoring on loss to follow-up. This will
yield an unbiased estimate of an easily interpreted cause-specific (t) or S(t)
because censoring is non-informative [324, pp. 168–169]. One minus SKM (t)
computed in this manner will correctly estimate the probability of failing from
the event in the absence of other events. Even when the competing events are
not independent, the cause-specific hazard model may lead to valid results,
but the resulting model does not allow one to estimate risks conditional on
removal of one or more causes of the event. See Kay334 for a nice example
of competing risks analysis when a treatment reduces the risk of death from
one cause but increases the risk of death from another cause.
Larson and Dinse368 have an interesting approach that jointly models the
time until (any) failure and the failure type. For r failure types, they use

17.6 Analysis of Multiple Endpoints

421

an r-category polytomous logistic model to predict the probability of failing
from each cause. They assume that censoring is unrelated to cause of event.

17.6.2 Competing Dependent Risks
In many medical and epidemiologic studies one is interested in analyzing
multiple causes of death. If the goal is to estimate cause-specific failure probabilities, treating subjects dying from extraneous causes as censored and then
computing the ordinary Kaplan–Meier estimate results in biased (high) survival estimates208, 220 . If cause m is of interest, the cause-specific hazard function is defined as
m (t)

Pr{fail from cause m in [t, t + u)|alive at t}
.
u!0
u

= lim

(17.34)

The cumulative incidence function or probability of failure from cause m by
time t is given by
Z t
Fm (t) =
(17.35)
m (u)S(u)du,
0

where S(u) isR the
probability of surviving (ignoring cause of death), which
u P
equals exp[ 0 (
m (x))dx] [208]; [437, Chapter 10]; [99,401]. As previously
Rt
mentioned, 1 Fm (t) = exp[ 0 m (u)du] only if failures due to other causes
are eliminated and if the cause-specific hazard of interest remains unchanged
in doing so.208
Again letting t1 , t2 , . . . , tk denote the unique ordered failure times, a nonparametric estimate of Fm (t) is given by
F̂m (t) =

X dmi
SKM (ti
ni

1 ),

(17.36)

i:ti t

where dmi is the number of failures of type m at time ti and ni is the number
of subjects at risk of failure at ti .
Pepe and others487, 489, 490 showed how to use a combination of Kaplan–
Meier estimators to derive an estimator of the probability of being free of
event 1 by time t given event 2 has not occurred by time t (see also [342]).
Let T1 and T2 denote, respectively, the times until events 1 and 2. Let S1 (t)
and S2 (t) denote, respectively, the two survival functions. Let us suppose
that event 1 is not a terminating event (e.g., is not death) and that even
after event 1 subjects are followed to ascertain occurrences of event 2. The
probability that T1 > t given T2 > t is
Prob{T1 > t|T2 > t} =

Prob{T1 > t and T2 > t}
Prob{T2 > t}

422

17 Introduction to Survival Analysis

=

S12 (t)
,
S2 (t)

(17.37)

where S12 (t) is the survival function for min(T1 , T2 ), the earlier of the two
events. Since S12 (t) does not involve any informative censoring (assuming as
always that loss to follow-up is non-informative), S12 may be estimated by
the Kaplan–Meier estimator SKM12 (or by S⇤ ). For the type of event 1 we
have discussed above, S2 can also be estimated without bias by SKM2 . Thus
we estimate, for example, the probability that a subject still alive at time t
will be free of myocardial infarction as of time t by SKM12 /SKM2 .
Another quantity that can easily be computed from ordinary survival estimates is S2 (t) S12 (t) = [1 S12 (t)] [1 S2 (t)], which is the probability
that event 1 occurs by time t and that event 2 has not occurred by time t.
The ratio estimate above is used to estimate the survival function for one
event given that another has not occurred. Another function of interest is
the crude survival function which is a marginal distribution; that is, it is the
probability that T1 > t whether or not event 2 occurs:355
Sc (t) = 1

F1 (t)

F1 (t) = Prob{T1  t},

11

(17.38)

where F1 (t) is the crude incidence function defined previously. Note that the
T1  t implies that the occurrence of event 1 is part of the probability being
computed. If event 2 is a terminating event so that some subjects can never
su↵er event 1, the crude survival function for T1 will never drop to zero. The
crude survival function can be interpreted as the survival distribution of W
where W = T1 if T1 < T2 and W = 1 otherwise.355

17.6.3 State Transitions and Multiple Types of
Nonfatal Events
In many studies there is one final, absorbing state (death, all causes) and multiple live states. The live states may represent di↵erent health states or phases
of a disease. For example, subjects may be completely free of cancer, have an
isolated tumor, metastasize to a distant organ, and die. Unlike this example,
the live states need not have a definite ordering. One may be interested in estimating transition probabilities, for example, the probability ⇡ij (t1 , t2 ) that
an individual in state i at time t1 is in state j after an additional time t2 .
Strauss and Shavelle590 have developed an extended Kaplan–Meier estimator
i
for this situation. Let SKM
(t|t1 ) denote the ordinary Kaplan–Meier estimate
of the probability of not dying before time t (ignoring distinctions between
multiple live states) for a cohort of subjects beginning follow-up at time t1
in state i. This is an estimate of the probability of surviving an additional t

17.6 Analysis of Multiple Endpoints

423

time units (in any live state) given that the subject was alive and in state i
at time t1 . Strauss and Shavelle’s estimator is given by
⇡ij (t1 , t2 ) =

nij (t1 , t2 ) i
S
(t2 |t1 ),
ni (t1 , t2 ) KM

(17.39)

where ni (t1 , t2 ) is the number of subjects in live state i at time t1 who are
alive and uncensored t2 time units later, and nij (t1 , t2 ) is the number of such
subjects in state j t2 time units beyond t1 .

12

17.6.4 Joint Analysis of Time and Severity of an
Event
In some studies, an endpoint is given more weight if it occurs earlier or
if it is more severe clinically, or both. For example, the event of interest
may be myocardial infarction, which may be of any severity from minimal
damage to the left ventricle to a fatal infarction. Berridge and Whitehead51
have provided a promising model for the analysis of such endpoints. Their
method assumes that the severity of endpoints which do occur is measured
on an ordinal categorical scale and that severity is assessed at the time of
the event. Berridge and Whitehead’s example was time until first headache,
with severity of headaches graded on an ordinal scale. They proposed a joint
hazard of an individual who responds with ordered category j:
j (t)

= (t)⇡j (t),

(17.40)

where (t) is the hazard for the failure time and ⇡j (t) is the probability of
an individual having event severity j given she fails at time t. Note that a
shift in the distribution of response severity is allowed as the time until the
event increases.

17.6.5 Analysis of Multiple Events
It is common to choose as an endpoint in a clinical trial an event that can
recur. Examples include myocardial infarction, gastric ulcer, pregnancy, and
infection. Using only the time until the first event can result in a loss of
statistical information and power.a There are specialized multivariate survival
a

An exception to this is the case in which once an event occurs for the first time, that
event is likely to recur multiple times for any patient. Then the latter occurrences are
redundant.

13

424

14

17 Introduction to Survival Analysis

models (whose assumptions are extremely difficult to verify) for handling this
setup, but in many cases a simpler approach will be efficient.
The simpler approach involves modeling the marginal distribution of the
time until each event.400, 488 Here one forms one record per subject per event,
and the survival time is the time to the first event for the first record, or is
the time from the previous event to the next event for all later records. This
approach yields consistent estimates of distribution parameters as long as the
marginal distributions are correctly specified.649 One can allow the number of
previous events to influence the hazard function of another event by modeling
this count as a covariable.
The multiple events within subject are not independent, so variance estimates must be corrected for intracluster correlation. The clustered sandwich
covariance matrix estimator described in Section 9.5 and in [400] will provide
consistent estimates of variances and covariances even if the events are dependent. Lin400 also discussed how this method can easily be used to model
multiple events of di↵ering types.

17.7 R Functions
The event.chart function of Lee et al.386 will draw a variety of charts for
displaying raw survival time data, for both single and multiple events per subject. Relationships with covariables can also be displayed. The event.history
function of Dubin et al.163 draws an event history graph for right-censored
survival data, including time-dependent covariate status. These function are
in the Hmisc package.
The analyses described in this chapter can be viewed as special cases
of the Cox proportional hazards model.129 The programs for Cox model
analyses described in Section 20.12 can be used to obtain the results described here, as long as there is at least one stratification factor in the
model. There are, however, several R functions that are pertinent to the
homogeneous or stratified case. The R function survfit, and its particular renditions of the print, plot, lines, and points generic functions (all part
of the survival package written by Terry Therneau), will compute, print,
and plot Kaplan–Meier and Nelson survival estimates. Confidence intervals
for S(t) may be based on S, ⇤, or log ⇤. The rms package’s front-end to
the survival package’s survfit function is npsurv for “nonparametric survival”. It and other functions described in later chapters use Therneau’s
Surv function to combine the response variable and event indicator into a
single R “survival time” object. In its simplest form, use Surv(y, event),
where y is the failure/right–censoring time and event is the event/censoring indicator, usually coded T/F, 0 = censored 1 = event or 1 = censored
2 = event. If the event status variable has other coding (e.g., 3 means death),
use Surv(y, s==3). To handle interval time-dependent covariables, or to

17.7 R Functions

425

use Andersen and Gill’s counting process formulation of the Cox model,23
use the notation Surv(tstart, tstop, status). The counting process notation allows subjects to enter and leave risk sets at random. For each time
interval for each subject, the interval is made up of tstart< t tstop.
For time-dependent stratification, there is an optional origin argument to
Surv that indicates the hazard shape time origin at the time of crossover
to a new stratum. A type argument is used to handle left– and interval–
censoring, especially for parametric survival models. Possible values of type
are "right","left","interval","counting","interval2","mstate".
The Surv expression will usually be used inside another function, but it is
fine to save the result of Surv in another object and to use this object in the
particular fitting function.
npsurv is invoked by the following, with default parameter settings indicated.
r e q u i r e ( rms )
units (y)
”Month”
# Default is "Day" - used for axis labels, etc.
npsurv ( Surv ( y , e v e n t ) ⇠ s v a r 1 + s v a r 2 + . . . , data , s u b s e t ,
t y p e=c ( ” kaplan meier ” , ” f l e m i n g h a r r i n g t o n ” , ” f h 2 ” ) ,
e r r o r=c ( ” greenwood ” , ” t s i a t i s ” ) , s e . f i t =TRUE,
c o n f . i n t=. 9 5 ,
c o n f . t y p e=c ( ” l o g ” , ” l o g l o g ” , ” p l a i n ” , ” none ” ) , . . . )

If there are no stratification variables (svar1, . . . ), omit them. To print a
table of estimates, use
f
npsurv ( . . . )
print ( f )
# print brief summary of f
summary ( f , t i m e s , c e n s o r e d=FALSE)
# in survival

For failure times stored in days, use
f
npsurv ( Surv ( f u t i m e , e v e n t ) ⇠ s e x )
summary ( f , s e q ( 3 0 , 1 8 0 , by =30))

to print monthly estimates.
There is a plot method To plot the object returned by survfit and
npsurv. This invokes plot.survfit.
Objects created by npsurv can be passed to the more comprehensive plotting function survplot (here, actually survplot.npsurv) for other options
that include automatic curve labeling and showing the number of subjects at
risk at selected times. See Figure 17.6 for an example. Stratified estimates,
with four treatments distinguished by line type and curve labels, could be
drawn by
units (y)
” Year ”
f
npsurv ( Surv ( y , s t a t ) ⇠ t r e a t m e n t )
s u r v p l o t ( f , y l a b=” F r a c t i o n Pain Free ” )

The groupkm in rms computes and optionally plots SKM (u) or log ⇤KM (u)
(if loglog=TRUE) for fixed u with automatic stratification on a continuous

426

17 Introduction to Survival Analysis

predictor x. As in cut2 (Section 6.2) you can specify the number of subjects
per interval (default is m=50), the number of quantile groups (g), or the actual cutpoints (cuts). groupkm plots the survival or log–log survival estimate
against mean x in each x interval.
The bootkm function in the Hmisc package bootstraps Kaplan–Meier survival estimates or Kaplan–Meier estimates of quantiles of the survival time
distribution. It is easy to use bootkm to compute, for example, a nonparametric confidence interval for the ratio of median survival times for two groups.
See the Web site for a list of functions from other users for nonparametric
estimation of S(t) with left–, right–, and interval–censored data. The adaptive
linear spline log-hazard fitting function heft354 is freely available.

17.8 Further Reading
1

2
3
4
5
6

7

8

9

Some excellent general references for survival analysis are [56, 82, 111, 130, 151,
193, 278, 301, 324, 343, 374, 384, 437, 477, 570, 597]. Govindarajulu et al.224 have a nice
review of frailty models in survival analysis, for handling clustered time-to-event
data.
See Goldman,216 Bull and Spiegelhalter,82 , Lee et al.386 , and Dubin et al.163
for ways to construct descriptive graphs depicting right–censored data.
Some useful references for left–truncation are [82, 109, 239, 519]. Mandel428 carefully described the di↵erence between censoring and truncation.
See [376, p. 164] for some ideas for detecting informative censoring. Bilker and
Wang53 discuss right–truncation and contrast it with right–censoring.
Arjas29 has applications based on properties of the cumulative hazard function.
Kooperberg et al.354, 588 have an adaptive method for fitting hazard functions
using linear splines in the log hazard. Binquet et al.55 studied a related approach
using quadratic splines. Mudholkar et al.459 presented a generalized Weibull
model allowing for a variety of hazard shapes.
Hollander et al.292 provide a nonparametric simultaneous confidence band for
S(t), surprisingly using likelihood ratio methods. Miller452 showed that if the
parametric form of S(t) is known to be Weibull with known shape parameter (an
unlikely scenario), the Kaplan–Meier estimator is very inefficient (i.e., has high
variance) when compared with the parametric maximum likelihood estimator.
See [660] for a discussion of how the efficiency of Kaplan–Meier estimators can
be improved by interpolation as opposed to piecewise flat step functions. That
paper also discusses a variety of other estimators, some of which are significantly
more efficient than Kaplan–Meier.
See [109, 239, 431, 566, 607, 612] for methods of estimating S or ⇤ in the presence of
left–truncation. See Turnbull609 for nonparametric estimation of S(t) with left–
, right–, and interval–censoring, and Kooperberg and Clarkson353 for a flexible
parametric approach to modeling that allows for interval–censoring. Lindsey
and Ryan406 have a nice tutorial on the analysis of interval–censored data.
Hogan and Laird290, 291 developed methods for dealing with mixtures of fatal and nonfatal outcomes, including some ideas for handling outcome-related
dropouts on the repeated measurements. See also Finkelstein and Schoenfeld.189
The 30 April 1997 issue of Statistics in Medicine (Vol. 16) is devoted to methods for analyzing multiple endpoints as well as designing multiple endpoint
studies. The papers in that issue are invaluable, as is Therneau and Hamilton599 and Therneau and Grambsch.597 Huang and Wang304 presented a joint

17.9 Problems

10

11

12
13
14

427

model for recurrent events and a terminating event, addressing such issues as
the frequency of recurrent events by the time of the terminating event.
See Lunn and McNeil422 and Marubini and Valsecchi [437, Chapter 10] for
practical approaches to analyzing competing risks using ordinary Cox proportional hazards models. A nice overview of competing risks with comparisons of
various approaches is found in Tai et al.593 , Geskus210 , and Koller et al.351 .
Bryant and Dignam77 developed a semiparametric procedure in which competing risks are adjusted for nonparametrically while a parametric cumulative
incidence function is used for the event of interest, to gain precision. Fine and
Gray188 developed methods for analyzing competing risks by estimating subdistribution functions. Nishikawa et al.471 developed some novel approaches to
competing risk analysis involving time to adverse drug events competing with
time to withdrawal from therapy. They also dealt with di↵erent severities of
events in an interesting way. Putter et al.510 has a nice tutorial on competing
risks, multi-state models, and associated R software. Fiocco et al.190 developed
an approach to avoid the problems caused by having to estimate a large number of regression coefficients in multi-state models. Ambrogi et al.22 provide
clinically useful estimates from competing risks analyses.
Jiang, Chappell, and Fine315 present methods for estimating the distribution
of event times of nonfatal events in the presence of terminating events such as
death.
Shen and Thall563 have developed a flexible parametric approach to multi-state
survival analysis.
Lancar et al.365 developed a method for analyzing repeated events of varying
severities.
Lawless and Nadeau376 have a very good description of models dealing with
recurrent events. They use the notion of the cumulative mean function, which
is the expected number of events experienced by a subject by a certain time.
Lawless375 contrasts this approach with other approaches. See Aalen et al.3
for a nice example in which multivariate failure times (time to failure of fillings in multiple teeth per subject) are analyzed. Francis and Fuller200 developed a graphical device for depicting complex event history data. Therneau and
Hamilton599 have very informative comparisons of various methods for modeling multiple events, showing the importance of whether the analyst starts the
clock over after each event. Kelly and Lim337 have another very useful paper
comparing various methods for analyzing recurrent events. Wang and Chang643
demonstrated the difficulty of using Kaplan–Meier estimates for recurrence time
data.

17.9 Problems
1. Make a rough drawing of a hazard function from birth for a man who develops significant coronary artery disease at age 50 and undergoes coronary
artery bypass surgery at age 55.
2. Define in words the relationship between the hazard function and the
survival function.
3. In a study of the life expectancy of light bulbs as a function of the bulb’s
wattage, 100 bulbs of various wattage ratings were tested until each had
failed. What is wrong with using the product-moment linear correlation

428

4.

5.

6.

7.
8.

17 Introduction to Survival Analysis

test to test whether wattage is associated with life length concerning (a)
distributional assumptions and (b) other assumptions?
A placebo-controlled study is undertaken to ascertain whether a new drug
decreases mortality. During the study, some subjects are withdrawn because of moderate to severe side e↵ects. Assessment of side e↵ects and
withdrawal of patients is done on a blinded basis. What statistical technique can be used to obtain an unbiased treatment comparison of survival
times? State at least one efficacy endpoint that can be analyzed unbiasedly.
Consider long-term follow-up of patients in the support dataset. What
proportion of the patients have censored survival times? Does this imply
that one cannot make accurate estimates of chances of survival? Make
a histogram or empirical distribution function estimate of the censored
follow-up times. What is the typical follow-up duration for a patient in
the study who has survived so far? What is the typical survival time for
patients who have died? Taking censoring into account, what is the median
survival time from the Kaplan–Meier estimate of the overall survival function? Estimate the median graphically or using any other sensible method.
Plot Kaplan–Meier survival function estimates stratified by dzclass. Estimate the median survival time and the first quartile of time until death
for each of the four disease classes.
Repeat Problem 6 except for tertiles of meanbp.
The commonly used log-rank test for comparing survival times between
groups of patients is a special case of the test of association between the
grouping variable and survival time in a Cox proportional hazards regression model. Depending on how one handles tied failure times, the log-rank
2
statistic exactly equals the score 2 statistic from the Cox model, and
the likelihood ratio and Wald 2 test statistics are also appropriate. To
obtain global score or LR 2 tests and P -values you can use a statement
as the following, where cph is in the rms package. It is similar to the
survival package’s coxph function.
cph ( S u r v o b j e c t ⇠ p r e d i c t o r )

Here Survobject is a survival time object created by the Surv function.
Obtain the log-rank (score) 2 statistic, degrees of freedom, and P -value
for testing for di↵erences in survival time between levels of dzclass. Interpret this test, referring to the graph you produced in Problem 6 if needed.
9. Do preliminary analyses of survival time using the Mayo Clinic primary biliary cirrhosis dataset described in Section 8.9. Make graphs of Altschuler–
Nelson or Kaplan–Meier survival estimates stratified separately by a few
categorical predictors and by categorized versions of one or two continuous
predictors. Estimate median failure time for the various strata. You may
want to suppress confidence bands when showing multiple strata on one
graph. See [354] for parametric fits to the survival and hazard function for
this dataset.

Chapter 18

Parametric Survival Models

18.1 Homogeneous Models (No Predictors)
The nonparametric estimator of S(t) is a very good descriptive statistic for
displaying survival data. For many purposes, however, one may want to make
more assumptions to allow the data to be modeled in more detail. By specifying a functional form for S(t) and estimating any unknown parameters in
this function, one can
1. easily compute selected quantiles of the survival distribution;
2. estimate (usually by extrapolation) the expected failure time;
3. derive a concise equation and smooth function for estimating S(t), ⇤(t),
and (t); and
4. estimate S(t) more precisely than SKM (t) or S⇤ (t) if the parametric form
is correctly specified.

18.1.1 Specific Models
Parametric modeling requires choosing one or more distributions. The Weibull
and exponential distributions were discussed in Chapter 18. Other commonly
used survival distributions are obtained by transforming T and using a standard distribution. The log transformation is most commonly employed. The
log-normal distribution specifies that log(T ) has a normal distribution with
mean µ and variance 2 . Stated another way, log(T ) ⇠ µ + ✏, where ✏
has a standard normal distribution. Then S(t) = 1
((log(t) µ)/ ),
where is the standard normal cumulative distribution function. The loglogistic distribution is given by S(t) = [1 + exp( (log(t) µ)/ )] 1 . Here
log(T ) ⇠ µ + ✏ where ✏ follows a logistic distribution [1 + exp( u)] 1 . The
log extreme value distribution is given by S(t) = exp[ exp((log(t) µ)/ )],
and log(T ) ⇠ µ + ✏, where ✏ ⇠ 1 exp[ exp(u)].
429

430

18 Parametric Survival Models

The generalized gamma and generalized F distributions provide a richer
variety of distribution and hazard functions124, 125 . Spline hazard models280, 281, 354 are other excellent alternatives.

18.1.2 Estimation
Maximum likelihood (ML) estimation is used to estimate the unknown parameters of S(t). The general method presented in Chapter 9 must be augmented, however, to allow for censored failure times. The basic idea is as
follows. Again let T be a random variable representing time until the event,
Ti be the (possibly censored) failure time for the ith observation, and Yi
denote the observed failure or censoring time min(Ti , Ci ), where Ci is the
censoring time. If Yi is uncensored, observation i contributes a factor to the
likelihood equal to the density function for T evaluated at Yi , f (Yi ). If Yi
instead represents a censored time so that Ti = Yi+ , it is only known that
Ti exceeds Yi . The contribution to the likelihood function is the probability
that Ti > Ci (equal to Prob{Ti > Yi }). This probability is S(Yi ). The joint
likelihood over all observations i = 1, 2, . . . , n is
L=

n
Y

f (Yi )

i:Yi uncensored

n
Y

S(Yi ).

(18.1)

i:Yi censored

There is one more component to L: the distribution of censoring times if
these are not fixed in advance. Recall that we assume that censoring is noninformative, that is, it is independent of the risk of the event. This independence implies that the likelihood component of the censoring distribution
simply multiplies L and that the censoring distribution contains little information about the survival distribution. In addition, the censoring distribution
may be very difficult to specify. For these reasons we can maximize L separately to estimate parameters of S(t) and ignore the censoring distribution.
Recalling that f (t) = (t)S(t) and ⇤(t) = log S(t), the log likelihood
can be written as
log L =

n
X

i:Yi uncensored

log (Yi )

n
X

⇤(Yi ).

(18.2)

i=1

All observations then contribute an amount to the log likelihood equal to the
negative of the cumulative hazard evaluated at the failure/censoring time.
In addition, uncensored observations contribute an amount equal to the log
of the hazard function evaluated at the time of failure. Once L or log L
is specified, the general ML methods outlined earlier can be used without
change in most situations. The principal di↵erence is that censored observations contribute less information to the statistical inference than uncensored

18.1 Homogeneous Models (No Predictors)

431

observations. For distributions such as the log-normal that are written only
in terms of S(t), it may be easier to write the likelihood in terms of S(t) and
f (t).
As an example, we turn to the exponential distribution, for which log
L has a simple form that can be maximized explicitly. Recall that for this
distribution (t) = and ⇤(t) = t. Therefore,
log L =

n
X

n
X

log

Yi .

(18.3)

i=1

i:Yi uncensored

Letting nu denote the number of uncensored event times,
log L = nu log

n
X

Yi .

(18.4)

i=1

Letting w denote the sum of all failure/censoring times (“person years of
exposure”):
n
X
w=
Yi ,
(18.5)
i=1

the derivatives of log L are given by

@ log L
= nu /
@
@ 2 log L
= nu /
@ 2

w
2

.

Equating the derivative of log L to zero implies that the MLE of
ˆ = nu /w

(18.6)
is
(18.7)

or the number of failures per person-years of exposure. By inserting the MLE
of into the formula for the second derivative we obtain the observed estimated information, w2 /nu . The estimated variance of ˆ is thus nu /w2 and
1/2
the standard error is nu /w. The precision of the estimate depends primarily
on nu .
Recall that the expected life length µ is 1/ for the exponential distribution. The MLE of µ is w/nu and its estimated variance is w2 /n3u . The MLE
ˆ
of S(t), Ŝ(t), is exp( ˆ t), and the estimated variance of log(⇤(t))
is simply
1/nu .
As an example, consider the sample listed previously,
1 3 3 6+ 8+ 9 10+ .
Here nu = 4 and w = 40, so the MLE of is 0.1 failure per person-period.
The estimated standard error is 2/40 = 0.05. Estimated expected life length

432

18 Parametric Survival Models

is 10 units with a standard error of 5 units. Estimated median failure time is
log(2)/0.1 = 6.931. The estimated survival function is exp( 0.1t), which at
t = 1, 3, 9, 10 yields 0.90, 0.74, 0.41, and 0.37, which can be compared to the
product limit estimates listed earlier (0.85, 0.57, 0.29, 0.29).
Now consider the Weibull distribution. The log likelihood function is
log L =

n
X

log[↵ Yi

1

]

n
X

↵Yi .

(18.8)

i=1

i:Yi uncensored

Although log L can be simplified somewhat, it cannot be solved explicitly for
↵ and . An iterative method such as the Newton–Raphson method is used
to compute the MLEs of ↵ and . Once these estimates are obtained, the
estimated variance–covariance matrix and other derived quantities such as
Ŝ(t) can be obtained in the usual manner.
For the dataset used in the exponential fit, the Weibull fit follows.
↵
ˆ = 0.0728
ˆ = 1.164
Ŝ(t) = exp( 0.0728t1.164 )
Ŝ

1

(0.5) = [(log 2)/ˆ
↵]

1/ˆ

(18.9)

= 6.935 (estimated median).

This fit is very close to the exponential fit since ˆ is near 1.0. Note that the
two medians are almost equal. The predicted survival probabilities for the
Weibull model for t = 1, 3, 9, 10 are, respectively, 0.93, 0.77, 0.39, 0.35.
Sometimes a formal test can be made to assess the fit of the proposed
parametric survival distribution. For the data just analyzed, a formal test of
exponentiality versus a Weibull alternative is obtained by testing H0 : = 1
in the Weibull model. A score test yielded 2 = 0.14 with 1 d.f., p = 0.7,
showing little evidence for non-exponentiality (note that the sample size is
too small for this test to have any power).

18.1.3 Assessment of Model Fit
The fit of the hypothesized survival distribution can often be checked easily using graphical methods. Nonparametric estimates of S(t) and ⇤(t)
are primary tools for this purpose. For example, the Weibull distribution
S(t) = exp( ↵t ) can be rewritten by taking logarithms twice:
log[ log S(t)] = log ⇤(t) = log ↵ + (log t).

(18.10)

ˆ versus log t
The fit of a Weibull model can be assessed by plotting log ⇤(t)
and checking whether the curve is approximately linear. Also, the plotted
curve provides approximate estimates of ↵ (the antilog of the intercept) and

18.2 Parametric Proportional Hazards Models

433

(the slope). Since an exponential distribution is a special case of a Weibull
distribution when = 1, exponentially distributed data will tend to have a
graph that is linear with a slope of 1.
For any assumed distribution S(t), a graphical assessment of goodness of
fit can be made by plotting S 1 [S⇤ (t)] or S 1 [SKM (t)] against t and checking
for linearity. For log distributions, S specifies the distribution of log(T ), so
1
we plot against log t. For a log-normal distribution we thus plot
[S⇤ (t)]
1
against log t, where
is the inverse of the standard normal cumulative
distribution function. For a log-logistic distribution we plot logit[S⇤ (t)] versus
log t. For an extreme value distribution we use log log plots as with the
Weibull distribution. Parametric model fits can also be checked by plotting
the fitted Ŝ(t) and S⇤ (t) against t on the same graph.

18.2 Parametric Proportional Hazards Models
In this section we present one way to generalize the survival model to a
survival regression model. In other words, we allow the sample to be heterogeneous by adding predictor variables X = {X1 , X2 , . . . , Xk }. As with other
regression models, X can represent a mixture of binary, polytomous, continuous, spline-expanded, and even ordinal predictors (if the categories are
scored to satisfy the linearity assumption). Before discussing ways in which
the regression part of a survival model might be specified, first recall how
regression e↵ects have been modeled in other settings. In multiple linear regression, the regression e↵ect X = 0 + 1 X1 + 2 X2 + . . . + k Xk can
be thought of as an increment in the expected value of the response Y . In
binary logistic regression, X specifies the log odds that Y = 1, or exp(X )
multiplies the odds that Y = 1.

18.2.1 Model
The most widely used survival regression specification is to allow the hazard
function (t) to be multiplied by exp(X ). The survival model is thus generalized from a hazard function (t) for the failure time T to a hazard function
(t) exp(X ) for the failure time given the predictors X:
(t|X) = (t) exp(X ).

(18.11)

This regression formulation is called the proportional hazards (PH) model.
The (t) part of (t|X) is sometimes called an underlying hazard function or
a hazard function for a standard subject, which is a subject with X = 0. Any
parametric hazard function can be used for (t), and as we show later, (t)

434

18 Parametric Survival Models

can be left completely unspecified without sacrificing the ability to estimate
, by the use of Cox’s semi-parametric PH model.129 Depending on whether
the underlying hazard function (t) has a constant scale parameter, X may
or may not include an intercept 0 . The term exp(X ) can be called a relative
hazard function and in many cases it is the function of primary interest as it
describes the (relative) e↵ects of the predictors.
The PH model can also be written in terms of the cumulative hazard and
survival functions:
⇤(t|X) = ⇤(t) exp(X )
S(t|X) = exp[ ⇤(t) exp(X )] = exp[ ⇤(t)]exp(X ) .

(18.12)

⇤(t) is an “underlying” cumulative hazard function. S(t|X), the probability
of surviving past time t given the values of the predictors X, can also be
written as
S(t|X) = S(t)exp(X ) ,
(18.13)
where S(t) is the “underlying” survival distribution, exp( ⇤(t)). The e↵ect
of the predictors is to multiply the hazard and cumulative hazard functions
by a factor exp(X ), or equivalently to raise the survival function to a power
equal to exp(X ).

18.2.2 Model Assumptions and Interpretation of
Parameters
In the general regression notation of Section 2.2, the log hazard or log cumulative hazard can be used as the property of the response T evaluated at time
t that allows distributional and regression parts to be isolated and checked.
The PH model can be linearized with respect to X using the following
identities.
log (t|X) = log (t) + X
log ⇤(t|X) = log ⇤(t) + X .

(18.14)

No matter which of the three model statements are used, there are certain
assumptions in a parametric PH survival model. These assumptions are listed
below.
1. The true form of the underlying functions ( , ⇤, and S) should be specified
correctly.
2. The relationship between the predictors and log hazard or log cumulative
hazard should be linear in its simplest form. In the absence of interaction
terms, the predictors should also operate additively.

18.2 Parametric Proportional Hazards Models

435

3. The way in which the predictors a↵ect the distribution of the response
should be by multiplying the hazard or cumulative hazard by exp(X )
or equivalently by adding X to the log hazard or log cumulative hazard
at each t. The e↵ect of the predictors is assumed to be the same at all
values of t since log (t) can be separated from X . In other words, the
PH assumption implies no t by predictor interaction.
The regression coefficient for Xj , j , is the increase in log hazard or log
cumulative hazard at any fixed point in time if Xj is increased by one unit
and all other predictors are held constant. This can be written formally as
log (t|X1 , . . . , Xj , . . . , Xk ),
(18.15)
which is equivalent to the log of the ratio of the hazards at time t. The
regression coefficient can just as easily be written in terms of a ratio of hazards
at time t. The ratio of hazards at Xj + d versus Xj , all other factors held
constant, is exp( j d). Thus the e↵ect of increasing Xj by d is to increase the
hazard of the event by a factor of exp( j d) at all points in time, assuming Xj
is linearly related to log (t). In general, the ratio of hazards for an individual
with predictor variable values X ⇤ compared to an individual with predictors
X is
j

= log (t|X1 , X2 , . . . , Xj + 1, Xj+1 , . . . , Xk )

X ⇤ : X hazard ratio = [ (t) exp(X ⇤ )]/[ (t) exp(X )]
= exp(X ⇤ )/ exp(X ) = exp[(X ⇤

X) ].(18.16)

If there is only one predictor X1 and that predictor is binary, the PH model
can be written
(t|X1 = 0) = (t)
(t|X1 = 1) = (t) exp(

1 ).

(18.17)

Here exp( 1 ) is the X1 = 1 : X1 = 0 hazard ratio. This simple case has
no regression assumption but assumes PH and a form for (t). If the single
predictor X1 is continuous, the model becomes
(t|X1 ) = (t) exp(

1 X).

(18.18)

Without further modification (such as taking a transformation of the predictor), the model assumes a straight line in the log hazard or that for all t, an
increase in X by one unit increases the hazard by a factor of exp( 1 ).
As in logistic regression, much more general regression specifications can
be made, including interaction e↵ects. Unlike logistic regression, however, a
model containing, say age, sex, and age ⇥ sex interaction is not equivalent to
fitting two separate models. This is because even though males and females
are allowed to have unequal age slopes, both sexes are assumed to have the

436

18 Parametric Survival Models

Table 18.1

Subject 5-Year Di↵erence
Survival
C T
1
0.98 0.99
0.01
2
0.80 0.89
0.09
3
0.25 0.50
0.25

underlying hazard function proportional to
holds for sex in addition to age).

Mortality
Ratio (T/C)
0.01/0.02 = 0.5
0.11/0.2 = 0.55
0.5/0.75 = 0.67

(t) (i.e., the PH assumption

18.2.3 Hazard Ratio, Risk Ratio, and Risk Di↵erence
Other ways of modeling predictors can also be specified besides a multiplicative e↵ect on the hazard. For example, one could postulate that the e↵ect of
a predictor is to add to the hazard of failure instead of to multiply it by a
factor. The e↵ect of a predictor could also be described in terms of a mortality ratio (relative risk), risk di↵erence, odds ratio, or increase in expected
failure time. However, just as an odds ratio is a natural way to describe an
e↵ect on a binary response, a hazard ratio is often a natural way to describe
an e↵ect on survival time. One reason is that a hazard ratio can be constant.
Table 18.1 provides treated (T) to control (C) survival (mortality) differences and mortality ratios for three hypothetical types of subjects. We
suppose that subjects 1, 2, and 3 have increasingly worse prognostic factors.
For example, the age at baseline of the subjects might be 30, 50, and 70 years,
respectively. We assume that the treatment a↵ects the hazard by a constant
multiple of 0.5 (i.e., PH is in e↵ect and the constant hazard ratio is 0.5). Note
0.5
that ST = SC
. Notice that the mortality di↵erence and ratio depend on the
survival of the control subject. A control subject having “good” predictor
values will leave little room for an improved prognosis from the treatment.
The hazard ratio is a basis for describing the mechanism of an e↵ect. In the
above example, it is reasonable that the treatment a↵ect each subject by lowering her hazard of death by a factor of 2, even though less sick subjects have
a low mortality di↵erence. Hazard ratios also lead to good statistical tests
for di↵erences in survival patterns and to predictive models. Once the model
is developed, however, survival di↵erences may better capture the impact of
a risk factor. Absolute survival di↵erences rather than relative di↵erences
(hazard ratios) also relate more closely to statistical power. For example,
even if the e↵ect of a treatment is to halve the hazard rate, a population
where the control survival is 0.99 will require a much larger sample than will
a population where the control survival is 0.3.

18.2 Parametric Proportional Hazards Models

437

Figure 18.1 depicts the relationship between survival S(t) of a control
subject at any time t, relative reduction in hazard (h), and di↵erence in
survival S(t) S(t)h . This figure demonstrates that absolute clinical benefit

0.1

Improvement in Survival

0.7
0.6

0.2

0.5
0.3

0.4
0.4

0.3

0.5
0.6

0.2

0.7

0.1

0.8
0.9

0.0
0.0

0.2

0.4

0.6

0.8

1.0

Survival for Control Subject
Fig. 18.1 Absolute clinical benefit as a function of survival in a control subject and
the relative benefit (hazard ratio). The hazard ratios are given for each curve.

is primarily a function of the baseline risk of a subject. Clinical benefit will
also be a function of factors that interact with treatment, that is, factors
that modify the relative benefit of treatment. Once a model is developed
for estimating S(t|X), this model can be used to estimate absolute benefit
as a function of baseline risk factors as well as factors that interact with a
treatment. Let X1 be a binary treatment indicator and let A = {X2 , . . . , Xp }
be the other factors (which for convenience we assume do not interact with
X1 ). Then the estimate of S(t|X1 = 0, A) S(t|X1 = 1, A) can be plotted
against S(t|X1 = 0) or against levels of variables in A to display absolute
benefit versus overall risk or specific subject characteristics.

18.2.4 Specific Models
Let X denote the linear combination of predictors excluding an intercept
term. Using the PH formulation, an exponential survival regression model214
can be stated as

1

438

18 Parametric Survival Models

(t|X) =

exp(X )

S(t|X) = exp[

t exp(X )] = exp(

t)exp(X ) .

(18.19)

The parameter can be thought of as the antilog of an intercept term since
the model could be written (t|X) = exp[(log ) + X ]. The e↵ect of X on
the expected or median failure time is as follows.
E{T |X} = 1/[ exp(X )]

T0.5 |X = (log 2)/[ exp(X )].

(18.20)

The exponential regression model can be written in another form that is more
numerically stable by replacing the parameter with an intercept term in
X , specifically = exp( 0 ). After redefining X to include 0 , can be
dropped in all the above formulas.
The Weibull regression model is defined by one of the following functions
(assuming that X does not contain an intercept).
(t|X) = ↵ t

1

exp(X )

⇤(t|X) = ↵t exp(X )
S(t|X) = exp[ ↵t exp(X )]

(18.21)

= [exp( ↵t )]exp(X ) .
Note that the parameter ↵ in the homogeneous Weibull model has been
replaced with ↵ exp(X ). The median survival time is given by
T0.5 |X = {log 2/[↵ exp(X )]}1/ .

(18.22)

As with the exponential model, the parameter ↵ could be dropped (and
replaced with exp( 0 )) if an intercept 0 is added to X .
For numerical reasons it is sometimes advantageous to write the Weibull
PH model as
S(t|X) = exp( ⇤(t|X)),
(18.23)
where
⇤(t|X) = exp( log t + X ).

(18.24)

18.2.5 Estimation
The parameters in and
are estimated by maximizing a log likelihood
function constructed in the same manner as described in Section 18.1. The
only di↵erence is the insertion of exp(Xi ) in the likelihood function:

18.2 Parametric Proportional Hazards Models

log L =

n
X

log[ (Yi ) exp(Xi )]

439
n
X

⇤(Yi ) exp(Xi ).

(18.25)

i=1

i:Yi uncensored

Once ˆ, the MLE of , is computed along with the large-sample standard
error estimates, hazard ratio estimates and their confidence intervals can
readily be computed. Letting s denote the estimated standard error of ˆj ,
a 1 ↵ confidence interval for the Xj + 1 : Xj hazard ratio is given by
exp[ ˆj ± zs], where z is the 1 ↵/2 critical value for the standard normal
distribution.
Once the parameters of the underlying hazard function are estimated, the
MLE of (t), ˆ (t), can be derived. The MLE of (t|X), the hazard as a
function of t and X, is given by
ˆ (t|X) = ˆ (t) exp(X ˆ).

(18.26)

ˆ
The MLE of ⇤(t), ⇤(t),
can be derived from the integral of ˆ (t) with respect
to t. Then the MLE of S(t|X) can be derived:
ˆ exp(X ˆ)].
Ŝ(t|X) = exp[ ⇤(t)

(18.27)

For the Weibull model, we denote the MLEs of the hazard parameters ↵ and
by ↵
ˆ and ˆ . The MLE of (t|X), ⇤(t|X), and S(t|X) for this model are
ˆ (t|X) = ↵
ˆ ˆ t ˆ 1 exp(X ˆ)
ˆ
⇤(t|X)
=↵
ˆ t ˆ exp(X ˆ)
ˆ
Ŝ(t|X) = exp[ ⇤(t|X)].

(18.28)

Confidence intervals for S(t|X) are best derived using general matrix notation
to obtain an estimate s of the standard error of log[ ˆ (t|X)] from the estimated
information matrix of all hazard and regression parameters. A confidence
interval for Ŝ will be of the form
Ŝ(t|X)exp(±zs) .

(18.29)

The MLEs of and of the hazard shape parameters lead directly to MLEs
of the expected and median life length. For the Weibull model the MLE of
the median life length given X is
T̂0.5 |X = {log 2/[ˆ
↵ exp(X ˆ)]}1/ˆ .

(18.30)

For the exponential model, the MLE of the expected life length for a subject
having predictor values X is given by
Ê(T |X) = [ ˆ exp(X ˆ)]

1

,

(18.31)

440

18 Parametric Survival Models

X1=1
β1
X1=0

t

Fig. 18.2 PH model with one binary predictor. Y -axis is log (t) or log ⇤(t). For log ⇤(t), the
curves must be non-decreasing. For log (t), they may be any shape.

where ˆ is the MLE of .

18.2.6 Assessment of Model Fit
Three assumptions of the parametric PH model were listed in Section 18.2.2.
We now lay out in more detail what relationships need to be satisfied. We
first assume a PH model with a single binary predictor X1 . For a general
underlying hazard function (t), all assumptions of the model are displayed
in Figure 18.2. In this case, the assumptions are PH and a shape for (t).
If (t) is Weibull, the two curves will be linear if log t is plotted instead
of t on the x-axis. Note also that if there is no association between X and
survival ( 1 = 0), estimates of the two curves will be close and will intertwine
due to random variability. In this case, PH is not an issue.
If the single predictor is continuous, the relationships in Figures 18.3
and 18.4 must hold. Here linearity is assumed (unless otherwise specified)
besides PH and the form of (t). In Figure 18.3, the curves must be parallel
for any choices of times t1 and t2 as well as each individual curve being linear. Also, the di↵erence between ordinates needs to conform to the assumed
distribution. This di↵erence is log[ (t2 )/ (t1 )] or log[⇤(t2 )/⇤(t1 )].
Figure 18.4 highlights the PH assumption. The relationship between the
two curves must hold for any two values c and d of X1 . The shape of the
function for a given value of X1 must conform to the assumed (t). For a
Weibull model, the functions should each be linear in log t.
When there are multiple predictors, the PH assumption can be displayed in
a way similar to Figures 18.2 and 18.4 but with the population additionally
cross-classified by levels of the other predictors besides X1 . If there is one
binary predictor X1 and one continuous predictor X2 , the relationship in

18.2 Parametric Proportional Hazards Models

441

t =t 2
t =t 1

X1

Fig. 18.3 PH model with one continuous predictor. Y -axis is log (t) or log ⇤(t); for log ⇤(t),
drawn for t2 > t1 . The slope of each line is

1.

X =d
1

(d-c) β 1

X =c
1

t

Fig. 18.4 PH model with one continuous predictor. Y -axis is log (t) or log ⇤(t). For log ,
the functions need not be monotonic.

Figure 18.5 must hold at each time t if linearity is assumed for X2 and there
is no interaction between X1 and X2 . Methods for verifying the regression
assumptions (e.g., splines and residuals) and the PH assumption are covered
in detail under the Cox PH model in Chapter 20.
The method for verifying the assumed shape of S(t) in Section 18.1.3 is also
useful when there are a limited number of categorical predictors. To validate
a Weibull PH model one can stratify on X and plot log ⇤KM (t|X stratum)
against log t. This graph simultaneously assesses PH in addition to shape
assumptions—all curves should be parallel as well as straight. Straight but
nonparallel (non-PH) curves indicate that a series of Weibull models with
di↵ering parameters will fit.

442

18 Parametric Survival Models

X1=1

X1=0
β1

slope = β 2
X2

Fig. 18.5 Regression assumptions, linear additive PH or AFT model with two predictors. For
PH, Y -axis is log (t) or log ⇤(t) for a fixed t. For AFT, Y -axis is log(T ).

18.3 Accelerated Failure Time Models
18.3.1 Model

2

3

Besides modeling the e↵ect of predictors by a multiplicative e↵ect on the
hazard function, other regression e↵ects can be specified. The accelerated
failure time (AFT) model is commonly used; it specifies that the predictors
act multiplicatively on the failure time or additively on the log failure time.
The e↵ect of a predictor is to alter the rate at which a subject proceeds along
the time axis (i.e., to accelerate the time to failure [324, pp. 33–35]). The
model is
S(t|X) = ((log(t) X )/ ),
(18.32)
where
is any standardized survival distribution function. The parameter
is called the scale parameter. The model can also be stated as (log(T )
X )/ ⇠
or log(T ) = X + ✏, where ✏ is a random variable from the
distribution . Sometimes the untransformed T is used in place of log(T ).
When the log form is used, the models are said to be log-normal, log-logistic,
and so on.
The exponential and Weibull are the only two distributions that can describe either a PH or an AFT model.

18.3.2 Model Assumptions and Interpretation of
Parameters
The log or log ⇤ transformation of the PH model has the following equivalent for AFT models.

hazard ratio->1 as
t->infinity

18.3 Accelerated Failure Time Models
1

443

[S(t|X)] = (log(t)

X )/ .

(18.33)

Letting as before ✏ denote a random variable from the distribution S, the
model is also
log(T ) = X + ✏.
(18.34)
So the property of the response T of interest for regression modeling is log(T ).
In the absence of censoring, we could check the model by plotting an X
against log T and checking that the residuals log(T ) X ˆ are distributed as
to within a scale factor.
The assumptions of the AFT model are thus the following.
1. The true form of (the distributional family) is correctly specified.
2. In the absence of nonlinear and interaction terms, each Xj a↵ects log(T )
1
or
[S(t|X)] linearly.
3. Implicit in these assumptions is that is a constant independent of X.
A one-unit change in Xj is then most simply understood as a j change in
the log of the failure time. The one-unit change in Xj increases the failure
time by a factor of exp( j ).
The median survival time is obtained by solving ((log(t) X )/ ) = 0.5
giving
1
T0.5 |X = exp[X +
(0.5)]
(18.35)

18.3.3 Specific Models
Common choices for the distribution function
in Equation 18.32 are the
extreme value distribution (u) = exp( exp(u)), the logistic distribution
(u) = [1 + exp(u)] 1 , and the normal distribution (u) = 1
(u). The
AFT model equivalent of the Weibull model is obtained by using the extreme
value distribution, negating , and replacing with 1/ in Equation 18.24:
S(t|X) = exp[ exp((log(t)

X )/ )]

T0.5 |X = [log(2)] exp(X ).
The exponential model is obtained by restricting
distribution.
The log-normal regression model is
S(t|X) = 1

(18.36)
= 1 in the extreme value

((log(t)

X )/ ),

S(t|X) = [1 + exp((log(t)

X )/ )]

(18.37)

and the log-logistic model is
1

.

(18.38)

444

18 Parametric Survival Models

The t distribution allows for more flexibility by varying the degrees of freedom. Figure 18.6 depicts possible hazard functions for the log t distribution
for varying and degrees of freedom. However, this distribution does not
have a late increasing hazard phase typical of human survival.
r e q u i r e ( rms )
haz
s u r v r e g . a u x i n f o $ t $ h az ard
times
c ( s e q ( 0 , . 2 5 , l e n g t h =100) , s e q ( . 2 6 , 2 , l e n g t h =150))
high
c (6 , 1 .5 , 1 .5 , 1 .75 )
low
c (0 , 0 , 0 , .25 )
dfs
c (1 , 2 , 3 , 5 , 7 , 15 , 500)
cols
rep (1 , 7)
ltys
1:7
i
0
f o r ( s c a l e in c ( .25 , .6 , 1 , 2)) {
i
i + 1
p l o t ( 0 , 0 , x l i m=c ( 0 , 2 ) , y l i m=c ( low [ i ] , h i g h [ i ] ) ,
x l a b=e x p r e s s i o n ( t ) , y l a b=e x p r e s s i o n ( lambda ( t ) ) , t y p e=”n” )
col
1 .09
j
0
f o r ( df in dfs ) {
j
j +1
## Divide by t to get hazard for log t distribution
l i n e s ( times ,
haz ( l o g ( t i m e s ) , 0 , c ( l o g ( s c a l e ) , d f ) ) / t i m e s ,
c o l=c o l s [ j ] , l t y=l t y s [ j ] )
i f ( i ==1) t e x t ( 1 . 7 , . 2 3 + haz ( l o g ( 1 . 7 ) , 0 ,
c ( l o g ( s c a l e ) , df ) ) /1 .7 , format ( df ) )
}
t i t l e ( paste ( ” S c a l e : ” , format ( s c a l e ) ) )
}
# Figure 18.6

All three of these parametric survival models have median survival time
T0.5 |X = exp(X ).

18.3.4 Estimation
Maximum likelihood estimation is used much the same as in Section 18.2.5.
Care must be taken in the choice of initial values; iterative methods are
especially prone to problems in choosing the initial ˆ . Estimation works better
if is parameterized as exp( ). Once and (exp( )) are estimated, MLEs of
secondary parameters such as survival probabilities and medians can readily
be obtained:
Ŝ(t|X) = ((log(t) X ˆ)/ˆ )
T̂0.5 |X = exp[X ˆ + ˆ 1 (0.5)].

(18.39)

18.3 Accelerated Failure Time Models

445

Scale: 0.25
500

6
5

1.5

15

4

1.0

7
5
3
2
1

3
2
1

λ(t)

λ(t)

Scale: 0.6

0.5

0

0.0
0.0

0.5

1.0

1.5

2.0

0.0

0.5

1.0

1.5

t

t

Scale: 1

Scale: 2

2.0

1.5
1.5

λ(t)

λ(t)

1.0
1.0

0.5
0.5
0.0
0.0

0.5

1.0

1.5

2.0

t

0.0

0.5

1.0

1.5

2.0

t

Fig. 18.6 log(T ) distribution for
= 0.25, 0.6, 1, 2 and for degrees of freedom
1, 2, 3, 5, 7, 15, 500 (almost log-normal). The top left plot has degrees of freedom written in the plot.

For normal and logistic distributions, T̂0.5 |X = exp(X ˆ). The MLE of the
e↵ect on log(T ) of increasing Xj by d units is ˆj d if Xj is linear and additive.
The delta (statistical di↵erential) method can be used to compute an estimate of the variance of f = [log(t) X ˆ]/ˆ . Let ( ˆ, ˆ) denote the estimated
parameters, and let V̂ denote the estimated covariance matrix for these parameter estimates. Let F denote the vector of derivatives of f with respect to
( 0 , 1 , . . . , p , ); that is, F = [ 1, X1 , X2 , . . . , Xp , (log(t) X ˆ)]/ˆ .
The variance of f is then approximately
Var(f ) = F V̂ F 0 .

(18.40)

446

18 Parametric Survival Models

Letting s be the square root of the variance estimate and z1
normal critical value, a 1 ↵ confidence limit for S(t|X) is
((log(t)

X ˆ)/ˆ ± z1

↵/2

⇥ s).

↵/2

be the

(18.41)

18.3.5 Residuals
For an AFT model, standardized residuals are simply
r = (log(T )
4

X ˆ)/ .

(18.42)

When T is right-censored, r is right-censored. Censoring must be taken into
account, for example, by displaying Kaplan–Meier estimates based on groups
of residuals rather than showing individual residuals. The residuals can be
used to check for lack of fit as described in the next section. Note that examining individual uncensored residuals is not appropriate, as their distribution
is conditional on Ti < Ci , where Ci is the censoring time.
Cox and Snell131 proposed a type of general residuals that also work for
censored data. Using their method on the cumulative probability scale results
in the probability integral transformation. If the probability of failure before
time t given X is S(t|X), F (T |X) = 1 S(T |X) has a uniform [0, 1] distribution, where T is a subject’s actual failure time. When T is right-censored,
so is 1 S(T |X). Substituting Ŝ for S results in an approximate uniform
[0, 1] distribution for any value of X. One minus the Kaplan–Meier estimate
of 1 Ŝ(T |X) (using combined data for all X) is compared against a 45
line to check for goodness of fit. A more stringent assessment is obtained by
repeating this process while stratifying on X.

18.3.6 Assessment of Model Fit
For a single binary predictor, all assumptions of the AFT model are depicted
in Figure 18.7. That figure also shows the assumptions for any two values of
a single continuous predictor that behaves linearly. For a single continuous
predictor, the relationships in Figure 18.8 must hold for any two follow-up
times. The regression assumptions are isolated in Figure 18.5.
To verify the fit of a log-logistic model with age as the only predictor, one
could stratify by quartiles of age and check for linearity and parallelism of the
four logit S⇤ (t) or SKM (t) curves over increasing t as in Figure 18.7, which
stresses the distributional assumption (no T by X interaction and linearity vs.
log(t)). To stress the linear regression assumption while checking for absence
of time interactions (part of the distributional assumptions), one could make

18.3 Accelerated Failure Time Models

447

X =c
1

(d-c) β1 /σ

X =d
1

log t

Fig. 18.7 AFT model with one predictor. Y -axis is
for d > c. The slope of the lines is

1

1

[S(t|X)] = (log(t)

X )/ . Drawn

.

t =t 2
t =t 1

X1
1
[S(t|X)] = (log(t) X )/ .
and the di↵erence between the lines is

Fig. 18.8 AFT model with one continuous predictor. Y -axis is
Drawn for t2 > t1 . The slope of each line is
log(t2 /t1 )/ .

1/

a plot like Figure 18.8. For each decile of age, the logit transformation of the
1-, 3-, and 5-year survival estimates for that decile would be plotted against
the mean age in the decile. This checks for linearity and constancy of the
age e↵ect over time. Regression splines will be a more e↵ective method for
checking linearity and determining transformations. This is demonstrated in
Chapter 20 with the Cox model, but identical methods apply here.
As an example, consider data from Kalbfleisch and Prentice [324, pp. 1-2],
who present data from Pike501 on the time from exposure to the carcinogen
DMBA to mortality from vaginal cancer in rats. The rats are divided into
two groups on the basis of a pre-treatment regime. Survival times in days
(with censored times marked + ) are found in Table 18.2.
getHdata ( k p r a t s )
k p r a t s $ group
f a c t o r ( k p r a t s $ group , 0 : 1 , c ( ’ Group 1 ’ , ’ Group 2 ’ ) )
dd
d a t a d i s t ( k p r a t s ) ; o p t i o n s ( d a t a d i s t=”dd” )

448

18 Parametric Survival Models

Table 18.2

Group 1 143
220
Group 2 142
233
344+

164
227
156
239

188
230
163
240

188
234
198
261

190
246
205
280

192
265
232
280

206
304
232
296

209
216+
233
296

213 216
244+
233 233
323 204+

S
with ( k p r a t s , Surv ( t , death ) )
f
npsurv ( S ⇠ group , t y p e=” f l e m i n g ” , data=k p r a t s )
s u r v p l o t ( f , n . r i s k=TRUE, c o n f= ’ none ’ ,
# Figure 18.9
l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) , l e v e l s . o n l y=TRUE)
t i t l e ( sub=” Nonparametric e s t i m a t e s ” , a d j =0 , c e x=. 7 )
# Check fits of Weibull, log-logistic, log-normal
xl
c (4 .8 , 5 . 9 )
s u r v p l o t ( f , l o g l o g=TRUE, l o g t=TRUE, c o n f=” none ” , x l i m=x l ,
l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) , l e v e l s . o n l y=TRUE)
t i t l e ( sub=” W e i b u l l ( extreme v a l u e ) ” , a d j =0 , c e x=. 7 )
s u r v p l o t ( f , f u n=f u n c t i o n ( y ) l o g ( y/ ( 1 y ) ) , y l a b=” l o g i t S ( t ) ” ,
l o g t=TRUE, c o n f=” none ” , x l i m=x l ,
l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) , l e v e l s . o n l y=TRUE)
t i t l e ( sub=” L o g l o g i s t i c ” , a d j =0 , c e x=. 7 )
s u r v p l o t ( f , f u n=qnorm , y l a b=” I n v e r s e Normal S ( t ) ” ,
l o g t=TRUE, c o n f=” none ” ,
x l i m=x l , c e x . l a b e l=. 7 ,
l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) , l e v e l s . o n l y=TRUE)
t i t l e ( sub=” Log normal ” , a d j =0 , c e x=. 7 )

The top left plot in Figure 18.9 displays nonparametric survival estimates for
the two groups, with the number of rats “at risk” at each 30-day mark written
above the x-axis. The remaining three plots are for checking assumptions of
three models. None of the parametric models presented will completely allow
for such a long period with no deaths. Neither will any allow for the early
crossing of survival curves. Log-normal and log-logistic models yield very similar results due to the similarity in shapes between (z) and [1 + exp( z)] 1
for non-extreme z. All three transformations show good parallelism after the
early crossing. The log-logistic and log-normal transformations are slightly
more linear. The fitted models are:
psm ( S ⇠ group , d a t a=k p r a t s ,
psm ( S ⇠ group , d a t a=k p r a t s ,
y=TRUE)
fn
psm ( S ⇠ group , d a t a=k p r a t s ,
l a t e x ( fw , f i = ’ ’ )

fw
fl

Prob{T

t} = exp[

d i s t= ’ w e i b u l l ’ )
d i s t= ’ l o g l o g i s t i c ’ ,
d i s t= ’ l o g n o r m a l ’ )

exp(

Xˆ=
5.450859

log(t) X
)] where
0.1832976

18.3 Accelerated Failure Time Models

449

log(−log Survival Probability)

Survival Probability

1.0

0.8

0.6
Group 1
0.4

Group 2

0.2

0.0

19

19

19

19

19

17

11

3

1

21

21

21

21

21

18

15

7

6

0

35

105

175

245

Group 1
2

Group 2

0

Group 1
Group 2

−1

−2

−3

315

4.8
−4

s
4

1

Nonparametric estimates

5.0

5.2

5.4

5.6

5.8

log Survival Time in s
Weibull (extreme value)

2.0
3

1.5

Inverse Normal S(t)

logit S(t)

2
1
0
Group 1
−1

Group 2

−2

1.0
0.5
0.0
Group 1
−0.5

Group 2

−1.0
−1.5

−3

−2.0
−4 4.8

5.0

5.2

5.4

5.6

5.8

6.0

4.8

log Survival Time in s

5.0

5.2

5.4

5.6

log Survival Time in s

Log−logistic

Log−normal

Fig. 18.9 Altschuler–Nelson–Fleming–Harrington nonparametric survival estimates
for rats treated with DMBA,501 along with various transformations of the estimates
for checking distributional assumptions of three parametric survival models. Thick
lines correspond to Group 2.

+0.131983[Group 2]

and [c] = 1 if subject is in group c, 0 otherwise.
latex ( fl ,

f i=’ ’ )

Prob{T

t} = [1 + exp(

log(t) X
)]
0.1159753

1

where

5.8

450

18 Parametric Survival Models

Table 18.3

Model

Group 2:1
Median Survival Time
Failure Time Ratio Group 1 Group 2
Extreme Value (Weibull)
1.14
217
248
Log-logistic
1.11
217
241
Log-normal
1.10
217
238

Xˆ=
5.375675
+0.1051005[Group 2]

and [c] = 1 if subject is in group c, 0 otherwise.
l a t e x ( fn ,

f i=’ ’ )

Prob{T

t} = 1

(

log(t) X
) where
0.2100184

Xˆ=
5.375328
+0.0930606[Group 2]

and [c] = 1 if subject is in group c, 0 otherwise.

The estimated failure time ratios and median failure times for the two
groups are given in Table 18.3. For example, the e↵ect of going from Group 1
to Group 2 is to increase log failure time by 0.132 for the extreme value model,
giving a Group 2:1 failure time ratio of exp(0.132) = 1.14. This ratio is also
the ratio of median survival times. We choose the log-logistic model for its
simpler form. The fitted survival curves are plotted with the nonparametric
estimates in Figure 18.10. Excellent agreement is seen, except for 150 to 180
days for Group 2. The standard error of the regression coefficient for group
in the log-logistic model is 0.0636 giving a Wald 2 for group di↵erences of
(.105/.0636)2 = 2.73, P = 0.1.
s u r v p l o t ( f , c o n f . i n t=FALSE,
# Figure 18.10
l e v e l s . o n l y=TRUE, l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) )
s u r v p l o t ( f l , add=TRUE, l a b e l . c u r v e s=FALSE, c o n f . i n t=FALSE)

The Weibull PH form of the fitted extreme value model, using Equation 18.24,
is
Prob{T t} = exp{ t5.456 exp(X ˆ)} where
Xˆ=

18.3 Accelerated Failure Time Models

451

Survival Probability

1.0
0.8
0.6

Group 1
Group 2

0.4
0.2
0.0
0

35

70

140

210

280

350

s
Fig. 18.10 Agreement between fitted log-logistic model and nonparametric survival
estimates for rat vaginal cancer data.

29.74
0.72[Group 2]

and [c] = 1 if subject is in group c, 0 otherwise.
A sensitive graphical verification of the distributional assumptions of the
AFT model is obtained by plotting the estimated survival distribution of
standardized residuals (Equation 18.3.5), censored identically to the way T
is censored. This distribution is plotted along with the theoretical distribution
. The assessment may be made more stringent by stratifying the residuals
by important subject characteristics and plotting separate survival function
estimates; they should all have the same standardized distribution (e.g., same
).
r
r e s i d ( f l , ’ cens ’ )
s u r v p l o t ( npsurv ( r ⇠ group , data=k p r a t s ) ,
c o n f= ’ none ’ , x l a b= ’ R e s i d u a l ’ ,
l a b e l . c u r v e s= l i s t ( k e y s= ’ l i n e s ’ ) , l e v e l s . o n l y=TRUE)
s u r v p l o t ( npsurv ( r ⇠ 1 ) , c o n f= ’ none ’ , add=TRUE, c o l= ’ r e d ’ )
l i n e s ( r , lwd =1 , c o l= ’ b l u e ’ )
# Figure 18.11

As an example, Figure 18.11 shows the Kaplan–Meier estimate of the distribution of residuals, Kaplan–Meier estimates stratified by group, and the
assumed log-logistic distribution.
Section 19.2 has a more in-depth example of this approach.

5

452

18 Parametric Survival Models

Survival Probability

1.0
0.8
0.6
0.4

Group 1
Group 2

0.2
0.0
−6 −5 −4 −3 −2 −1

0

1

2

3

4

Residual
Fig. 18.11 Kaplan–Meier estimates of distribution of standardized censored residuals from the log-logistic model, along with the assumed standard log-logistic distribution (dashed curve). The step functions in red is the estimated distribution of all
residuals, and the step functions in black are the estimated distributions of residuals
stratified by group, as indicated. The blue curve is the assumed log-logistic distribution.

18.3.7 Validating the Fitted Model
AFT models may be validated for both calibration and discrimination accuracy using the same methods that are presented for the Cox model in Section 20.10. The methods discussed there for checking calibration are based on
choosing a single follow-up time. Checking the distributional assumptions of
the parametric model is also a check of calibration accuracy in a sense. Another indirect calibration assessment may be obtained from a set of Cox–Snell
residuals (Section 18.3.5) or by using ordinary residuals as just described. A
higher resolution indirect calibration assessment based on plotting individual
uncensored failure times is available when the theoretical censoring times for
those observations are known. Let C denote a subject’s censoring time and F
the cumulative distribution of a failure time T . The expected value of F (T |X)
is 0.5 when T is an actual failure time random variable. The expected value
for an event time that is observed because it is uncensored is the expected
value of F (T |T  C, X) = 0.5F (C|X). A smooth plot (using, say, loess) of
F (T |X) 0.5F (C|X) against X ˆ should be a flat line through y = 0 if the
model is well calibrated. A smooth plot of 2F (T |X)/F (C|X) against X ˆ (or
anything else) should be a flat line through y = 1. This method assumes that
the model is calibrated well enough that we can substitute 1 Ŝ(C|X) for
F (C|X).

18.8 Time-Dependent Covariates

453

18.4 Buckley–James Regression Model
Buckley and James80 developed a method for estimating regression coefficients using least squares after imputing censored residuals. Their method
does not assume a distribution for survival time or the residuals, but is aimed
at estimating expected survival time or expected log survival time given predictor variables. This method has been generalized to allow for smooth nonlinear e↵ects and interactions in the S bj function in the rms package, written
by Stare and Harrell581 .

18.5 Design Formulations
Various designs can be formulated with survival regression models just as
with other regression models. By constructing the proper dummy variables,
ANOVA and ANOCOVA models can easily be specified for testing di↵erences
in survival time between multiple treatments. Interactions and complex nonlinear e↵ects may also be modeled.

18.6 Test Statistics
As discussed previously, likelihood ratio, score, and Wald statistics can be
derived from the maximum likelihood analysis, and the choice of test statistic
depends on the circumstance and on computational convenience.

18.7 Quantifying Predictive Ability
See Section 20.9 for a generalized measure of concordance between predicted
and observed survival time (or probability of survival) for right-censored data.

18.8 Time-Dependent Covariates
Time-dependent covariates (predictors) requires special likelihood functions
and add significant complexity to analyses in exchange for greater versatility and enhanced predictive discrimination597 . Nicolaie et al.470 and
D’Agostinoet al.142 provide useful static covariate approaches to modeling
time-dependent predictors using landmark analysis.

454

18 Parametric Survival Models

18.9 R Functions
Therneau’s survreg function (part of his survival package) can fit regression
models in the AFT family with left–, right–, or interval–censoring. The time
variable can be untransformed or log-transformed (the default). Distributions
supported are extreme value (Weibull and exponential), normal, logistic, and
Student-t. The version of survreg in rms that fits parametric survival models
in the same framework as lrm, ols, and cph is called psm. psm works with
print, coef, formula, specs, summary, anova, predict, Predict, fastbw,
latex, nomogram, validate, calibrate, survest, and survplot functions
for obtaining and plotting predicted survival probabilities. The dist argument to psm can be "exponential", "extreme", "gaussian", "logistic",
"loglogistic", "lognormal", "t", or "weibull". To fit a model with no
covariables, use the command
psm ( Surv ( d . t i m e , e v e n t ) ⇠ 1 )

To restate a Weibull or exponential model in PH form, use the pphsm function.
An example of how many of the functions are used is found below.
units ( d.time )
” Year ”
f
psm ( Surv ( d . t i m e , c d e a t h ) ⇠ l s p ( age , 6 5 ) ⇤ s e x )
# default is Weibull
anova ( f )
summary ( f )
# summarize effects with delta log T
latex ( f )
# typeset math. form of fitted model
s u r v e s t ( f , t i m e s =1) # 1y survival est. for all subjects
s u r v e s t ( f , e x p a n d . g r i d ( s e x=” f e m a l e ” , age = 3 0 : 8 0 ) , t i m e s =1:2)
# 1y, 2y survival estimates vs. age, for females
s u r v e s t ( f , d a t a . f r a m e ( s e x=” f e m a l e ” , age =50))
# survival curve for an individual subject
s u r v p l o t ( f , s e x=NA, age =50 , n . r i s k=T)
# survival curves for each sex, adjusting age to 50
f.ph
pphsm ( f )
# convert from AFT to PH
summary ( f . p h )
# summarize with hazard ratios
# instead of changes in log(T)

Special functions work with objects created by psm to create S functions
that contain the analytic form for predicted survival probabilities (Survival),
hazard functions (Hazard), quantiles of survival time (Quantile), and mean
or expected survival time (Mean). Once the S functions are constructed, they
can be used in a variety of contexts. The survplot and survest functions
have a special argument for psm fits: what. The default is what="survival"
to estimate or plot survival probabilities. Specifying what="hazard" will plot
hazard functions. Predict also has a special argument for psm fits: time.
Specifying a single value for time results in survival probability for that time
being plotted instead of X ˆ. Examples of many of the functions appear below,
with the output of the survplot command shown in Figure 18.12.
med
meant

Quantile ( f l )
Mean ( f l )

18.9 R Functions

455

haz
Hazard ( f l )
surv
Survival ( f l )
l a t e x ( surv , f i l e = ’ ’ , t y p e= ’ S i n p u t ’ )
surv
{

f u n c t i o n ( t i m e s = NULL, l p = NULL, parms =
1 / ( 1 + exp ( ( l o g b ( t i m e s )

2.15437773933124 )

l p ) / exp ( parms ) ) )

}
# Plot estimated hazard functions and add median
# survival times to graph
s u r v p l o t ( f l , group , what=” h aza rd ” )
# Figure 18.12
# Compute median survival time
m
med ( l p=p r e d i c t ( f l ,
d a t a . f r a m e ( group=l e v e l s ( k p r a t s $ group ) ) ) )
m
1
2
216.0857 240.0328

med ( l p=r a n g e ( f l $ l i n e a r . p r e d i c t o r s ) )
[1] 216.0857 240.0328

m
f o r m a t (m, d i g i t s =3)
t e x t ( 6 8 , . 0 2 , p a s t e ( ”Group 1 median : ” , m[ 1 ] , ” \n” ,
”Group 2 median : ” , m[ 2 ] , s e p=” ” ) )
# Compute survival probability at 210 days
xbeta
predict ( fl ,
d a t a . f r a m e ( group=c ( ”Group 1 ” , ”Group 2 ” ) ) )
surv (210 , xbeta )
1
2
0.5612718 0.7599776

The S object called survreg.distributions in Therneau’s survival package and the object survreg.auxinfo in the rms package have detailed information for extreme-value, logistic, normal, and t distributions. For each distribution, components include the deviance function, an algorithm for obtaining
starting parameter estimates, a LATEX representation of the survival function,
and S functions defining the survival, hazard, quantile functions, and basic
survival inverse function (which could have been used in Figure 18.9). See
Figure 18.6 for examples. rms’s val.surv function is useful for indirect external validation of parametric models using Cox–Snell residuals and other
approaches of Section 18.3.7. The plot method for an object created by
val.surv makes it easy to stratify all computations by a variable of interest
to more stringently validate the fit with respect to that variable.
rms’s bj function fits the Buckley–James model for right-censored responses.
Kooperberg et al.’s adaptive linear spline log-hazard model353, 354, 588 has
been implemented in the S function hare. Their procedure searches for

456

18 Parametric Survival Models
0.030

Hazard Function

0.025
0.020

Group 1 median: 216
Group 2 median: 240

Group 1

0.015

Group 2
0.010
0.005
0.000
0

30

60

90 120

180

240

300

Days
Fig. 18.12 Estimated hazard functions for log-logistic fit to rat vaginal cancer data,
along with median survival times.

second-order interactions involving predictors (and linear splines of them)
and linear splines in follow-up time (allowing for non-proportional hazards).
hare is also used to estimate calibration curves for parametric survival models
(rms function calibrate.psm) as it is for Cox models.

18.10 Further Reading
1
2
3
4
5

Wellek651 developed a test statistic for a specified maximum survival di↵erence
after relating this di↵erence to a hazard ratio.
Hougaard301 compared accelerated failure time models with proportional hazard models.
Gore et al.221 discuss how an AFT model (the log-logistic model) gives rise to
varying hazard ratios.
See Hillis286 for other types of residuals and plots that use them.
See Gore et al.221 and Lawless374 for other methods of checking assumptions for
AFT models. Lawless is an excellent text for in-depth discussion of parametric
survival modeling. Kwong and Hutton362 present other methods of choosing
parametric survival models, and discuss the robustness of estimates when fitting
an incorrectly chosen accelerated failure time model.

18.11 Problems
1. For the failure times (in days)

18.11 Problems

457

1 3 3+ 6+ 7+
compute MLEs of the following parameters of an exponential distribution
by hand: , µ, T0.5 , and S(3 days). Compute 0.95 confidence limits for
and S(3), basing the latter on log[⇤(t)].
2. For the same data in Problem 1, compute MLEs of parameters of a Weibull
distribution. Also compute the MLEs of S(3) and T0.5 .

Chapter 19

Case Study in Parametric Survival
Modeling and Model Approximation

Consider the random sample of 1000 patients from the SUPPORT study,345
described in Section 3.12. In this case study we develop a parametric survival time model (accelerated failure time model) for time until death for the
acute disease subset of SUPPORT (acute respiratory failure, multiple organ
system failure, coma). We eliminate the chronic disease categories because
the shapes of the survival curves are di↵erent between acute and chronic disease categories. To fit both acute and chronic disease classes would require a
log-normal model with parameter that is disease-specific.
Patients had to survive until day 3 of the study to qualify. The baseline
physiologic variables were measured during day 3.

19.1 Descriptive Statistics
First we create a variable acute to flag the categories of interest, and print
univariable descriptive statistics for the data subset.
r e q u i r e ( rms )
getHdata ( s u p p o r t )
# Get data frame from web site
acute
s u p p o r t $ d z c l a s s %i n% c ( ’ARF/MOSF ’ , ’Coma ’ )
l a t e x ( d e s c r i b e ( support [ acute , ] ) , f i l e = ’ ’ )

support[acute, ]
35 Variables
537 Observations
age : Age
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
537
0
529
1 60.7 28.49 35.22 47.93 63.67 74.49 81.54 85.56
lowest : 18.04 18.41 19.76 20.30 20.31
highest: 91.62 91.82 91.93 92.74 95.51

459

460

19 Parametric Survival Modeling and Model Approximation

death : Death at any time up to NDI date:31DEC94
n missing unique Info Sum Mean
537
0
2 0.67 356 0.6629

sex

n missing unique
537
0
2

female (251, 47%), male (286, 53%)

hospdead : Death in Hospital
n missing unique Info Sum Mean
537
0
2 0.7 201 0.3743

slos : Days from Study Entry to Discharge
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
537
0
85
1 23.44 4.0 5.0 9.0 15.0 27.0 47.4 68.2
lowest :

3

4

5

6

7, highest: 145 164 202 236 241

d.time : Days of Follow-Up
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
537
0
340
1 446.1 4 6 16 182 724 1421 1742
lowest :

3

4

5

6

7, highest: 1977 1979 1982 2011 2022

dzgroup

n missing unique
537
0
3

ARF/MOSF w/Sepsis (391, 73%), Coma (60, 11%), MOSF w/Malig (86, 16%)

dzclass

n missing unique
537
0
2

ARF/MOSF (477, 89%), Coma (60, 11%)

num.co : number of comorbidities
n missing unique Info Mean
537
0
7 0.93 1.525

0
1
2 3 4 5 6
Frequency 111 196 133 51 31 10 5
%
21 36 25 9 6 2 1

edu : Years of Education

n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
411
126
22 0.96 12.03 7 8 10 12 14 16 17

lowest :

0

1

2

3

4, highest: 17 18 19 20 22

income

n missing unique
335
202
4

under $11k (158, 47%), $11-$25k (79, 24%), $25-$50k (63, 19%)
>$50k (35, 10%)

scoma : SUPPORT Coma Score based on Glasgow D3
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
537
0
11 0.82 19.24 0 0 0 0 37 55 100
0 9 26 37 41 44 55 61 89 94 100
Frequency 301 50 44 19 17 43 11 6 8 6 32
%
56 9 8 4 3 8 2 1 1 1
6

19.1 Descriptive Statistics

461

charges : Hospital Charges
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
517
20
516
1 86652 11075 15180 27389 51079 100904 205562 283411
lowest :
3448
4432
4574
5555
5849
highest: 504660 538323 543761 706577 740010

totcst : Total RCC cost

n missing unique Info Mean .05 .10
.25
.50
.75
.90
.95
471
66
471
1 46360 6359 8449 15412 29308 57028 108927 141569

lowest :
0
2071
2522
3191
3325
highest: 269057 269131 338955 357919 390460

totmcst : Total micro-cost

n missing unique Info Mean .05 .10
.25
.50
.75
.90
.95
331
206
328
1 39022 6131 8283 14415 26323 54102 87495 111920

lowest :
0
1562
2478
2626
3421
highest: 144234 154709 198047 234876 271467

avtisst : Average TISS, Days 3-25
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
536
1
205
1 29.83 12.46 14.50 19.62 28.00 39.00 47.17 50.37
lowest : 4.000 5.667 8.000 9.000 9.500
highest: 58.500 59.000 60.000 61.000 64.000

race

n missing unique
535
2
5

Frequency
%

white black asian other hispanic
417
84
4
8
22
78
16
1
1
4

meanbp : Mean Arterial Blood Pressure Day 3
n missing unique Info Mean .05 .10 .25 .50
.75
.90
.95
537
0
109
1 83.28 41.8 49.0 59.0 73.0 111.0 124.4 135.0
lowest :

0

20

27

30

32, highest: 155 158 161 162 180

wblc : White Blood Cell Count Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
532
5
241
1 14.1 0.8999 4.5000 7.9749 12.3984 18.1992 25.1891 30.1873
lowest :
highest:

0.05000
51.39844

0.06999
58.19531

0.09999
61.19531

0.14999
0.19998
79.39062 100.00000

hrt : Heart Rate Day 3
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
537
0
111
1 105 51 60 75 111 126 140 155
lowest :

0

11

30

36

40, highest: 189 193 199 232 300

resp : Respiration Rate Day 3
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
537
0
45
1 23.72 8 10 12 24 32 39 40
lowest :

0

4

6

7

8, highest: 48 49 52 60 64

temp : Temperature (celcius) Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
537
0
61
1 37.52 35.50 35.80 36.40 37.80 38.50 39.09 39.50
lowest : 32.50 34.00 34.09 34.90 35.00
highest: 40.20 40.59 40.90 41.00 41.20

462

19 Parametric Survival Modeling and Model Approximation

pafi : PaO2/(.01*FiO2) Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
500
37
357
1 227.2 86.99 105.08 137.88 202.56 290.00 390.49 433.31
lowest : 45.00 48.00 53.33 54.00 55.00
highest: 574.00 595.12 640.00 680.00 869.38

alb : Serum Albumin Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
346
191
34
1 2.668 1.700 1.900 2.225 2.600 3.100 3.400 3.800
lowest : 1.100 1.200 1.300 1.400 1.500
highest: 4.100 4.199 4.500 4.699 4.800

bili : Bilirubin Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
386
151
88
1 2.678 0.3000 0.4000 0.6000 0.8999 2.0000 6.5996 13.1743
lowest : 0.09999 0.19998 0.29999 0.39996 0.50000
highest: 22.59766 30.00000 31.50000 35.00000 39.29688

crea : Serum creatinine Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
537
0
84
1 2.232 0.6000 0.7000 0.8999 1.3999 2.5996 5.2395 7.3197
lowest :

0.3

0.4

0.5

0.6

0.7, highest: 10.4 10.6 11.2 11.6 11.8

sod : Serum sodium Day 3
n missing unique Info Mean .05 .10 .25 .50 .75 .90 .95
537
0
38
1 138.1 129 131 134 137 142 147 150
lowest : 118 120 121 126 127, highest: 156 157 158 168 175

ph : Serum pH (arterial) Day 3
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
500
37
49
1 7.416 7.270 7.319 7.380 7.420 7.470 7.510 7.529
lowest : 6.960 6.989 7.069 7.119 7.130
highest: 7.560 7.569 7.590 7.600 7.659

glucose : Glucose Day 3
n missing unique Info Mean .05 .10
.25
.50
.75
.90
.95
297
240
179
1 167.7 76.0 89.0 106.0 141.0 200.0 292.4 347.2
lowest :

30

42

52

55

68, highest: 446 468 492 576 598

bun : BUN Day 3
n missing unique Info Mean .05
.10
.25
.50
.75
.90
.95
304
233
100
1 38.91 8.00 11.00 16.75 30.00 56.00 79.70 100.70
lowest :

1

3

4

5

6, highest: 123 124 125 128 146

urine : Urine Output Day 3
n missing unique Info Mean .05
.10
.25
.50
.75
.90
.95
303
234
262
1 2095 20.3 364.0 1156.5 1870.0 2795.0 4008.6 4817.5
lowest :

0

5

8

15

adlp : ADL Patient Day 3
n missing unique Info Mean
104
433
8 0.87 1.577
0 1 2 3 4 5 6 7
Frequency 51 19 7 6 4 7 8 2
%
49 18 7 6 4 7 8 2

20, highest: 6865 6920 7360 7560 7750

19.2 Checking Adequacy of Log-Normal Model

463

adls : ADL Surrogate Day 3
n missing unique Info Mean
392
145
8 0.89 1.86
0 1 2 3 4 5 6 7
Frequency 185 68 22 18 17 20 39 23
%
47 17 6 5 4 5 10 6

sfdm2

n missing unique
468
69
5

no(M2 and SIP pres) (134, 29%), adl>=4 (>=5 if sur) (78, 17%)
SIP>=30 (30, 6%), Coma or Intub (5, 1%), <2 mo. follow-up (221, 47%)

adlsc : Imputed ADL Calibrated to Surrogate
n missing unique Info Mean
.05
.10
.25
.50
.75
.90
.95
537
0
144 0.96 2.119 0.000 0.000 0.000 1.839 3.375 6.000 6.000
lowest : 0.0000 0.4948 0.4948 1.0000 1.1667
highest: 5.7832 6.0000 6.3398 6.4658 7.0000

Next, patterns of missing data are displayed.
p l o t ( n a c l u s ( support [ acute , ] ) )

# Figure 19.1

The Hmisc varclus function is used to quantify and depict associations between predictors, allowing for general nonmonotonic relationships. This is
done by using Hoe↵ding’s D as a similarity measure for all possible pairs of
predictors instead of the default similarity, Spearman’s ⇢.
ac
support [ acute , ]
ac $ dzgroup
ac $ dzgroup [ drop=TRUE]
# Remove unused levels
a t t a c h ( ac )
vc
v a r c l u s (⇠ age + s e x + dzgroup + num.co + edu + income +
scoma + r a c e + meanbp + wblc + h r t + r e s p +
temp + p a f i + a l b + b i l i + c r e a + sod + ph +
g l u c o s e + bun + u r i n e + a d l s c , sim= ’ h o e f f d i n g ’ )
p l o t ( vc )
# Figure 19.2

19.2 Checking Adequacy of Log-Normal Accelerated
Failure Time Model
Let us check whether a parametric survival time model will fit the data, with
respect to the key prognostic factors. First, Kaplan–Meier estimates stratified
by disease group are computed, and plotted after inverse normal transformation, against log t. Parallelism and linearity indicate goodness of fit to the
log normal distribution for disease group. Then a more stringent assessment
is made by fitting an initial model and computing right-censored residuals.
These residuals, after dividing by ˆ , should all have a normal distribution
if the model holds. We compute Kaplan–Meier estimates of the distribution
of the residuals and overlay the estimated survival distribution with the theoretical Gaussian one. This is done overall, and then to get more stringent
assessments of fit, residuals are stratified by key predictors and plots are

464

19 Parametric Survival Modeling and Model Approximation

0.5

num.co
dzclass
dzgroup
d.time
slos
hospdead
sex
age
death
urine
glucose
bun

adlp

0.4

totmcst

0.3

adls
edu
income

0.2

alb
bili

Fraction Missing

0.1

adlsc
sod
crea
temp
resp
hrt
meanbp
race
avtisst
wblc
charges
totcst
scoma
pafi
ph
sfdm2

0.0

Fig. 19.1 Cluster analysis showing which predictors tend to be missing on the same
patients

produced that contain multiple Kaplan–Meier curves along with a single theoretical normal curve. All curves should hover about the normal distribution.
To gauge the natural variability of stratified residual distribution estimates,
the residuals are also stratified by a random number that has no bearing on
the goodness of fit.
dd
d a t a d i s t ( ac )
# describe distributions of variables to rms
o p t i o n s ( d a t a d i s t= ’ dd ’ )
# Generate right-censored survival time variable
years
d . t i m e / 365 . 2 5

19.2 Checking Adequacy of Log-Normal Model

465

urine

0.15

glucose
bun

0.20

sexmale
sod
dzgroupComa
scoma
dzgroupMOSF w/Malig
wblc

0.10

income$11−$25k
income$25−$50k
income>$50k

0.05

num.co
adlsc
resp
hrt
temp
meanbp
pafi
ph
alb
bili
age
crea
edu

raceasian
raceother
racehispanic
raceblack

30 * Hoeffding D

0.00

Fig. 19.2 Hierarchical clustering of potential predictors using Hoe↵ding D as a similarity measure. Categorical predictors are automatically expanded into dummy variables.

units ( years )
’ Year ’
S
Surv ( y e a r s , death )
# Show normal inverse Kaplan-Meier estimates
# stratified by dzgroup
s u r v p l o t ( npsurv ( S ⇠ dzgroup ) , c o n f= ’ none ’ ,
f u n=qnorm , l o g t=TRUE)
# Figure 19.3
f
r

psm ( S ⇠ dzgroup + r c s ( age , 5 ) + r c s ( meanbp , 5 ) ,
d i s t= ’ l o g n o r m a l ’ , y=TRUE)
resid ( f )

survplot ( r
survplot ( r
survplot ( r
random
survplot ( r

, dzgroup , l a b e l . c u r v e=FALSE, main= ’ ’ )
, age ,
l a b e l . c u r v e=FALSE, main= ’ ’ )
, meanbp ,
l a b e l . c u r v e=FALSE, main= ’ ’ )
r u n i f ( l e n g t h ( age ) )
, random , l a b e l . c u r v e=FALSE, main= ’ ’ ) # Fig. 19.4

Now remove from consideration predictors that are missing in more than 0.2
of patients. Many of these were collected only for the second half of SUPPORT. Of those variables to be included in the model, find which ones have
enough potential predictive power to justify allowing for nonlinear relationships or multiple categories, which spend more d.f. For each variable compute
Spearman ⇢2 based on multiple linear regression of rank(x), rank(x)2 , and the

466

19 Parametric Survival Modeling and Model Approximation

2

1

dzgroup=MOSF w/Malig

0

dzgroup=ARF/MOSF w/Sepsis
−1

dzgroup=Coma
−2
−3

−2

−1

0

1

2

log Survival Time in Years
1
Fig. 19.3
(SKM (t)) stratified by dzgroup. Linearity and semi-parallelism indicate a reasonable fit to the log-normal accelerated failure time model with respect
to one predictor.

survival time, truncating survival time at the shortest follow-up for survivors
(356 days; see Section 4.1).
shortest.follow.up
min ( d . t i m e [ death ==0] , na.rm=TRUE)
d.timet
pmin ( d . t i m e , s h o r t e s t . f o l l o w . u p )
spearman2 ( d . t i m e t ⇠ age + num.co + scoma + meanbp +
h r t + r e s p + temp + c r e a + sod + a d l s c +
wblc + p a f i + ph + dzgroup + r a c e , p=2)
p l o t (w, main= ’ ’ )
# Figure 19.5

w

A better approach is to use the complete information in the failure and censoring times by computing Somers’ Dxy rank correlation allowing for censoring.
r c o r r c e n s ( S ⇠ age + num.co + scoma + meanbp + h r t + r e s p +
temp + c r e a + sod + a d l s c + wblc + p a f i + ph +
dzgroup + r a c e )
p l o t (w, main= ’ ’ )
# Figure 19.6

w

Remaining missing values are imputed using the “most normal” values, a
procedure found to work adequately for this particular study. Race is imputed
using the modal category.
# Compute number of missing values per variable
s a p p l y ( l l i s t ( age , num.co , scoma , meanbp , hrt , r e s p , temp , c r e a , sod ,
a d l s c , wblc , p a f i , ph ) , f u n c t i o n ( x ) sum ( i s . n a ( x ) ) )

467

1.0

1.0

0.8

0.8

Survival Probability

Survival Probability

19.2 Checking Adequacy of Log-Normal Model

0.6

0.4

0.2

0.0

0.6

0.4

0.2

0.0
−3.0

−2.0

−1.0

0.0 0.5 1.0 1.5 2.0

−3.0

−2.0

−1.0

0.0 0.5 1.0 1.5 2.0

Residual

1.0

1.0

0.8

0.8

Survival Probability

Survival Probability

Residual

0.6

0.4

0.2

0.0

0.6

0.4

0.2

0.0
−3.0

−2.0

−1.0

0.0 0.5 1.0 1.5 2.0

−3.0

Residual

−2.0

−1.0

0.0 0.5 1.0 1.5 2.0

Residual

Fig. 19.4 Kaplan-Meier estimates of distributions of normalized, right-censored
residuals from the fitted log-normal survival model. Residuals are stratified by important variables in the model (by quartiles of continuous variables), plus a random
variable to depict the natural variability (in the lower right plot). Theoretical standard Gaussian distributions of residuals are shown with a thick solid line. The upper
left plot is with respect to disease group, upper right by age, lower left by mean
baseline arterial blood pressure.

age num.co
0
0
sod adlsc
0
0

scoma meanbp
0
0
wblc
pafi
5
37

hrt
0
ph
37

resp
0

temp
0

crea
0

# Can also do naplot(naclus(support[acute,]))
# Can also use the Hmisc naclus and naplot functions
# Impute missing values with normal or modal values
wblc.i
impute ( wblc , 9 )
pafi.i
impute ( p a f i , 333 . 3 )
ph.i
impute ( ph ,
7 .4 )
race2
race
l e v e l s ( race2 )
l i s t ( w h i t e= ’ w h i t e ’ , o t h e r=l e v e l s ( r a c e ) [ 1 ] )
race2 [ i s . n a ( race2 ) ]
’ white ’
dd
d a t a d i s t ( dd , w b l c . i , p a f i . i , p h . i , r a c e 2 )

468

19 Parametric Survival Modeling and Model Approximation

scoma
meanbp
dzgroup
crea
pafi
ph
sod
hrt
adlsc
temp
wblc
num.co
age
resp
race

N df
537 2
537 2
537 2
537 2
500 2
500 2
537 2
537 2
537 2
537 2
532 2
537 2
537 2
537 2
535 4

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

0.02

0.04

0.06

0.08

0.10

0.12

Adjusted ρ2
Fig. 19.5 Generalized Spearman ⇢2 rank correlation between predictors and truncated survival time

meanbp
crea
dzgroup
scoma
pafi
ph
adlsc
age
num.co
hrt
resp
race
sod
wblc
temp

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

0.05

0.10

0.15

N
537
537
537
537
500
500
537
537
537
537
537
535
537
532
537

0.20

|Dxy|
Fig. 19.6 Somers’ Dxy rank correlation between predictors and original survival
time. For dzgroup or race, the correlation coefficient is the maximum correlation
from using a dummy variable to represent the most frequent or one to represent the
second most frequent category.’,scap=’Somers’ Dxy rank correlation between predictors and original survival time

19.2 Checking Adequacy of Log-Normal Model

469

Now that missing values have been imputed, a formal multivariable redundancy analysis can be undertaken. The Hmisc package’s redun function goes
farther than the varclus pairwise correlation approach and allows for nonmonotonic transformations in predicting each predictor from all the others.
redun (⇠ c r e a + age + s e x + dzgroup + num.co + scoma + a d l s c +
r a c e 2 + meanbp + h r t + r e s p + temp + sod + w b l c . i +
p a f i . i + p h . i , nk=4)
Redundancy Analysis
redun(formula = ⇠crea + age + sex + dzgroup + num.co + scoma +
adlsc + race2 + meanbp + hrt + resp + temp + sod + wblc.i +
pafi.i + ph.i, nk = 4)
n: 537

p: 16

Number of NAs:

nk: 4
0

Transformation of target variables forced to be linear
R2 cutoff: 0.9
R

2

Type: ordinary

with which each variable can be predicted from all other variables:

crea
0.133
race2
0.151
pafi.i
0.143

age
0.246
meanbp
0.178
ph.i
0.171

sex dzgroup
0.132
0.451
hrt
resp
0.258
0.131

num.co
0.147
temp
0.197

scoma
0.418
sod
0.135

adlsc
0.153
wblc.i
0.093

No redundant variables

Now turn to a more efficient approach for gauging the potential of each
predictor, one that makes maximal use of failure time and censored data
is to all all continuous variables to have a maximum number of knots in
a log-normal survival model. This approach must use imputation to have
an adequate sample size. A semi-saturated main e↵ects additive log-normal
model is fitted. It is necessary to limit restricted cubic splines to 4 knots, force
scoma to be linear, and to omit ph.i in order to avoid a singular covariance
matrix in the fit.
k
f

4
psm ( S ⇠ r c s ( age , k)+ s e x+dzgroup+p o l ( num.co ,2)+ scoma+
p o l ( a d l s c ,2)+ r a c e+r c s ( meanbp , k)+ r c s ( hrt , k)+
r c s ( r e s p , k)+ r c s ( temp , k)+ r c s ( c r e a ,3)+ r c s ( sod , k)+
r c s ( w b l c . i , k)+ r c s ( p a f i . i , k ) , d i s t= ’ l o g n o r m a l ’ )
p l o t ( anova ( f ) )
# Figure 19.7

Figure 19.7 properly blinds the analyst to the form of e↵ects (tests of linearity). Next fit a log-normal survival model with number of parameters
corresponding to nonlinear e↵ects determined from the partial 2 tests in
Figure 19.7. For the most promising predictors, five knots can be allocated,
as there are fewer singularity problems once less promising predictors are
simplified.

470

19 Parametric Survival Modeling and Model Approximation

sex
temp
race
sod
num.co
hrt
wblc.i
adlsc
resp
scoma
pafi.i
age
meanbp
crea
dzgroup

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0

10

20

30

χ2 − df
Fig. 19.7 Partial 2 statistics for association of each predictor with response from
saturated main e↵ects model, penalized for d.f.

psm ( S ⇠ r c s ( age ,5)+ s e x+dzgroup+num.co+
scoma+p o l ( a d l s c ,2)+ r a c e 2+r c s ( meanbp ,5)+
r c s ( hrt ,3)+ r c s ( r e s p ,3)+ temp+
r c s ( c r e a ,4)+ sod+r c s ( w b l c . i ,3)+ r c s ( p a f i . i , 4 ) ,
d i s t= ’ l o g n o r m a l ’ ) # ’gaussian’ for S+
p r i n t ( f , l a t e x=TRUE, c o e f s=FALSE)
f

Parametric Survival Model: Log Normal Distribution
psm(formula = S ˜ rcs(age, 5) + sex + dzgroup + num.co + scoma +
pol(adlsc, 2) + race2 + rcs(meanbp, 5) + rcs(hrt, 3) + rcs(resp,
3) + temp + rcs(crea, 4) + sod + rcs(wblc.i, 3) + rcs(pafi.i,
4), dist = "lognormal")
Model Likelihood
Ratio Test
Obs
537 LR 2
236.83
Events 356 d.f.
30
2.230782 Pr(> 2 ) < 0.0001

Discrimination
Indexes
R2
0.594
Dxy
0.485
g
0.033
gr
1.959

19.3 Summarizing the Fitted Model

471

Table 19.1 Wald Statistics for S
2

age
Nonlinear
sex
dzgroup
num.co
scoma
adlsc
Nonlinear
race2
meanbp
Nonlinear
hrt
Nonlinear
resp
Nonlinear
temp
crea
Nonlinear
sod
wblc.i
Nonlinear
pafi.i
Nonlinear
TOTAL NONLINEAR
TOTAL

d.f.
P
15.99 4 0.0030
0.23 3 0.9722
0.11 1 0.7354
45.69 2 < 0.0001
4.99 1 0.0255
10.58 1 0.0011
8.28 2 0.0159
3.31 1 0.0691
1.26 1 0.2624
27.62 4 < 0.0001
10.51 3 0.0147
11.83 2 0.0027
1.04 1 0.3090
11.10 2 0.0039
8.56 1 0.0034
0.39 1 0.5308
33.63 3 < 0.0001
21.27 2 < 0.0001
0.08 1 0.7792
5.47 2 0.0649
5.46 1 0.0195
15.37 3 0.0015
6.97 2 0.0307
60.48 14 < 0.0001
261.47 30 < 0.0001

19.3 Summarizing the Fitted Model
First let’s plot the shape of the e↵ect of each predictor on log survival time.
All e↵ects are centered so that they can be placed on a common scale. This
allows the relative strength of various predictors to be judged. Then Wald
2
statistics, penalized for d.f., are plotted in descending order. Next, relative e↵ects of varying predictors over reasonable ranges (survival time ratios
varying continuous predictors from the first to the third quartile) are charted.
g g p l o t ( P r e d i c t ( f , r e f . z e r o=TRUE) , vnames= ’ names ’ ,
s e p d i s c r e t e= ’ v e r t i c a l ’ ) # Figure 19.8
l a t e x ( anova ( f ) , f i l e = ’ ’ , l a b e l= ’ tab : support
p l o t ( anova ( f ) )

# Figure 19.9

anovat ’ ) # Table 19.1

472

19 Parametric Survival Modeling and Model Approximation

adlsc

age

crea

hrt

2
1
0
−1
−2
−3
0

2

30

60

4
meanbp

6

20

40

60
num.co

80

2.5

5.0
pafi.i

7.5

50

100
resp

150

10

20 30
wblc.i

2

log(T)

1
0
−1
−2
−3
90 120 1500
scoma

2

4
sod

6 100 200 300 400 500
temp

40

2
1
0
−1
−2
−3
0

25

50

75

100 130 135 140 145 150 155
35 36 37 38 39 40 0
dzgroup

MOSF w/Malig
Coma
ARF/MOSF w/Sepsis

race2

●

white

●

0

1

2

●

−3 −2 −1

20

30

40

sex

●

other

●

−3 −2 −1

10

0

1

2

male

●

female

●

−3 −2 −1

log(T)
Fig. 19.8 E↵ect of each predictor on log survival time. Predicted values have been
centered so that predictions at predictor reference values are zero. Pointwise 0.95
confidence bands are also shown. As all Y -axes have the same scale, it is easy to see
which predictors are strongest.

0

1

2

19.4 Internal Validation of the Fitted Model Using the Bootstrap

sod
sex
temp
race2
wblc.i
num.co
adlsc
resp
scoma
hrt
age
pafi.i
meanbp
crea
dzgroup

473

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0

10

20

30

40

χ2 − df
Fig. 19.9 Contribution of variables in predicting survival time in log-normal model

o p t i o n s ( d i g i t s =3)
p l o t ( summary ( f ) , l o g=TRUE, main= ’ ’ )

# Figure 19.10

19.4 Internal Validation of the Fitted Model Using the
Bootstrap
Let us decide whether there was significant overfitting during the development
of this model, using the bootstrap.
# First add data to model fit so bootstrap can re-sample
# from the data
g
update ( f , x=TRUE, y=TRUE)
s e t . s e e d (717)
l a t e x ( v a l i d a t e ( g , B=300) , d i g i t s =2 , s i z e= ’ S s i z e ’ )

474

19 Parametric Survival Modeling and Model Approximation

0.10

0.50

1.00

age − 74.5:47.9
num.co − 2:1
scoma − 37:0
adlsc − 3.38:0
meanbp − 111:59
hrt − 126:75
resp − 32:12
temp − 38.5:36.4
crea − 2.6:0.9
sod − 142:134
wblc.i − 18.2:8.1
pafi.i − 323:142
sex − female:male
dzgroup − Coma:ARF/MOSF w/Sepsis
dzgroup − MOSF w/Malig:ARF/MOSF w/Sepsis
race2 − other:white

Fig. 19.10 Estimated survival time ratios for default settings of predictors. For
example, when age changes from its lower quartile to the upper quartile (47.9y to
74.5y), median survival time decreases by more than half. Di↵erent shaded areas of
bars indicate di↵erent confidence levels (.9, 0.95, 0.99).

Index

Original Training Test Optimism Corrected n
Sample

Dxy
R2
Intercept
Slope
D
U
Q
g

0.49
0.59
0.00
1.00
0.48
0.00
0.48
1.96

Sample

0.51
0.66
0.00
1.00
0.55
0.00
0.56
2.05

Sample

0.46
0.54
0.05
0.90
0.42
0.01
0.43
1.87

Index

0.05
0.12
0.05
0.10
0.13
0.01
0.12
0.19

0.43
0.47
0.05
0.90
0.35
0.01
0.36
1.77

300
300
300
300
300
300
300
300

Judging from Dxy and R2 there is a moderate amount of overfitting. The
slope shrinkage factor (0.9) is not troublesome, however. An almost unbiased
estimate of future predictive discrimination on similar patients is given by
the corrected Dxy of 0.43. This index equals the di↵erence between the probability of concordance and the probability of discordance of pairs of predicted
survival times and pairs of observed survival times, accounting for censoring.
Next, a bootstrap overfitting-corrected calibration curve is estimated. Patients are stratified by the predicted probability of surviving one year, such
that there are at least 60 patients in each group.

2.00 3.50

19.5 Approximating the Full Model

475

Fraction Surviving 1 Years

s e t . s e e d (717)
cal
c a l i b r a t e ( g , u=1 , B=300)
p l o t ( c a l , s u b t i t l e s=FALSE)
cal
c a l i b r a t e ( g , cmethod= ’KM’ , u=1 , m=60 , B=120 , pr=FALSE)
p l o t ( c a l , add=TRUE)
# Figure 19.11

●

0.8

●

0.6
●
●

0.4
●
●

0.2
●

●

0.0
0.0

0.2

0.4

0.6

0.8

Predicted 1 Year Survival
Fig. 19.11 Bootstrap validation of calibration curve. Dots represent apparent calibration accuracy; ⇥ are bootstrap estimates corrected for overfitting, based on binning predicted survival probabilities and and computing Kaplan-Meier estimates.
Black curve is the estimated observed relationship using hare and the blue curve is
the overfitting-corrected hare estimate. The gray-scale line depicts the ideal relationship.

19.5 Approximating the Full Model
The fitted log-normal model is perhaps too complex for routine use and for
routine data collection. Let us develop a simplified model that can predict
the predicted values of the full model with high accuracy (R2 = 0.967). The
simplification is done using a fast backward step-down against the full model
predicted values.
Z
a

predict ( f )
# X⇤beta hat
o l s ( Z ⇠ r c s ( age ,5)+ s e x+dzgroup+num.co+
scoma+p o l ( a d l s c ,2)+ r a c e 2+
r c s ( meanbp ,5)+ r c s ( hrt ,3)+ r c s ( r e s p ,3)+

476

19 Parametric Survival Modeling and Model Approximation

temp+r c s ( c r e a ,4)+ sod+r c s ( w b l c . i ,3)+
r c s ( p a f i . i , 4 ) , sigma =1)
# sigma=1 is used to prevent sigma hat from being zero when
# R2=1.0 since we start out by approximating Z with all
# component variables
f a s t b w ( a , a i c s =10000)
# fast backward stepdown
Deleted
sod
sex
temp
race2
wblc.i
num.co
resp
adlsc
pafi.i
scoma
hrt
age
crea
meanbp
dzgroup
R2
1.000
0.999
0.998
0.994
0.976
0.957
0.924
0.892
0.851
0.803
0.752
0.710
0.517
0.270
0.000

Chi-Sq
0.43
0.57
2.20
6.81
29.52
30.84
54.18
52.46
66.78
78.07
83.17
68.08
314.47
403.04
441.28

d.f.
1
1
1
1
2
1
2
2
3
1
2
4
3
4
2

P
Residual d.f.
0.512
0.43
1
0.451
1.00
2
0.138
3.20
3
0.009
10.01
4
0.000
39.53
6
0.000
70.36
7
0.000 124.55
9
0.000 177.00 11
0.000 243.79 14
0.000 321.86 15
0.000 405.02 17
0.000 473.10 21
0.000 787.57 24
0.000 1190.61 28
0.000 1631.89 30

P
AIC
0.5117
-1.57
0.6073
-3.00
0.3621
-2.80
0.0402
2.01
0.0000
27.53
0.0000
56.36
0.0000 106.55
0.0000 155.00
0.0000 215.79
0.0000 291.86
0.0000 371.02
0.0000 431.10
0.0000 739.57
0.0000 1134.61
0.0000 1571.89

Approximate Estimates after Deleting Factors
Coef
S.E. Wald Z P
[1,] -0.5928 0.04315 -13.74 0
Factors in Final Model
None

o l s ( Z ⇠ dzgroup + r c s ( meanbp , 5 ) + r c s ( c r e a , 4 ) +
r c s ( age , 5 ) + r c s ( hrt , 3 ) + scoma +
r c s ( p a f i . i , 4 ) + p o l ( a d l s c ,2)+
r c s ( r e s p , 3 ) , x=TRUE)
f.approx $ stats
f.approx

n Model L.R.
537.000
1688.225
Sigma
0.370

d.f.
23.000

R2
0.957

g
1.915

We can estimate the variance–covariance matrix of the coefficients of the
reduced model using Equation 5.2 in Section 5.5.2. The computations below

19.5 Approximating the Full Model

477

result in a covariance matrix that does not include elements related to the
scale parameter. In the code x is the matrix T in Section 5.5.2.
V
X
x
w
v

vcov ( f , r e g c o e f . o n l y=TRUE)
c b i n d ( I n t e r c e p t =1 , g $x )
c b i n d ( I n t e r c e p t =1 , f . a p p r o x $x )
s o l v e ( t ( x ) %⇤% x , t ( x ) ) %⇤% X
w %⇤% V %⇤% t (w)

#
#
#
#

var(full model)
full model design
approx. model design
contrast matrix

Let’s compare the variance estimates (diagonals of v) with variance estimates
from a reduced model that is fitted against the actual outcomes.
f.sub

psm ( S ⇠ dzgroup + r c s ( meanbp , 5 ) + r c s ( c r e a , 4 ) +
r c s ( age , 5 ) + r c s ( hrt , 3 ) + scoma + r c s ( p a f i . i , 4 ) +
p o l ( a d l s c ,2)+ r c s ( r e s p , 3 ) , d i s t= ’ l o g n o r m a l ’ )

d i a g ( v ) / d i a g ( vcov ( f . s u b , r e g c o e f . o n l y=TRUE) )
Intercept
0.981
dzgroup=MOSF w/Malig
0.979
meanbp’
0.979
meanbp’’’
0.979
crea’
0.979
age
0.982
age’’
0.981
hrt
0.978
scoma
0.979
pafi.i’
0.980
adlsc
0.981
resp
0.978

dzgroup=Coma
0.979
meanbp
0.977
meanbp’’
0.979
crea
0.979
crea’’
0.979
age’
0.981
age’’’
0.980
hrt’
0.976
pafi.i
0.980
pafi.i’’
0.980
adlsc^2
0.981
resp’
0.977

r
d i a g ( v ) / d i a g ( vcov ( f . s u b , r e g c o e f . o n l y=TRUE) )
r [ c ( which.min ( r ) , which.max ( r ) ) ]
hrt’
age
0.976 0.982

The estimated variances from the reduced model are actually slightly smaller
than those that would have been obtained from stepwise variable selection
in this case, had variable selection used a stopping rule that resulted in the
same set of variables being selected. Now let us compute Wald statistics for
the reduced model.
f . a p p r o x $ var
v
l a t e x ( anova ( f . a p p r o x , t e s t= ’ Chisq ’ , s s=FALSE) ,
l a b e l= ’ tab : support anovaa ’ )

f i l e=’ ’ ,

478

19 Parametric Survival Modeling and Model Approximation

Table 19.2 Wald Statistics for Z
2

dzgroup
meanbp
Nonlinear
crea
Nonlinear
age
Nonlinear
hrt
Nonlinear
scoma
pafi.i
Nonlinear
adlsc
Nonlinear
resp
Nonlinear
TOTAL NONLINEAR
TOTAL

d.f.
P
55.94 2 < 0.0001
29.87 4 < 0.0001
9.84 3 0.0200
39.04 3 < 0.0001
24.37 2 < 0.0001
18.12 4 0.0012
0.34 3 0.9517
9.87 2 0.0072
0.40 1 0.5289
9.85 1 0.0017
14.01 3 0.0029
6.66 2 0.0357
9.71 2 0.0078
2.87 1 0.0904
9.65 2 0.0080
7.13 1 0.0076
58.08 13 < 0.0001
252.32 23 < 0.0001

The results are shown in Table 19.2. Note the similarity of the statistics
to those found in the table for the full model. This would not be the case had
deleted variables been very collinear with retained variables.
The equation for the simplified model follows. The model is also depicted
graphically in Figure 19.12. The nomogram allows one to calculate mean and
median survival time. Survival probabilities could have easily been added as
additional axes.
# Typeset mathematical form of approximate model
latex ( f.approx , f i l e=’ ’ )

E(Z) = X , where
Xˆ=
2.51
1.94[Coma]

1.75[MOSF w/Malig]

+0.068meanbp
4.91⇥10

5

3.08⇥10

(meanbp

0.553crea

0.229(crea

+0.0131(crea

7.32)3+

5

(meanbp

73)3+

41.8)3+ + 7.9⇥10

+ 2.61⇥10

6

(meanbp

0.6)3+ + 0.45(crea

1.1)3+

5

61)3+

(meanbp

109)3+

1.7⇥10

0.233(crea

6

(meanbp

1.94)3+

135)3+

19.5 Approximating the Full Model

0.0165age
2.15⇥10

1.13⇥10
5

(age

5

(age

63.7)3+

0.0136hrt + 6.09⇥10

7

479

28.5)3+ + 4.05⇥10

2.68⇥10

(hrt

60)3+

5

(age

5

49.5)3+

(age

72.7)3+ + 1.9⇥10

1.68⇥10

6

(hrt

111)3+

5

(age

85.6)3+
6

+ 1.07⇥10

(hrt

140)3+

0.0135 scoma
+0.0161pafi.i
5.02⇥10

7

+0.0394resp

4.77⇥10

(pafi.i

7

(pafi.i

88)3+ + 9.11⇥10

276)3+ + 6.76⇥10

9.11⇥10

5

(resp

8

(pafi.i

7

(pafi.i

0.369 adlsc + 0.0409 adlsc2

426)3+

10)3+ + 0.000176(resp

167)3+

24)3+

8.5⇥10

5

(resp

39)3+

and [c] = 1 if subject is in group c, 0 otherwise; (x)+ = x if x > 0, 0 otherwise.
# Derive S functions that express mean and quantiles
# of survival time for specific linear predictors
# analytically
expected.surv
Mean ( f )
quantile.surv
Quantile ( f )
l a t e x ( e x p e c t e d . s u r v , f i l e = ’ ’ , t y p e= ’ S i n p u t ’ )
expected.surv
f u n c t i o n ( l p = NULL, parms = 0 . 8 0 2 3 5 2 0 3 7 6 0 6 4 8 8 )
{
names ( parms )
NULL
exp ( l p + exp ( 2 ⇤ parms ) / 2 )
}
latex ( quantile.surv ,

f i l e = ’ ’ , t y p e= ’ S i n p u t ’ )

quantile.surv
f u n c t i o n ( q = 0 . 5 , l p = NULL, parms = 0 . 8 0 2 3 5 2 0 3 7 6 0 6 4 8 8 )
{
names ( parms )
NULL
f
f u n c t i o n ( lp , q , parms ) l p + exp ( parms ) ⇤ qnorm ( q )
names ( q )
format ( q )
drop ( exp ( o u t e r ( lp , q , FUN = f , parms = parms ) ) )
}
median.surv

f u n c t i o n ( x ) q u a n t i l e . s u r v ( l p=x )

# Improve variable labels for the nomogram
f.approx
N e w l a b e l s ( f . a p p r o x , c ( ’ D i s e a s e Group ’ , ’ Mean A r t e r i a l BP ’ ,
’ C r e a t i n i n e ’ , ’ Age ’ , ’ Heart Rate ’ , ’SUPPORT Coma S c o r e ’ ,
’ PaO2/ ( . 0 1 ⇤FiO2 ) ’ , ’ADL ’ , ’ R e s p . Rate ’ ) )
nom
nomogram ( f . a p p r o x ,
p a f i . i =c ( 0 , 5 0 , 1 0 0 , 2 0 0 , 3 0 0 , 5 0 0 , 6 0 0 , 7 0 0 , 8 0 0 , 9 0 0 ) ,
f u n= l i s t ( ’ Median S u r v i v a l Time ’=m e d i a n . s u r v ,
’ Mean S u r v i v a l Time ’ =e x p e c t e d . s u r v ) ,
f u n . a t=c ( . 1 , . 2 5 , . 5 , 1 , 2 , 5 , 1 0 , 2 0 , 4 0 ) )
p l o t (nom , c e x . v a r =1 , c e x . a x i s=. 7 5 , lmgp=. 2 5 )
# Figure 19.12

480

19 Parametric Survival Modeling and Model Approximation
0

10

20

30

40

50

60

70

80

90

100

Points
Disease Group

MOSF w/Malig
Coma

ARF/MOSF w/Sepsis

Mean Arterial BP
0
6

7

8

20
10 11

9

40

60

80 120

12

Creatinine
5

3

2

1

0

Age
100

70

60

50

30

10

Heart Rate
300

SUPPORT Coma
Score

200

100

100

50

0

70 50 30 10
300

PaO2/(.01*FiO2)
0
5

50

100

200 500

700

900

7

ADL
Resp. Rate

4.5 2 1
0
65
60
55
50

45

40

35

30

0 5

15

Total Points
0

50

100

150

200

250

300

350

400

450

Linear Predictor
−7

−5

−3

−1

1

2

3

4

Median Survival Time
0.10.25
0.5 1 2 5 102040

Mean Survival Time
0.10.25
0.5 1 2 5 102040

Fig. 19.12 Nomogram for predicting median and mean survival time, based on approximation of full model

19.6 Problems

481

19.6 Problems
Analyze the Mayo Clinic PBC dataset.
1. Graphically assess whether Weibull (extreme value), exponential, loglogistic, or log-normal distributions will fit the data, using a few apparently
important stratification factors.
2. For the best fitting parametric model from among the four examined,
fit a model containing several sensible covariables, both categorical and
continuous. Do a Wald test for whether each factor in the model has an
association with survival time, and a likelihood ratio test for the simultaneous contribution of all predictors. For classification factors having more
than two levels, be sure that the Wald test has the appropriate degrees
of freedom. For continuous factors, verify or relax linearity assumptions.
If using a Weibull model, test whether a simpler exponential model would
be appropriate. Interpret all estimated coefficients in the model. Write the
full survival model in mathematical form. Generate a predicted survival
curve for a patient with a given set of characteristics.
See [354] for an analysis of this dataset using linear splines in time and in the
covariables.

Chapter 20

Cox Proportional Hazards Regression
Model

20.1 Model
20.1.1 Preliminaries
The Cox proportional hazards model129 is the most popular model for the
analysis of survival data. It is a semiparametric model; it makes a parametric
assumption concerning the e↵ect of the predictors on the hazard function,
but makes no assumption regarding the nature of the hazard function (t)
itself. The Cox PH model assumes that predictors act multiplicatively on the
hazard function but does not assume that the hazard function is constant (i.e.,
exponential model), Weibull, or any other particular form. The regression
portion of the model is fully parametric; that is, the regressors are linearly
related to log hazard or log cumulative hazard. In many situations, either
the form of the true hazard function is unknown or it is complex, so the
Cox model has definite advantages. Also, one is usually more interested in
the e↵ects of the predictors than in the shape of (t), and the Cox approach
allows the analyst to essentially ignore (t), which is often not of primary
interest.
The Cox PH model uses only the rank ordering of the failure and censoring
times and thus is less a↵ected by outliers in the failure times than fully
parametric methods. The model contains as a special case the popular logrank test for comparing survival of two groups. For estimating and testing
regression coefficients, the Cox model is as efficient as parametric models
(e.g., Weibull model with PH) even when all assumptions of the parametric
model are satisfied.168
When a parametric model’s assumptions are not true (e.g., when a Weibull
model is used and the population is not from a Weibull survival distribution
so that the choice of model is incorrect), the Cox analysis is more efficient
than the parametric analysis. As shown below, diagnostics for checking Cox
model assumptions are very well developed.

483

1

484

20 Cox Proportional Hazards Regression Model

20.1.2 Model Definition
The Cox PH model is most often stated in terms of the hazard function:
(t|X) = (t) exp(X ).

(20.1)

We do not include an intercept parameter in X here. Note that this is
identical to the parametric PH model stated earlier. There is an important
di↵erence, however, in that now we do not assume any specific shape for (t).
For the moment, we are not even interested in estimating (t). The reason
for this departure from the fully parametric approach is due to an ingenious
conditional argument by Cox.129 Cox argued that when the PH model holds,
information about (t) is not very useful in estimating the parameters of
primary interest, . By special conditioning in formulating the log likelihood
function, Cox showed how to derive a valid estimate of that does not require
estimation of (t) as (t) dropped out of the new likelihood function. Cox’s
derivation focuses on using the information in the data that relates to the
relative hazard function exp(X ).

20.1.3 Estimation of
Cox’s derivation of an estimator of can be loosely described as follows. Let
t1 < t2 < . . . < tk represent the unique ordered failure times in the sample of
n subjects; assume for now that there are no tied failure times (tied censoring
times are allowed) so that k = n. Consider the set of individuals at risk of
failing an instant before failure time ti . This set of individuals is called the
risk set at time ti , and we use Ri to denote this risk set. Ri is the set of
subjects j such that the subject had not failed or been censored by time ti ;
that is, the risk set Ri includes subjects with failure/censoring time Yj ti .
The conditional probability that individual i is the one that failed at ti ,
given that the subjects in the set Ri are at risk of failing, and given further
that exactly one failure occurs at ti , is
Prob{subject i fails at ti |Ri and one failure at ti } =
Prob{subject i fails at ti |Ri }
Prob{ one failure at ti |Ri }

(20.2)

using the rules of conditional probability. This conditional probability equals
P

(ti ) exp(Xi )
exp(Xi )
exp(Xi )
=P
=P
j2Ri (ti ) exp(Xj )
j2Ri exp(Xj )
Yj ti exp(Xj )

(20.3)

20.1 Model

485

independent of (t). To understand this likelihood, consider a special case
where the predictors have no e↵ect; that is,
= 0 [90, pp. 48–49]. Then
exp(Xi ) = exp(Xj ) = 1 and Prob{subject i is the subject that failed at
ti |Ri and one failure occurred at ti } is 1/ni where ni is the number of subjects
at risk at time ti .
By arguing that these conditional probabilities are themselves conditionally independent across the di↵erent failure times, a total likelihood can be
computed by multiplying these individual likelihoods over all failure times.
Cox termed this a partial likelihood for :
Y

L( ) =

P

Yi uncensored

The log partial likelihood is
X
log L( ) =

Yi uncensored

exp(Xi )
.
Yj Yi exp(Xj )

(20.4)

X

(20.5)

{Xi

log[

Yj

exp(Xj )]}.

Yi

Cox and others have shown that this partial log likelihood can be treated as
an ordinary log likelihood to derive valid (partial) MLEs of . Note that this
log likelihood is una↵ected by the addition of a constant to any or all of the
Xs. This is consistent with the fact that an intercept term is unnecessary and
cannot be estimated since the Cox model is a model for the relative hazard
and does not directly estimate the underlying hazard (t).
When there are tied failure times in the sample, the true partial log likelihood function involves permutations so it can be time-consuming to compute.
When the number of ties is not large, Breslow69 has derived a satisfactory
approximate log likelihood function. The formula given above, when applied
without modification to samples containing ties, actually uses Breslow’s approximation. If there are ties so that k < n and t1 , . . . , tk denote the unique
failure times as we originally intended, Breslow’s approximation is written as
log L( ) =

k
X
i=1

{Si

di log[

X

Yj

exp(Xj )]},

(20.6)

ti

P
where Si = j2Di Xj , Di is the set of indexes j for subjects failing at time
ti , and di is the number of failures at ti .
Efron168 derived another approximation to the true likelihood that is significantly more accurate than the Breslow approximation and often yields
estimates that are very close to those from the more cumbersome permutation likelihood:282
log L( ) =

k
X
i=1

{Si

di
X
j=1

log[

X

Yj

ti

exp(Xj )

486

20 Cox Proportional Hazards Regression Model

j
di

1 X

exp(Xl )]}.

(20.7)

l2Di

In the special case when all tied failure times are from subjects with identical Xi , the Efron approximation yields the exact (permutation) marginal
likelihood (Therneau, personal communication, 1993).
Kalbfleisch and Prentice323 showed that Cox’s partial likelihood, in the
absence of predictors that are functions of time, is a marginal distribution of
the ranks of the failure/censoring times.
See Therneau and Grambsch597 and Huang and Harrington303 for descriptions of penalized partial likelihood estimation methods for improving mean
squared error of estimates of in a similar fashion to what was discussed in
Section 9.10.

20.1.4 Model Assumptions and Interpretation of
Parameters
The Cox PH regression model has the same assumptions as the parametric
PH model except that no assumption is made regarding the shape of the
underlying hazard or survival functions (t) and S(t). The Cox PH model
assumes, in its most basic form, linearity and additivity of the predictors
with respect to log hazard or log cumulative hazard. It also assumes the PH
assumption of no time by predictor interactions; that is, the predictors have
the same e↵ect on the hazard function at all values of t. The relative hazard
function exp(X ) is constant through time and the survival functions for
subjects with di↵erent values of X are powers of each other. If, for example,
the hazard of death at time t for treated patients is half that of control
patients at time t, this same hazard ratio is in e↵ect at any other time point.
In other words, treated patients have a consistently better hazard of death
over all follow-up time.
The regression parameters are interpreted the same as in the parametric
PH model. The only di↵erence is the absence of hazard shape parameters
in the model, since the hazard shape is not estimated in the Cox partial
likelihood procedure.

20.1.5 Example
Consider again the rat vaginal cancer data from Section 18.3.6. Figure 20.1
displays the nonparametric survival estimates for the two groups along with
estimates derived from the Cox model (by a method discussed later).
r e q u i r e ( rms )

20.1 Model

487

group
c ( r e p ( ’ Group 1 ’ , 1 9 ) , r e p ( ’ Group 2 ’ , 2 1 ) )
group
f a c t o r ( group )
dd
d a t a d i s t ( group ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )
days
c (143 ,164 ,188 ,188 ,190 ,192 ,206 ,209 ,213 ,216 ,220 ,227 ,230 ,
234 ,246 ,265 ,304 ,216 ,244 ,142 ,156 ,163 ,198 ,205 ,232 ,232 ,
233 ,233 ,233 ,233 ,239 ,240 ,261 ,280 ,280 ,296 ,296 ,323 ,204 ,344)
death
rep (1 ,40)
death [ c ( 1 8 , 1 9 , 3 9 , 4 0 ) ]
0
u n i t s ( days )
’ Day ’
df
d a t a . f r a m e ( days , death , group )
S
Surv ( days , death )
f
npsurv ( S ⇠ group , t y p e= ’ f l e m i n g ’ )
f o r ( meth i n c ( ’ e x a c t ’ , ’ b r e s l o w ’ , ’ e f r o n ’ ) ) {
g
cph ( S ⇠ group , method=meth , s u r v=TRUE, x=TRUE, y=TRUE)
# print(g) to see results
}
f.exp
psm ( S ⇠ group , d i s t= ’ e x p o n e n t i a l ’ )
fw
psm ( S ⇠ group , d i s t= ’ w e i b u l l ’ )
phform
pphsm ( fw )
co
gray ( c ( 0 , . 8 ) )
s u r v p l o t ( f , l t y=c ( 1 , 1 ) , lwd=c ( 1 , 3 ) , c o l=co ,
l a b e l . c u r v e s=FALSE, c o n f= ’ none ’ )
s u r v p l o t ( g , l t y=c ( 3 , 3 ) , lwd=c ( 1 , 3 ) , c o l=co , # Efron approx.
add=TRUE, l a b e l . c u r v e s=FALSE, c o n f . t y p e= ’ none ’ )
legend ( c (2 , 160) , c ( .38 , .54 ) ,
c ( ’ Nonparametric E s t i m a t e s ’ , ’ Cox Breslow E s t i m a t e s ’ ) ,
l t y=c ( 1 , 3 ) , c e x=. 8 , bty= ’ n ’ )
l e g e n d ( c ( 2 , 1 6 0 ) , c ( . 1 8 , . 3 4 ) , c e x=. 8 ,
c ( ’ Group 1 ’ , ’ Group 2 ’ ) , lwd=c ( 1 , 3 ) , c o l=co , bty= ’ n ’ )

The predicted survival curves from the fitted Cox model are in good agreement with the nonparametric estimates, again verifying the PH assumption
for these data. The estimates of the group e↵ect from a Cox model (using the
exact likelihood since there are ties, along with both Efron’s and Breslow’s
approximations) as well as from a Weibull model and an exponential model
are shown in Table 20.1. The exponential model, with its constant hazard,
cannot accommodate the long early period with no failures. The group predictor was coded as X1 = 0 and X1 = 1 for Groups 1 and 2, respectively. For
this example, the Breslow likelihood approximation resulted in ˆ closer to
that from maximizing the exact likelihood. Note how the group e↵ect (47%
reduction in hazard of death by the exact Cox model) is underestimated by
the exponential model (9% reduction in hazard). The hazard ratio from the
Weibull fit agrees with the Cox fit.

488

20 Cox Proportional Hazards Regression Model

Survival Probability

1.0

0.8

0.6

0.4

Nonparametric Estimates
Cox−Breslow Estimates

0.2

Group 1
Group 2

0.0
0

30

60

90

120 150 180 210 240 270 300 330

Days
Fig. 20.1 Altschuler–Nelson–Fleming–Harrington nonparametric survival estimates
and Cox-Breslow estimates for rat data501
Table 20.1

Model

Group Regression
Coefficient
Cox (Exact)
0.629
Cox (Efron)
0.569
Cox (Breslow)
0.596
Exponential
0.093
Weibull (AFT)
0.132
Weibull (PH)
0.721

S.E.
0.361
0.347
0.348
0.334
0.061
–

Wald
Group 2:1
P -Value Hazard Ratio
0.08
0.533
0.10
0.566
0.09
0.551
0.78
0.911
0.03
–
–
0.486

20.1.6 Design Formulations
Designs are no di↵erent for the Cox PH model than for other models except
for one minor distinction. Since the Cox model does not have an intercept
parameter, the group omitted from X in an ANOVA model will go into the
underlying hazard function. As an example, consider a three-group model for
treatments A, B, and C. We use the two dummy variables
X1 = 1

if treatment is A, 0 otherwise, and

X2 = 1

if treatment is B, 0 otherwise.

20.1 Model

489

The parameter 1 is the A : C log hazard ratio or di↵erence in hazards at
any time t between treatment A and treatment C. 2 is the B : C log hazard
ratio (exp( 2 ) is the B : C hazard ratio, etc.). Since there is no intercept
parameter, there is no direct estimate of the hazard function for treatment
C or any other treatment; only relative hazards are modeled.
As with all regression models, a Wald, score, or likelihood ratio test for
di↵erences between any treatments is conducted by testing H0 : 1 = 2 = 0
with 2 d.f.

20.1.7 Extending the Model by Stratification
A unique feature of the Cox PH model is its ability to adjust for factors that
are not modeled. Such factors usually take the form of polytomous stratification factors that either are too difficult to model or do not satisfy the PH
assumption. For example, a subject’s occupation or clinical study site may
take on dozens of levels and the sample size may not be large enough to
model this nominal variable with dozens of dummy variables. Also, one may
know that a certain predictor (either a polytomous one or a continuous one
that is grouped) may not satisfy PH and it may be too complex to model the
hazard ratio for that predictor as a function of time.
The idea behind the stratified Cox PH model is to allow the form of the
underlying hazard function to vary across levels of the stratification factors.
A stratified Cox analysis ranks the failure times separately within strata.
Suppose that there are b strata indexed by j = 1, 2, . . . , b. Let C denote the
stratum identification. For example, C = 1 or 2 may stand for the female and
male strata, respectively. The stratified PH model is
(t|X, C = j) =

j (t) exp(X

S(t|X, C = j) = Sj (t)

exp(X )

),
.

or
(20.8)

Here j (t) and Sj (t) are, respectively, the underlying hazard and survival
functions for the jth stratum. The model does not assume any connection
between the shapes of these functions for di↵erent strata.
In this stratified analysis, the data are stratified by C but, by default, a
common vector of regression coefficients is fitted across strata. These common
regression coefficients can be thought of as “pooled” estimates. For example,
a Cox model with age as a (modeled) predictor and sex as a stratification
variable essentially estimates the common slope of age by pooling information
about the age e↵ect over the two sexes. The e↵ect of age is adjusted by sex
di↵erences, but no assumption is made about how sex a↵ects survival. There
is no PH assumption for sex. Levels of the stratification factor C can represent
multiple stratification factors that are cross-classified. Since these factors are
not modeled, no assumption is made regarding interactions among them.

490

20 Cox Proportional Hazards Regression Model

At first glance it appears that stratification causes a loss of efficiency.
However, in most cases the loss is small as long as the number of strata is not
too large with regard to the total number of events. A stratum that contains
no events contributes no information to the analysis, so such a situation
should be avoided if possible.
The stratified or “pooled” Cox model is fitted by formulating a separate
log likelihood function for each stratum, but with each log likelihood having a
common vector. If di↵erent strata are made up of independent subjects, the
strata are independent and the likelihood functions are multiplied together
to form a joint likelihood over strata. Log likelihood functions are thus added
over strata. This total log likelihood function is maximized once to derive a
pooled or stratified estimate of and to make an inference about . No inference can be made about the stratification factors. They are merely “adjusted
for.”
Stratification is useful for checking the PH and linearity assumptions for
one or more predictors. Predicted Cox survival curves (Section 20.2) can
be derived by modeling the predictors in the usual way, and then stratified
survival curves can be estimated by using those predictors as stratification
factors. Other factors for which PH is assumed can be modeled in both instances. By comparing the modeled versus stratified survival estimates, a
graphical check of the assumptions can be made. Figure 20.1 demonstrates
this method although there are no other factors being adjusted for and stratified Cox estimates are KM estimates. The stratified survival estimates are
derived by stratifying the dataset to obtain a separate underlying survival
curve for each stratum, while pooling information across strata to estimate
coefficients of factors that are modeled.
Besides allowing a factor to be adjusted for without modeling its e↵ect,
a stratified Cox PH model can also allow a modeled factor to interact with
strata.140, 177, 596 For the age–sex example, consider the following model with
X1 denoting age and C = 1, 2 denoting females and males, respectively.
(t|X1 , C = 1) =

1 (t) exp( 1 X1 )

(t|X1 , C = 2) =

2 (t) exp( 1 X1

+

2 X1 ).

(20.9)

j (t) exp( 1 X1

+

2 X2 )

(20.10)

This model can be simplified to
(t|X1 , C = j) =

if X2 is a product interaction term equal to 0 for females and X1 for males.
The 2 parameter quantifies the interaction between age and sex: it is the
di↵erence in the age slope between males and females. Thus the interaction
between age and sex can be quantified and tested, even though the e↵ect of
sex is not modeled!
The stratified Cox model is commonly used to adjust for hospital di↵erences in a multicenter randomized trial. With this method, one can allow

20.2 Survival Probability and Secondary Parameters

491

for di↵erences in outcome between q hospitals without estimating q 1 parameters. Treatment ⇥ hospital interactions can be tested efficiently without
computational problems by estimating only the treatment main e↵ect, after
stratifying on hospital. The score statistic (with q 1 d.f.) for testing q 1
treatment ⇥ hospital interaction terms is then computed (“residual 2 ” in a
stepwise procedure with treatment ⇥ hospital terms as candidate predictors).
The stratified Cox model turns out to be a generalization of the conditional
logistic model for analyzing matched set (e.g., case-control) data.70 Each
stratum represents a set, and the number of “failures” in the set is the number
of “cases” in that set. For r : 1 matching (r may vary across sets), the
Breslow69 likelihood may be used to fit the conditional logistic model exactly.
For r : m matching, an exact Cox likelihood must be computed.

20.2 Estimation of Survival Probability and Secondary
Parameters
As discussed above, once a partial log likelihood function is derived, it is
used as if it were an ordinary log likelihood function to estimate , estimate
standard errors of , obtain confidence limits, and make statistical tests. Point
and interval estimates of hazard ratios are obtained in the same fashion as
with parametric PH models discussed earlier.
The Cox model and parametric survival models di↵er markedly in how one
estimates S(t|X). Since the Cox model does not depend on a choice of the
underlying survival function S(t), fitting a Cox model does not result directly
in an estimate of S(t|X). However, several authors have derived secondary
estimates of S(t|X). One method is the discrete hazard model of Kalbfleisch
and Prentice [324, pp. 36–37, 84–87]. Their estimator has two advantages: it
is an extension of the Kaplan–Meier estimator and is identical to SKM if the
estimated value of happened to be zero or there are no covariables being
modeled; and it is not a↵ected by the choice of what constitutes a “standard”
subject having the underlying survival function S(t). In other words, it would
not matter whether the standard subject is one having age equal to the mean
age in the sample or the median age in the sample; the estimate of S(t|X)
as a function of X = age would be the same (this is also true of another
estimator which follows).
Let t1 , t2 , . . . , tk denote the unique failure times in the sample. The discrete
hazard model assumes that the probability of failure is greater than zero only
at observed failure times. The probability of failure at time tj given that the
subject has not failed before that time is also the hazard of failure at time
tj since the model is discrete. The hazard at tj for the standard subject is
written j . Letting ↵j = 1
j , the underlying survival function can be
written

492

20 Cox Proportional Hazards Regression Model

S(ti ) =

iY
1

↵j , i = 1, 2, . . . , k

(↵0 = 1).

(20.11)

j=0

A separate equation can be solved using the Newton–Raphson method to
estimate each ↵j . If there is only one failure at time ti , there is a closed-form
solution for the maximum likelihood estimate of ↵i , ai , letting j denote the
subject who failed at ti . ˆ denotes the partial MLE of .
X
ˆ
↵
ˆ i = [1 exp(Xj ˆ)/
exp(Xm ˆ)]exp( Xj ) .
(20.12)
Ym Yj

If ˆ = 0, this formula reduces to a conditional probability component of the
product-limit estimator, 1 (1/number at risk).
The estimator of the underlying survival function is
Y
Ŝ(t) =
↵
ˆj ,
(20.13)
j:tj t

and the estimate of the probability of survival past time t for a subject with
predictor values X is
ˆ
Ŝ(t|X) = Ŝ(t)exp(X ) .
(20.14)
When the model is stratified, estimation of the ↵j and S is carried out
separately within each stratum once ˆ is obtained by pooling over strata. The
stratified survival function estimates can be thought of as stratified Kaplan–
Meier estimates adjusted for X, with the adjustment made by assuming PH
and linearity. As mentioned previously, these stratified adjusted survival estimates are useful for checking model assumptions and for providing a simple
way to incorporate factors that violate PH.
The stratified estimates are also useful in themselves as descriptive statistics without making assumptions about a major factor. For example, in a
study from Cali↵ et al.86 to compare medical therapy with coronary artery
bypass grafting (CABG), the model was stratified by treatment but adjusted
for a variety of baseline characteristics by modeling. These adjusted survival
estimates do not assume a form for the e↵ect of surgery. Figure 20.2 displays
unadjusted (Kaplan–Meier) and adjusted survival curves, with baseline predictors adjusted to their mean levels in the combined sample. Notice that
valid adjusted survival estimates are obtained even though the curves cross
(i.e., PH is violated for the treatment variable). These curves are essentially
product limit estimates with respect to treatment and Cox PH estimates with
respect to the baseline descriptor variables.
The Kalbfleisch–Prentice discrete underlying hazard model estimates of
the ↵j are one minus estimates of the hazard function at the discrete failure
times. However, these estimated hazard functions are usually too “noisy” to
be useful unless the sample size is very large or the failure times have been
grouped (say by rounding).

20.2 Survival Probability and Secondary Parameters

493

Fig. 20.2 Unadjusted (Kaplan–Meier) and adjusted (Cox–Kalbfleisch–Prentice) estimates of survival. Top, Kaplan–Meier estimates for patients treated medically and
surgically at Duke University Medical Center from November 1969 through December
1984. These survival curves are not adjusted for baseline prognostic factors. Numbers
of patients alive at each follow-up interval for each group are given at the bottom of
the figure. Bottom, survival curves for patients treated medically or surgically after
adjusting for all known important baseline prognostic characteristics.86 Reprinted by
permission, American Medical Association.

Just as Kalbfleisch and Prentice have generalized the Kaplan–Meier estimator to allow for covariables, Breslow69 has generalized the Altschuler–
Nelson–Aalen–Fleming–Harrington estimator to allow for covariables. Using
the notation in Section 20.1.3, Breslow’s estimate is derived through an estimate of the cumulative hazard function:
ˆ =
⇤(t)

X

i:ti <t

P

di
Yi ti

For any X, the estimates of ⇤ and S are

exp(Xi ˆ)

.

(20.15)

494

20 Cox Proportional Hazards Regression Model

ˆ
ˆ exp(X ˆ)
⇤(t|X)
= ⇤(t)
ˆ exp(X ˆ)].
Ŝ(t|X) = exp[ ⇤(t)

2
3

(20.16)

More asymptotic theory has been derived from the Breslow estimator than
for the Kalbfleisch–Prentice estimator. Another advantage of the Breslow
estimator is that it does not require iterative computations for di > 1. Lawless [374, p. 362] states that the two survival function estimators di↵er little
except in the right-hand tail when all di s are unity. Like the Kalbfleisch–
Prentice estimator, the Breslow estimator is invariant under di↵erent choices
of “standard subjects” for the underlying survival S(t).
Somewhat complex formulas are available for computing confidence limits
of Ŝ(t|X).608

20.3 Test Statistics
Wald, score, and likelihood ratio statistics are useful and valid for drawing
inferences about in the Cox model. The score test deserves special mention
here. If there is a single binary predictor in the model that describes two
groups, the score test for assessing the importance of the binary predictor
is virtually identical to the Mantel–Haenszel log-rank test for comparing the
two groups. If the analysis is stratified for other (nonmodeled) factors, the
score test from a stratified Cox model is equivalent to the corresponding
stratified log-rank test. Of course, the likelihood ratio or Wald tests could
also be used in this situation, and in fact the likelihood ratio test may be
better than the score test (i.e., type I errors by treating the likelihood ratio
test statistic as having a 2 distribution may be more accurate than using
the log-rank statistic).
The Cox model can be thought of as a generalization of the log-rank procedure since it allows one to test continuous predictors, perform simultaneous
tests of various predictors, and adjust for other continuous factors without
grouping them. Although a stratified log-rank test does not make assumptions regarding the e↵ect of the adjustment (stratifying) factors, it makes the
same assumption (i.e., PH) as the Cox model regarding the treatment e↵ect
for the statistical test of no di↵erence in survival between groups.

20.4 Residuals

4

Therneau et al.598 discussed four types of residuals from the Cox model:
martingale, score, Schoenfeld, and deviance. The first three have been proven
to be very useful, as indicated in Table 20.2.

20.5 Assessment of Model Fit

495

Table 20.2

Residual
Purposes
Martingale Assessing adequacy of a hypothesized predictor
transformation. Graphing an estimate of a
predictor transformation (Section 20.5.1).
Score
Detecting overly influential observations
(Section 20.8). Robust estimate of
covariance matrix of ˆ (Section 9.5).403
Schoenfeld Testing PH assumption (Section 20.5.2).
Graphing estimate of hazard ratio function
(Section 20.5.2).

20.5 Assessment of Model Fit
As stated before, the Cox model makes the same assumptions as the parametric PH model except that it does not assume a given shape for (t) or
S(t). Because the Cox PH model is so widely used, methods of assessing its fit
are dealt with in more detail than was done with the parametric PH models.

20.5.1 Regression Assumptions
Regression assumptions (linearity, additivity) for the PH model are displayed
in Figures 18.3 and 18.5. As mentioned earlier, the regression assumptions can
ˆ
be verified by stratifying by X and examining log ⇤(t|X)
or log[⇤KM (t|X)]
estimates as a function of X at fixed time t. However, as was pointed out
in logistic regression, the stratification method is prone to problems of high
variability of estimates. The sample size must be moderately large before
estimates are precise enough to observe trends through the “noise.” If one
wished to divide the sample by quintiles of age and 15 events were thought to
be needed in each stratum to derive a reliable estimate of log[⇤KM (2 years)],
there would need to be 75 events in the entire sample. If the Kaplan–Meier
estimates were needed to be adjusted for another factor that was binary,
twice as many events would be needed to allow the sample to be stratified
by that factor.
Figure 20.3 displays Kaplan–Meier three-year log cumulative hazard estimates stratified by sex and decile of age. The simulated sample consists of
2000 hypothetical subjects (389 of whom had events), with 1174 males (146
deaths) and 826 females (243 deaths). The sample was drawn from a population with a known survival distribution that is exponential with hazard
function

496

20 Cox Proportional Hazards Regression Model

(t|X1 , X2 ) = .02 exp[.8X1 + .04(X2

50)],

(20.17)

where X1 represents the sex group (0 = male, 1 = female) and X2 age in
years, and censoring is uniform. Thus for this population PH, linearity, and
additivity hold. Notice the amount of variability and wide confidence limits
in the stratified nonparametric survival estimates.
n
2000
set.seed (3)
age
50 + 12 ⇤ rnorm ( n )
l a b e l ( age )
’ Age ’
sex
f a c t o r ( 1 + ( r u n i f ( n )  . 4 ) , 1 : 2 , c ( ’ Male ’ , ’ Female ’ ) )
cens
15 ⇤ r u n i f ( n )
h
. 0 2 ⇤ exp ( . 0 4 ⇤ ( age
5 0 ) + . 8 ⇤ ( s e x == ’ Female ’ ) )
ft
log ( r u n i f ( n ) ) / h
e
i f e l s e ( f t  ce ns , 1 , 0 )
print ( table ( e ))
e
0
1611

1
389

ft
pmin ( f t , c e n s )
units ( ft )
’ Year ’
Srv
Surv ( f t , e )
age.dec
c u t 2 ( age , g =10 , l e v e l s . m e a n=TRUE)
label ( age.dec )
’ Age ’
dd
d a t a d i s t ( age , sex , a g e . d e c ) ;
o p t i o n s ( d a t a d i s t= ’ dd ’ )
f.np
cph ( Srv ⇠ s t r a t ( a g e . d e c ) + s t r a t ( s e x ) , s u r v=TRUE)
# surv=TRUE speeds up computations, and confidence limits when
# there are no covariables are still accurate.
p
P r e d i c t ( f . n p , a g e . d e c , sex , time =3 , l o g l o g=TRUE)
# Treat age.dec as a numeric variable (means within deciles)
p$ a g e . d e c
a s . n u m e r i c ( a s . c h a r a c t e r ( p$ a g e . d e c ) )
g g p l o t ( p , y l i m=c ( 5 ,
.5 ) )

As with the logistic model and other regression models, the restricted cubic
spline function is an excellent tool for modeling the regression relationship
with very few assumptions. A four-knot spline Cox PH model in two variables
(X1 , X2 ) that assumes linearity in X1 and no X1 ⇥ X2 interaction is given by
(t|X) = (t) exp(

1 X1

+

2 X2

+

= (t) exp(

1 X1

+ f (X2 )),

0
3 X2

+

00
4 X2 ),

(20.18)

where X20 and X200 are spline component variables as described earlier and
f (X2 ) is the spline function or spline transformation of X2 given by
f (X2 ) =

2 X2

+

0
3 X2

+

00
4 X2 .

(20.19)

In linear form the Cox model without assuming linearity in X2 is
log (t|X) = log (t) +

1 X1

+ f (X2 ).

(20.20)

20.5 Assessment of Model Fit

497

log[−log S(3)]

−1

−2

sex
Male

−3

Female

−4

−5
30

40

50

60

70

Age
Fig. 20.3 Kaplan–Meier log ⇤ estimates by sex and deciles of age, with 0.95 confidence limits. Solid line is for males, dashed line for females.

By computing partial MLEs of 2 , 3 , and 4 , one obtains the estimated
transformation of X2 that yields linearity in log hazard or log cumulative
hazard.
A similar model that does not assume PH in X1 is the Cox model stratified
on X1 . Letting the stratification factor be C = X1 , this model is
log (t|X2 , C = j) = log

j (t)

+

1 X2

+

= log

j (t)

+ f (X2 ).

0
2 X2

+

00
3 X2

(20.21)

This model does assume no X1 ⇥ X2 interaction.
Figure 20.4 displays the estimated spline function relating age and sex to
log[⇤(3)] in the simulated dataset, using the additive model stratified on sex.
f.noia
cph ( Srv ⇠ r c s ( age , 4 ) + s t r a t ( s e x ) , x=TRUE, y=TRUE)
# Get accurate C.L. for any age by specifying x=TRUE y=TRUE
# Note: for evaluating shape of regression, we would not
# ordinarily bother to get 3-year survival probabilities # would just use X ⇤ beta
# We do so here to use same scale as nonparametric estimates
w
l a t e x ( f . n o i a , i n l i n e=TRUE, d i g i t s =3)
l a t e x ( anova ( f . n o i a ) , t a b l e . e n v=FALSE, f i l e = ’ ’ )

498

20 Cox Proportional Hazards Regression Model
2

age
72.33
Nonlinear 0.69
TOTAL
72.33

d.f.
P
3 < 0.0001
2 0.7067
3 < 0.0001

p
P r e d i c t ( f . n o i a , age , sex , time =3 , l o g l o g=TRUE)
g g p l o t ( p , y l i m=c ( 5 ,
.5 ) )

log[−log S(3)]

−1

−2

sex
Male

−3

Female

−4

−5
20

40

60

80

Age
Fig. 20.4 Cox PH model stratified on sex, using spline function for age, no interaction. 0.95 confidence limits also shown. Solid line is for males, dashed line is for
females.

A formal test of the linearity assumption of the Cox PH model in the
above example is obtained by testing H0 : 2 = 3 = 0. The 2 statistic with
2 d.f. is 0.69, P = 0.7. The fitted equation, after simplifying the restricted
cubic spline to simpler (unrestricted) form, is X ˆ = 1.46 + 0.0255age +
2.59⇥10 5 (age 30.3)3+ 0.000101(age 45.1)3+ + 9.73⇥10 5 (age 54.6)3+
2.22⇥10 5 (age 69.6)3+ . Notice that the spline estimates are closer to the
true linear relationships than were the Kaplan–Meier estimates, and the confidence limits are much tighter. The spline estimates impose a smoothness on
the relationship and also use more information from the data by treating age
as a continuous ordered variable. Also, unlike the stratified Kaplan–Meier
estimates, the modeled estimates can make the assumption of no age ⇥ sex
interaction. When this assumption is true, modeling e↵ectively boosts the
sample size in estimating a common function for age across both sex groups.

20.5 Assessment of Model Fit

499

Of course, this assumption can be tested and interactions can be modeled if
necessary.
A Cox model that still does not assume PH for X1 = C but which allows
for an X1 ⇥ X2 interaction is
log (t|X2 , C = j) = log

j (t)

1 X2

+

4 X1 X2 +
00
6 X1 X2 .

+

+

0
2 X2

+

00
3 X2
0
5 X1 X2

+

(20.22)

This model allows the relationship between X2 and log hazard to be a smooth
nonlinear function and the shape of the X2 e↵ect to be completely di↵erent
for each level of X1 if X1 is dichotomous. Figure 20.5 displays a fit of this
model at t = 3 years for the simulated dataset.
cph ( Srv ⇠ r c s ( age , 4 ) ⇤ s t r a t ( s e x ) , x=TRUE, y=TRUE,
s u r v=TRUE)
w
l a t e x ( f . i a , i n l i n e=TRUE, d i g i t s =3)
l a t e x ( anova ( f . i a ) , t a b l e . e n v=FALSE, f i l e = ’ ’ )
f.ia

2

age (Factor+Higher Order Factors)
All Interactions
Nonlinear (Factor+Higher Order Factors)
age ⇥ sex (Factor+Higher Order Factors)
Nonlinear
Nonlinear Interaction : f(A,B) vs. AB
TOTAL NONLINEAR
TOTAL NONLINEAR + INTERACTION
TOTAL

72.82
1.05
1.80
1.05
1.05
1.05
1.80
1.80
72.82

d.f.
P
6 < 0.0001
3 0.7886
4 0.7728
3 0.7886
2 0.5911
2 0.5911
4 0.7728
5 0.8763
6 < 0.0001

p
P r e d i c t ( f . i a , age , sex , time =3 , l o g l o g=TRUE)
g g p l o t ( p , y l i m=c ( 5 ,
.5 ) )

The fitted equation is X ˆ = 1.8 + 0.0493age 2.15⇥10 6 (age 30.3)3+
2.82⇥10 5 (age 45.1)3+ + 5.18⇥10 5 (age 54.6)3+ 2.15⇥10 5 (age 69.6)3+ +
[Female][ 0.0366age+4.29⇥10 5 (age 30.3)3+ 0.00011(age 45.1)3+ +6.74⇥
10 5 (age 54.6)3+ 2.32⇥10 7 (age 69.6)3+ ]. The test for interaction yielded
2
= 1.05 with 3 d.f., P = 0.8. The simultaneous test for linearity and
additivity yielded 2 = 1.8 with 5 d.f., P = 0.9. Note that allowing the
model to be very flexible (not assuming linearity in age, additivity between
age and sex, and PH for sex) still resulted in estimated regression functions
that are very close to the true functions. However, confidence limits in this
unrestricted model are much wider.
Figure 20.6 displays the estimated relationship between left ventricular
ejection fraction (LVEF) and log hazard ratio for cardiovascular death in a
sample of patients with significant coronary artery disease. The relationship

500

20 Cox Proportional Hazards Regression Model

log[−log S(3)]

−1

−2

sex
Male

−3

Female

−4

−5
20

40

60

80

Age
Fig. 20.5 Cox PH model stratified on sex, with interaction between age spline and
sex. 0.95 confidence limits are also shown. Solid line is for males, dashed line for
females.

5

is estimated using three knots placed at quantiles 0.05, 0.5, and 0.95 of LVEF.
Here there is significant nonlinearity (Wald 2 = 9.6 with 1 d.f.). The graphs
leads to a transformation of LVEF that better satisfies the linearity assumption: min(LVEF, 0.5). This transformation has the best log likelihood “for
the money” as judged by the Akaike information criterion (AIC = 2 log
L.R. 2⇥ no. parameters = 127). The AICs for 3, 4, 5, and 6-knot spline fits
were, respectively, 126, 124, 122, and 120.
Had the suggested transformation been more complicated than a truncation, a tentative transformation could have been checked for adequacy by
expanding the new transformed variable into a new spline function and testing it for linearity.
Other methods based on smoothed residual plots are also valuable tools
for selecting predictor transformations. Therneau et al.598 describe residuals
based on martingale theory that can estimate transformations of any number of predictors omitted from a Cox model fit, after adjusting for other
variables included in the fit. Figure 20.7 used various smoothing methods on
the points (LVEF, residual). First, the R loess function93 was used to obtain
a smoothed scatterplot fit and approximate 0.95 confidence bars. Second, an
ordinary least squares model, representing LVEF as a restricted cubic spline
with five default knots, was fitted. Ideally, both fits should have used weighted
regression as the residuals do not have equal variance. Predicted values from
this fit along with 0.95 confidence limits are shown. The loess and spline-

501

-1.0

20.5 Assessment of Model Fit

-2.0
-2.5
-3.0
-4.0

-3.5

log Relative Hazard

-1.5

Cox Regression Model, n=979 events=198
Statistic
X2 df
Model
L.R. 129.92 2 AIC= 125.92
Association Wald 157.45 2 p= 0.000
Linearity
Wald
9.59 1 p= 0.002

0.2

0.4

0.6

0.8

LVEF

Fig. 20.6 Restricted cubic spline estimate of relationship between LVEF and relative log
hazard from a sample of 979 patients and 198 cardiovascular deaths. Data from the Duke Cardiovascular Disease Databank.

0.5
0.0

Martingale Residual

1.0

loess Fit and 0.95 Confidence Bars
ols Spline Fit and 0.95 Confidence Limits
lowess Smoother

0.2

0.4

0.6

0.8

LVEF

Fig. 20.7 Three smoothed estimates relating martingale residuals598 to LVEF.

linear regression agree extremely well. Third, Cleveland’s lowess scatterplot
smoother108 was used on the martingale residuals against LVEF. The suggested transformation from all three is very similar to that of Figure 20.6.
For smaller sample sizes, the raw residuals should also be displayed. There is
one vector of martingale residuals that is plotted against all of the predictors.
When correlations among predictors are mild, plots of estimated predictor
transformations without adjustment for other predictors (i.e., marginal transformations) may be useful. Martingale residuals may be obtained quickly by

502

20 Cox Proportional Hazards Regression Model

Table 20.3

Purpose
Estimate transformation for
a single variable
Check linearity assumption for
a single variable
Estimate marginal
transformations for p variables
Estimate transformation for
variable i adjusted for other
p 1 variables

6

Method
Force ˆ1 = 0 and compute
residuals from the null regression
Compute ˆ1 and compute
residuals from the linear regression
Force ˆ1 , . . . , ˆp = 0 and compute
residuals from the global null model
Estimate p 1 s, forcing ˆi = 0
Compute residuals from mixed
global/null model

fixing ˆ = 0 for all predictors. Then smoothed plots of predictor against
residual may be made for all predictors. Table 20.3 summarizes some of the
ways martingale residuals may be used. See section 10.5 for more information
on checking the regression assumptions. The methods for examining interaction surfaces described there apply without modification to the Cox model
(except that the nonparametric regression surface does not apply because of
censoring).

20.5.2 Proportional Hazards Assumption
Even though assessment of fit of the regression part of the Cox PH model
corresponds with other regression models such as the logistic model, the Cox
model has its own distributional assumption in need of validation. Here, of
course, the distributional assumption is not as stringent as with other survival models, but we do need to validate how the survival or hazard functions for various subjects are connected. There are many graphical and analytical methods of verifying the PH assumption. Two of the methods have
already been discussed: a graphical examination of parallelism of log ⇤ plots,
and a comparison of stratified with unstratified models (as in Figure 20.1).
Muenz460 suggested a simple modification that will make nonproportional
hazards more apparent: plot ⇤KM1 (t)/⇤KM2 (t) against t and check for flatness. The points on this curve can be passed through a smoother. One can
also plot di↵erences in log( log S(t)) against t.140 Arjas29 developed a graphical method based on plotting the estimated cumulative hazard versus the
cumulative number of events in a stratum as t progresses.
There are other methods for assessing whether PH holds that may be more
direct. Gore et al.,221 Harrell and Lee,261 and Kay334 (see also Anderson and
Senthilselvan27 ) describe a method for allowing the log hazard ratio (Cox

20.5 Assessment of Model Fit

503

Table 20.4

Time
Observations Deaths Log Hazard Standard
Interval
Ratio
Error
[0, 209)
40
12
0.47
0.59
[209, 234)
27
12
0.72
0.58
234 +
14
12
0.50
0.64

regression coefficient) for a predictor to be a function of time by fitting specially stratified Cox models. Their method assumes that the predictor being
examined for PH already satisfies the linear regression assumption. Followup time is stratified into intervals and a separate model is fitted to compute
the regression coefficient within each interval, assuming that the e↵ect of the
predictor is constant only within that small interval. It is recommended that
intervals be constructed so that there is roughly an equal number of events
in each. The number of intervals should allow at least 10 or 20 events per
interval.
The interval-specific log hazard ratio is estimated by excluding all subjects
with event/censoring time before the start of the interval and censoring all
events that occur after the end of the interval. This process is repeated for
all desired time intervals. By plotting the log hazard ratio and its confidence
limits versus the interval, one can assess the importance of a predictor as
a function of follow-up time and learn how to model non-PH using more
complicated models containing predictor by time interactions. If the hazard
ratio is approximately constant within broad time intervals, the time stratification method can be used for fitting and testing the predictor ⇥ time
interaction [261, p. 827]; [95].
Consider as an example the rat vaginal cancer data used in Figures 18.9, 18.10,
and 20.1. Recall that the PH assumption appeared to be satisfied for the two
groups although Figure 18.9 demonstrated some non-Weibullness. Figure 20.8
contains a ⇤ ratio plot.460
f
cph ( S ⇠ s t r a t ( group ) , s u r v=TRUE)
# For both strata, eval. S(t) at combined set of death times
times
s o r t ( u n i q u e ( days [ death == 1 ] ) )
est
s u r v e s t ( f , d a t a . f r a m e ( group=l e v e l s ( group ) ) ,
t i m e s=t i m e s , c o n f . t y p e=” none ” ) $ s u r v
cumhaz
log ( est )
p l o t ( t i m e s , cumhaz [ 2 , ] / cumhaz [ 1 , ] , x l a b=”Days” ,
y l a b=” Cumulative Hazard R a t i o ” , t y p e=” s ” )
a b l i n e ( h=1 , c o l=g r a y ( . 8 0 ) )
h a z a r d . r a t i o . p l o t ( g $x , g $y , e =12 , pr=TRUE)

The number of observations is declining over time because computations in
each interval were based on animals followed at least to the start of that
interval. The overall Cox regression coefficient was 0.57 with a standard

20 Cox Proportional Hazards Regression Model

Cumulative Hazard Ratio

504

2.5

2.0

1.5

1.0

0.5

150

200

250

300

Days
Fig. 20.8 Estimate of ⇤2 /⇤1 based on
Harrington nonparametric survival estimates.

log of Altschuler–Nelson–Fleming–

error of 0.35. There does not appear to be any trend in the hazard ratio over
time, indicating a constant hazard ratio or proportional hazards.
Now consider the Veterans Administration Lung Cancer dataset [324, pp.
60, 223–4]. Log ⇤ plots indicated that the four cell types did not satisfy PH.
To simplify the problem, omit patients with “large” cell type and let the
binary predictor be 1 if the cell type is “squamous” and 0 if it is “small”
or “adeno.” We are assessing whether survival patterns for the two groups
“squamous” versus “small” or “adeno” have PH. Interval-specific estimates
of the squamous : small,adeno log hazard ratios (using Efron’s likelihood) are
found in Table 20.5. Times are in days.
getHdata ( v a l u n g )
with ( valung , {
h a z a r d . r a t i o . p l o t ( 1 ⇤ ( c e l l == ’ Squamous ’ ) , Surv ( t , dead ) ,
e =25 , s u b s e t= c e l l != ’ Large ’ ,
pr=TRUE, p l=FALSE)
h a z a r d . r a t i o . p l o t ( 1 ⇤ kps , Surv ( t , dead ) , e =25 ,
pr=TRUE, p l=FALSE) } )

There is evidence of a trend of a decreasing hazard ratio over time which
is consistent with the observation that squamous cell patients had equal or
worse survival in the early period but decidedly better survival in the late
phase.
From the same dataset now examine the PH assumption for Karnofsky
performance status using data from all subjects, if the linearity assumption is

20.5 Assessment of Model Fit

505

Table 20.5

Time
Observations Deaths Log Hazard Standard
Interval
Ratio
Error
[0, 21)
110
26
0.46
0.47
[21, 52)
84
26
0.90
0.50
[52, 118)
59
26
1.35
0.50
118 +
28
26
1.04
0.45

Table 20.6

Time
Observations Deaths Log Hazard Standard
Interval
Ratio
Error
[0, 19]
137
27
0.053
0.010
[19, 49)
112
26
0.047
0.009
[49, 99)
85
27
0.036
0.012
99 +
28
26
0.012
0.014

satisfied. Interval-specific regression coefficients for this predictor are given in
Table 20.6. There is good evidence that the importance of performance status
is decreasing over time and that it is not a prognostic factor after roughly
99 days. In other words, once a patient survives 99 days, the performance
status does not contain much information concerning whether the patient will
survive 120 days. This non-PH would be more difficult to detect from Kaplan–
Meier plots stratified on performance status unless performance status was
stratified carefully.
Figure 20.9 displays a log hazard ratio plot for a larger dataset in which
more time strata can be formed. In 3299 patients with coronary artery disease,
827 su↵ered cardiovascular death or nonfatal myocardial infarction. Time
was stratified into intervals containing approximately 30 events, and within
each interval the Cox regression coefficient for an index of anginal pain and
ischemia was estimated. The pain/ischemia index, one component of which
is unstable angina, is seen to have a strong e↵ect for only six months. After
that, survivors have stabilized and knowledge of the angina status in the
previous six months is not informative.
Another method for graphically assessing the log hazard ratio over time is
based on Schoenfeld’s partial residuals496, 552 with respect to each predictor in
the fitted model. The residual is the contribution of the first derivative of the
log likelihood function with respect to the predictor’s regression coefficient,
computed separately at each risk set or unique failure time. In Figure 20.10
the “loess-smoothed”93 (with approximate 0.95 confidence bars) and “supersmoothed”203 relationship between the residual and unique failure time is
shown for the same data as Figure 20.9. For smaller n, the raw residuals

7

20 Cox Proportional Hazards Regression Model

0.1
0.0

•
•
•
• •
•
• • • •
•
•• •

Subset Estimate
0.95 C.L.
Smoothed

•

•
•

•
••

•

•

•
•

•

•

•

-0.1

Log Hazard Ratio

0.2

506

0

2

4

6

8

10

t
Predictor:Pain/Ischemia Index
Event:cdeathmi

Fig. 20.9 Stratified hazard ratios for pain/ischemia index over time. Data from the Duke
Cardiovascular Disease Databank.

should also be displayed to convey the proper sense of variability. The agreement with the pattern in Figure 20.9 is evident.
Pettitt and Bin Daud496 suggest scaling the partial residuals by the information matrix components. They also propose a score test for PH based on
the Schoenfeld residuals. Grambsch and Therneau228 found that the Pettitt–
Bin Daud standardization is sometimes misleading in that non-PH in one
variable may cause the residual plot for another variable to display nonPH. The Grambsch–Therneau weighted residual solves this problem and also
yields a residual that is on the same scale as the log relative hazard ratio.
Their residual is
ˆ + dRV̂ ,

8

(20.23)

where d is the total number of events, R is the n ⇥ p matrix of Schoenfeld
residuals, and V̂ is the estimated covariance matrix for ˆ. This new residual
can also be the basis for tests for PH, by correlating a user-specified function
of unique failure times with the weighted residuals.
The residual plot is computationally very attractive since the score residual
components are byproducts of Cox maximum likelihood estimation. Another
attractive feature is the lack of need to categorize the time axis. Unless approximate confidence intervals are derived from smoothing techniques, a lack
of confidence intervals from most software is one disadvantage of the method.

9

Formal tests for PH can be based on time-stratified Cox regression estimates.27, 261 Alternatively, more complex (and probably more efficient) formal
tests for PH can be derived by specifying a form for the time by predictor interaction (using what is called a time-dependent covariable in the Cox model)

507

0.0

0.1

0.2

loess Smoother, span=0.5, 0.95 C.L.
Super Smoother

-0.1

Scaled Schoenfeld Residual

20.5 Assessment of Model Fit

0

2

4

6

8

10

12

t

Fig. 20.10 Smoothed weighted228 Schoenfeld552 residuals for the same data in Figure 20.9.
Test for PH based on the correlation (⇢) between the individual weighted Schoenfeld residuals
and the rank of failure time yielded ⇢ = 0.23, z = 6.73, P = 2 ⇥ 10 11 .

and testing coefficients of such interactions for significance. The obsolete Version 5 SAS PHGLM procedure used a computationally fast procedure based on
an approximate score statistic that tests for linear correlation between the
rank order of the failure times in the sample and Schoenfeld’s partial residuals.253, 261 This test is available in R (for both weighted and unweighted
residuals) using Therneau’s cox.zph function in the survival package. For
the results in Figure 20.10, the test for PH is highly significant (correlation
coefficient = 0.23, normal deviate z = 6.73). Since there is only one regression parameter, the weighted residuals are a constant multiple of the
unweighted ones, and have the same correlation coefficient.
Another method for checking the PH assumption which is especially applicable to a polytomous predictor involves taking ratios of parametrically
estimated hazard functions estimated separately for each level of the predictor. For example, suppose that a risk factor X is either present (X = 1) or
absent (X = 0), and suppose that separate Weibull distributions adequately
fit the survival pattern of each group. If there are no other predictors to adjust for, define the hazard function for X = 0 as ↵ t 1 and the hazard for
X = 1 as ✓t✓ 1 . The X = 1 : X = 0 hazard ratio is
↵ t
✓t✓

1
1

=

↵
t
✓

✓

.

(20.24)

The hazard ratio is constant if the two Weibull shape parameters ( and ✓)
are equal. These Weibull parameters can be estimated separately and a Wald
test statistic of H0 : = ✓ can be computed by dividing the square of their
di↵erence by the sum of the squares of their estimated standard errors, or

10

11

508

20 Cox Proportional Hazards Regression Model

Table 20.7

t

log Hazard
Ratio
10
0.36
36
0.64
83.5
0.83
200
1.02

12

better by a likelihood ratio test. A plot of the estimate of the hazard ratio
above as a function of t may also be informative.
In the VA lung cancer data, the MLEs of the Weibull shape parameters
for squamous cell cancer is 0.77 and for the combined small + adeno is 0.99.
Estimates of the reciprocals of these parameters, provided by some software
packages, are 1.293 and 1.012 with respective standard errors of 0.183 and
0.0912. A Wald test for di↵erences in these reciprocals provides a rough test
for a di↵erence in the shape estimates. The Wald 2 is 1.89 with 1 d.f. indicating slight evidence for non-PH.
The fitted Weibull hazard function for squamous cell cancer is .0167t0.23
and for adeno + small is 0.0144t 0.01 . The estimated hazard ratio is then
1.16t 0.22 and the log hazard ratio is 0.148 0.22 log t. By evaluating this
Weibull log hazard ratio at interval midpoints (arbitrarily using t = 200
for the last (open) interval) we obtain log hazard ratios that are in good
agreement with those obtained by time-stratifying the Cox model (Table 20.5)
as shown in Table 20.7.
There are many methods of assessing PH using time-dependent covariables in the Cox model.221, 579 Gray232, 233 mentions a flexible and efficient
method of estimating the hazard ratio function using time-dependent covariables that are X ⇥ spline term interactions. Gray’s method uses B-splines and
requires one to maximize a penalized log-likelihood function. Verweij and van
Houwelingen633 developed a more nonparametric version of this approach.
Hess283 uses simple restricted cubic splines to model the time-dependent covariable e↵ects (see also [4, 281, 390, 491]). Suppose that k = 4 knots are used
and that a covariable X is already transformed correctly. The model is
log (t|X) = log (t) +

1X

+

2 Xt

+

3 Xt

0

+

4 Xt

00

,

(20.25)

where t0 , t00 are constructed spline variables (Equation 2.25). The X + 1 : X
log hazard ratio function is estimated by
ˆ1 + ˆ2 t + ˆ3 t0 + ˆ4 t00 .

(20.26)

20.5 Assessment of Model Fit

509

Table 20.8 Assumptions of the Proportional Hazards Model
Variables

Assumptions

Verification

Response Variable T
Time Until Event

Shape of (t|X) for fixed X
as t "
Cox: none
Weibull: t✓

Interaction Between X
and T

• Categorical X:
check parallelism of stratified
log[ log S(t)] plots as t "
• Muenz460 cum. hazard ratio
plots
• Arjas29 cum. hazard plots
Proportional
hazards— • Check agreement of stratified
e↵ect of X does not depend
and modeled estimates
on T (e.g., treatment e↵ect • Hazard ratio plots
is constant over time)
• Smoothed Schoenfeld residual plots and correlation test
(time vs. residual)
• Test time-dependent covariable such as X ⇥ log(t + 1)
• Ratio of parametrically estimated (t)

Individual
X

Shape of (t|X) for fixed t
• k-level ordinal X : linear term
as X "
+ k 2 dummy variables
Linear:
• Continuous X: polynomials,
log (t|X) = log (t) + X
spline functions, smoothed
Nonlinear: log (t|X) =
martingale residual plots
log (t) + f (X)

Interaction
X1 and X2

Predictors

Between

Additive e↵ects: e↵ect of
X1 on log is independent
of X2 and vice versa

Shape of SKM (t)

Test nonadditive terms (e.g.,
products)

This method can be generalized to allow for simultaneous estimation of the
shape of the X e↵ect and X ⇥ t interaction using spline surfaces in (X, t)
instead of (X1 , X2 ) (Section 2.7.2).
Table 20.8 summarizes many facets of verifying assumptions for PH models. The trade-o↵s of the various methods for assessing proportional hazards
are given in Table 20.9.

13

14

510

20 Cox Proportional Hazards Regression Model

Table 20.9
Method

log[ log], Muenz,
Arjas plots
Dabrowska log ⇤ˆ
di↵erence plots
Stratified vs.
Modeled
Estimates
Hazard ratio plot
Schoenfeld residual
plot
Schoenfeld residual
correlation test
Fit time-dependent
covariables
Ratio of parametric
estimates of (t)

Requires Requires Computa- Yields
Yields
Requires Must Choose
Grouping Grouping
tional
Formal Estimate of Fitting 2 Smoothing
X
t
Efficiency
Test
Models
Parameter
2 (t)/ 1 (t)
x
x
x
x

x

x

x

x

x

x

?
x

x

x

x

x

x

x

?
x

x

x

x

x

x

x

x

20.6 What to Do When PH Fails
When a factor violates the PH assumption and a test of association is not
needed, the factor can be adjusted for through stratification as mentioned
earlier. This is especially attractive if the factor is categorical. For continuous
predictors, one may want to stratify into quantile groups. The continuous
version of the predictor can still be adjusted for as a covariable to account
for any residual linearity within strata.
When a test of significance is needed and the P -value is impressive, the
“principle of conservatism” could be invoked, as the P -value would likely
have been more impressive had the factor been modeled correctly. Predicted
survival probabilities using this approach will be erroneous in certain time
intervals.
An efficient test of association can be done using time-dependent covariables [437, pp. 208–217]. For example, in the model
(t|X) =

0 (t) exp( 1 X

+

2X

⇥ log(t + 1))

(20.27)

one tests H0 : 1 = 2 = 0 with 2 d.f. This is similar to the approach used
by [71]. Stratification on time intervals can also be used:27, 221, 261
(t|X) =

0 (t) exp( 1 X

+

2X

⇥ [t > c]).

(20.28)

If this step-function model holds, and if a sufficient number of subjects have
late follow-up, you can also fit a model for early outcomes and a separate
one for late outcomes using interval-specific censoring as discussed in Section 20.5.2. The dual model approach provides easy to interpret models, assuming that proportional hazards is satisfied within each interval.

20.8 Overly Influential Observations

511

Kronborg and Aaby360 and Dabrowska et al.140 provide tests for di↵erences
in ⇤(t) at specific t based on stratified PH models. These can also be used
to test for treatment e↵ects when PH is violated for treatment but not for
adjustment variables. Di↵erences in mean restricted life length (di↵erences
in areas under survival curves up to a fixed finite time) can also be useful for
comparing therapies when PH fails.328
Parametric models that assume an e↵ect other than PH, for example, the
log-logistic model,221 can be used to allow a predictor to have a constantly
increasing or decreasing e↵ect over time. If one predictor satisfies PH but
another does not, this approach will not work.

20.7 Collinearity
See Section 4.6 for the general approach using variance inflation factors.

20.8 Overly Influential Observations
Therneau et al.598 describe the use of score residuals for assessing influence in
Cox and related regression models. They show that the infinitesimal jackknife
estimate of the influence of observation i on equals V s0 , where V is the
estimated variance–covariance matrix of the p regression estimates b and s =
(si1 , si2 , . . . , sip ) is the vector of score residuals for the p regression coefficients
for the ith observation. Let Sn⇥p denote the matrix of score residuals over
all observations. Then an approximation to the unstandardized change in b
(DFBETA) is SV . Standardizing by the standard errors of b found from the
diagonals of V , e = (V11 , V22 , . . . , Vpp )1/2 , yields
DFBETAS = SV Diag(e)

1

,

(20.29)

where Diag(e) is a diagonal matrix containing the estimated standard errors.
As discussed in Section 20.12, identification of overly influential observations is facilitated by printing, for each predictor, the list of observations
containing DFBETAS > u for any parameter associated with that predictor.
The choice of cuto↵ u depends on the sample size among other things. A
typical choice might be u = 0.2 indicating a change in a regression coefficient
of 0.2 standard errors.

15

512

20 Cox Proportional Hazards Regression Model

20.9 Quantifying Predictive Ability
To obtain a unitless measure of predictive ability for a Cox PH model we
can use the R index described in Section 9.8.3, which is the square root of
the fraction of log likelihood explained by the model of the log likelihood
that could be explained by a perfect model, penalized for the complexity of
the model. The lowest (best) possible 2 log likelihood for the Cox model is
zero, which occurs when the predictors can perfectly rank order the survival
times. Therefore, as was the case with the logistic model, the quantity L⇤
from Section 9.8.3 is zero and an R index that is penalized for the number
of parameters in the model is given by
R2 = (LR

2p)/L0 ,

(20.30)

where p is the number of parameters estimated and L0 is the 2 log likelihood
when is restricted to be zero (i.e., there are no predictors in the model). R
will be near one for a perfectly predictive model and near zero for a model
that does not discriminate between short and long survival times. The R
index does not take into account any stratification factors. If stratification
factors are present, R will be near one if survival times can be perfectly
ranked within strata even though there is overlap between strata.
Schemper541 and Korn and Simon358 have reported that R2 is too sensitive to the distribution of censoring times and have suggested alternatives based on the distance between estimated Cox survival probabilities
(using predictors) and Kaplan–Meier estimates (ignoring predictors). Kent
and O’Quigley338 also report problems with R2 and suggest a more complex
2
measure. Schemper543 investigated the Maddala–Magee424, 425 index RLR
described in Section 9.8.3, applied to Cox regression:
2
RLR
=1

=1

16

exp( LR/n)
! 2/n ,

(20.31)

where ! is the null model likelihood divided by the fitted model likelihood.
2
For many situations, RLR
performed as well as Schemper’s more complex
541, 544
measure
and hence it is preferred because of its ease of calculation
(assuming that PH holds). Ironically, Schemper543 demonstrated that the n
in the formula for this index is the total number of observations, not the
number of events (but see O’Quigley, Xu, and Stare474 ). To make the R2
2
index have a maximum value of 1.0, we use the Nagelkerke464 RN
discussed
in Section 9.8.3.
An easily interpretable index of discrimination for survival models is derived from Kendall’s ⌧ and Somers’ Dxy rank correlation,575 the Gehan–
Wilcoxon statistic for comparing two samples for survival di↵erences, and
the Brown–Hollander–Korwar nonparametric test of association for censored
data.75, 167, 257, 263 This index, c, is a generalization of the area under the ROC

20.10 Validating the Fitted Model

513

curve discussed under the logistic model, in that it applies to a continuous
response variable that can be censored. The c index is the proportion of all
pairs of subjects whose survival time can be ordered such that the subject
with the higher predicted survival is the one who survived longer. Two subjects’ survival times cannot be ordered if both subjects are censored or if one
has failed and the follow-up time of the other is less than the failure time
of the first. The c index is a probability of concordance between predicted
and observed survival, with c = 0.5 for random predictions and c = 1 for a
perfectly discriminating model. The c index is relatively una↵ected by the
amount of censoring. Dxy is obtained from 2(c 0.5). While c (and Dxy )
is a good measure of pure discrimination ability of a single model, it is not
sensitive enough to allow multiple models to be compared440 .
Since high hazard means short survival time, when the linear predictor
X ˆ from a Cox model is compared with observed survival time, Dxy will be
negative.Some analysts may want to negate reported values of Dxy .

17

20.10 Validating the Fitted Model
Separate bootstrap or cross-validation assessments can be made for calibration and discrimination of Cox model survival and log relative hazard estimates.

20.10.1 Validation of Model Calibration
One approach to validation of the calibration of predictions is to obtain unbiased estimates of the di↵erence between Cox predicted and Kaplan–Meier
survival estimates at a fixed time u. Here is one sequence of steps.
1. Obtain cutpoints (e.g., deciles) of predicted survival at time u so as to have
a given number of subjects (e.g., 50) in each interval of predicted survival.
These cutpoints are based on the distribution of Ŝ(u|X) in the whole
sample for the “final” model (for data-splitting, instead use the model
developed in the training sample). Let k denote the number of intervals
used.
2. Compute the average Ŝ(u|X) in each interval.
3. Compare this with the Kaplan–Meier survival estimates at time u, stratified by intervals of Ŝ(u|X). Let the di↵erences be denoted by d =
(d1 , . . . , dk ).
4. Use bootstrapping or cross-validation to estimate the overoptimism in d
and then to correct d to get a more fair assessment of these di↵erences.
For each repetition, repeat any stepwise variable selection or stagewise
significance testing using the same stopping rules as were used to derive

18

514

20 Cox Proportional Hazards Regression Model

the “final” model. No more than B = 200 replications are needed to obtain
accurate estimates.
5. If desired, the bias-corrected d can be added to the original stratified
Kaplan–Meier estimates to obtain a bias-corrected calibration curve.
However, any statistical method that uses binning of continuous variables
(here, the predicted risk), is arbitrary and has lower precision than smooth
estimates that allow for interpolation. A far better approach to estimating
calibration curves for survival models is to use the flexible adaptive hazard
regression approach of Kooperberg et al.354 as discussed on P. 18.9. Their
method does not assume linearity or proportional hazards. Hazard regression can be used to estimate the relationship between (suitably transformed)
predicted survival probabilities and observed outcomes, i.e., to derive a calibration curve. The bootstrap is used to de-bias the estimates to correct for
overfitting, allowing estimation of the likely future calibration performance
of the fitted model.
As an example, consider a dataset of 20 random uniformly distributed
predictors for a sample of size 200. Let the failure time be another random
uniform variable that is independent of all the predictors, and censor half of
the failure times at random. Due to fitting 20 predictors to 100 events, there
will apparently be fair agreement between predicted and observed survival
over all strata (smooth black curve from hazard regression in Figure 20.11).
However, the bias-corrected calibration (blue curve from hazard regression)
gives a more truthful answer: examining the Xs across levels of predicted
survival demonstrate that predicted and observed survival are weekly related,
in more agreement with how the data were generated. For the more arbitrary
Kaplan-Meier approach, we divide the observations into quintiles of predicted
0.5-year survival, so that there are 40 observations per stratum.
n
200
p
20
set.seed (6)
xx
m a t r i x ( rnorm ( n ⇤ p ) , nrow=n , n c o l=p )
y
runif (n)
units (y)
” Year ”
e
c ( rep (0 , n / 2) , rep (1 , n / 2))
f
cph ( Surv ( y , e ) ⇠ xx , x=TRUE, y=TRUE,
t i m e . i n c=. 5 , s u r v=TRUE)
cal
c a l i b r a t e ( f , u=. 5 , B=200)
Using Cox survival estimates at 0.5 Years

p l o t ( c a l , y l i m=c ( . 4 , 1 ) , s u b t i t l e s=FALSE)
calkm
c a l i b r a t e ( f , u=. 5 , m=40 , cmethod= ’KM’ , B=200)
Using Cox survival estimates at 0.5 Years

p l o t ( calkm , add=TRUE)

# Figure 20.11

Fraction Surviving 0.5 Year

20.10 Validating the Fitted Model

515

1.0
0.9

●
●

0.8
0.7

●
●

0.6
0.5

●

0.4
0.50

0.55

0.60

0.65

0.70

0.75

0.80

0.85

Predicted 0.5 Year Survival
Fig. 20.11 Calibration of random predictions using Efron’s bootstrap with B = 200
resamples. Dataset has n = 200, 100 uncensored observations, 20 random predictors, model 220 = 19. The smooth black line is the apparent calibration estimated
by adaptive linear spline hazard regression354 , and the blue line is the bootstrap
bias– (overfitting–) corrected calibration curve estimated also by hazard regression.
The gray scale line is the line of identity representing perfect calibration. Black dots
represent apparent calibration accuracy obtained by stratifiying into intervals of predicted 0.5y survival containing 40 events per interval and plotting the mean predicted
value within the interval against the stratum’s Kaplan-Meier estimate. The blue ⇥
represent bootstrap bias-corrected Kaplan-Meier estimates.

20.10.2 Validation of Discrimination and Other
Statistical Indexes
Here bootstrapping and cross-validation are used as for logistic models (Section 10.9). We can obtain bootstrap bias-corrected estimates of c or equivalently Dxy . To instead obtain a measure of relative calibration or slope
shrinkage, we can bootstrap the apparent estimate of = 1 in the model
(t|X) = (t) exp( Xb).

(20.32)

Besides being a measure of calibration in itself, the bootstrap estimate of
also leads to an unreliability index U which measures how far the model maximum log likelihood (which allows for an overall slope correction) is from the
log likelihood evaluated at “frozen” regression coefficients ( = 1) (see [262]
and Section 10.9).

516

20 Cox Proportional Hazards Regression Model

Table 20.10

Index Original Training Test Optimism Corrected n
Sample Sample Sample
Index
Dxy
0.213
0.335 0.147
0.188
0.025 200
R2
0.092
0.191 0.042
0.150
0.058 200
Slope
1.000
1.000 0.389
0.611
0.389 200
D
0.021
0.048 0.009
0.039
0.019 200
U
0.002
0.002 0.028
0.031
0.028 200
Q
0.023
0.050 0.020
0.070
0.047 200
g
0.516
0.878 0.339
0.539
0.023 200

U=

LR(ˆ Xb) LR(Xb)
,
L0

(20.33)

where L0 is the 2 log likelihood for the null model (Section 9.8.3). Similarly,
a discrimination index D262 can be derived from the 2 log likelihood at the
shrunken linear predictor, penalized for estimating one parameter ( ) (see
also [625, p. 1318] and [120]):
D=

LR(ˆ Xb)
L0

1

.

(20.34)

D is the same as R2 discussed above when p = 1 (indicating only one reestimated parameter, ), the penalized proportion of explainable log likelihood
that was explained by the model. Because of the remark of Schemper,541 all
of these indexes may unfortunately be functions of the censoring pattern.
An index of overall quality that penalizes discrimination for unreliability
is
LR(Xb) 1
Q=D U =
.
(20.35)
L0
Q is a normalized and penalized 2 log likelihood that is evaluated at the
uncorrected linear predictor.
For the random predictions used in Figure 20.11, the bootstrap estimates
with B = 200 resamples are found in Table 20.10.
l a t e x ( v a l i d a t e ( f , B=200) , d i g i t s =3 , f i l e = ’ ’ , c a p t i o n= ’ ’ ,
t a b l e . e n v=TRUE, l a b e l= ’ tab : cox val random ’ )

It can be seen that the apparent correlation (Dxy = 0.21) does not hold
up after correcting for overfitting (Dxy = 0.02). Also, the slope shrinkage
(0.39) indicates extreme overfitting.
See [625, Section 6] and [632] and Section 18.3.7 for still more useful methods for validating the Cox model.

20.11 Describing the Fitted Model

517

Fig. 20.12 A display of an interaction between treatment and extent of disease, and
between treatment and calendar year of start of treatment. Comparison of medical
and surgical average hazard ratios for patients treated in 1970, 1977, and 1984 according to coronary anatomy. Closed squares represent point estimates; bars represent
0.95 confidence limits of average hazard ratios.86 Reprinted by permission, American
Medical Association.

20.11 Describing the Fitted Model
As with logistic modeling, once a Cox PH model has been fitted and all
its assumptions verified, the final model needs to be presented and interpreted. The fastest way to describe the model is to interpret each e↵ect in
it. For each predictor the change in log hazard per desired units of change
in the predictor value may be computed, or the antilog of this quantity,
exp( j ⇥ change in Xj ), may be used to estimate the hazard ratio holding
all other factors constant. When Xj is a nonlinear factor, changes in predicted
X for sensible values of Xj such as quartiles can be used as described in
Section 10.10. Of course for nonmodeled stratification factors, this method is
of no help. Figure 20.12 depicts a way to display estimated surgical : medical
hazard ratios in the presence of a significant treatment by disease severity
interaction and a secular trend in the benefit of surgical therapy (treatment
by year of diagnosis interaction).
Often, the use of predicted survival probabilities may make the model
more interpretable. If the e↵ect of only one factor is being displayed and
that factor is polytomous or predictions are made for specific levels, survival
curves (with or without adjustment for other factors not shown) can be drawn
for each level of the predictor of interest, with follow-up time on the xaxis. Figure 20.2 demonstrated this for a factor which was a stratification
factor. Figure 20.13 extends this by displaying survival estimates stratified

518

20 Cox Proportional Hazards Regression Model

Fig. 20.13 Cox–Kalbfleisch–Prentice survival estimates stratifying on treatment and
adjusting for several predictors. Estimates are for patients with left main disease and
normal or impaired ventricular function.509 Reprinted by permission, Mosby, Inc. /
Harcourt Health Sciences.

by treatment but adjusted to various levels of two modeled factors, one of
which, year of diagnosis, interacted with treatment.
When a continuous predictor is of interest, it is usually more informative
to display that factor on the x-axis with estimated survival at one or more
time points on the y-axis. When the model contains only one predictor, even
if that predictor is represented by multiple terms such as a spline expansion,
one may simply plot that factor against the predicted survival. Figure 20.14
depicts the relationship between treadmill exercise score, which is a weighted
linear combination of several predictors in a Cox model, and the probability
of surviving five years.
When displaying the e↵ect of a single factor after adjusting for multiple
predictors which are not displayed, care only need be taken for the values
to which the predictors are adjusted (e.g., grand means). When instead the
desire is to display the e↵ect of multiple predictors simultaneously, an important continuous predictor can be displayed on the x-axis while separate
curves or graphs are made for levels of other factors. Figure 20.15, which
corresponds to the log ⇤ plots in Figure 20.5, displays the joint e↵ects of age
and sex on the three-year survival probability. Age is modeled with a cubic
spline function, and the model includes terms for an age ⇥ sex interaction.
p
P r e d i c t ( f . i a , age , sex , time =3)
ggplot (p)

Besides making graphs of survival probabilities estimated for given levels
of the predictors, nomograms have some utility in specifying a fitted Cox
model. A nomogram can be used to compute X ˆ, the estimated log hazard for
a subject with a set of predictor values X relative to the “standard” subject.
The central line in the nomogram will be on this linear scale unlike the logistic
model nomograms given in Section 10.10 which further transformed X ˆ into

20.11 Describing the Fitted Model

519

5−Year Survival Probability

1.0
0.9
0.8
0.7
0.6
0.5
−20

−15

−10

−5

0

5

10

15

Treadmill Score

Fig. 20.14 Cox model predictions with respect to a continuous variable. X-axis
shows the range of the treadmill score seen in clinical practice and Y -axis shows the
corresponding five-year survival probability predicted by the Cox regression model
for the 2842 study patients.433

3 Year Survival Probability

1.0
0.9
sex

0.8

Male
Female

0.7
0.6
0.5
20

40

60

80

Age
Fig. 20.15 Survival estimates for model stratified on sex, with interaction.

520

19

20 Cox Proportional Hazards Regression Model

[1 + exp( X ˆ)] 1 . Alternatively, the central line could be on the nonlinear
exp(X ˆ) hazard ratio scale or survival at fixed t.
A graph of the estimated underlying survival function Ŝ(t) as a function
of t can be coupled with the nomogram used to compute X ˆ. The survival
ˆ
for a specific subject, Ŝ(t|X) is obtained from Ŝ(t)exp(X ) . Alternatively, one
ˆ
could graph Ŝ(t)exp(X ) for various values of X ˆ (e.g., X ˆ = 2, 1, 0, 1, 2)
so that the desired survival curve could be read directly, at least to the nearest
tabulated X ˆ. For estimating survival at a fixed time, say two years, one only
need to provide the constant Ŝ(t). The nomogram could even be adapted to
ˆ
include a nonlinear scale Ŝ(2)exp(X ) to allow direct computation of two-year
survival.

20.12 R Functions
Harrell’s cpower, spower, and ciapower (in the Hmisc package) perform power
calculations for Cox tests in follow-up studies. cpower computes power for
a two-sample Cox (log-rank) test with random patient entry over a fixed
duration and a given length of minimum follow-up. The expected number of
events in each group is estimated by assuming exponential survival. cpower
uses a slight modification of the method of Schoenfeld553 (see [494]). Separate
specification of noncompliance in the active treatment arm and “drop-in”
from the control arm into the active arm are allowed, using the method
of Lachin and Foulkes.363 The ciapower function computes power of the
Cox interaction test in a 2 ⇥ 2 setup using the method of Peterson and
George.494 It does not take noncompliance into account. The spower function
simulates power for two-sample tests (the log-rank test by default) allowing
for very complex conditions such as continuously varying treatment e↵ect
and noncompliance probabilities.
The rms package cph function is a slight modification of the coxph function written by Terry Therneau (in his survival package to work in the
rms framework. cph computes MLEs of Cox and stratified Cox PH models, overall score and likelihood ratio 2 statistics for the model, martingale
residuals, the linear predictor (X ˆ centered to have mean 0), and collinearity diagnostics. Efron, Breslow, and exact partial likelihoods are supported
(although the exact likelihood is very computationally intensive if ties are
frequent). The function also fits the Andersen–Gill23 generalization of the
Cox PH model. This model allows for predictor values to change over time
in the form of step functions as well as allowing time-dependent stratification (subjects can jump to di↵erent hazard function shapes). The Andersen–
Gill formulation allows multiple events per subject and permits subjects to
move in and out of risk at any desired time points. The latter feature allows time zero to have a more general definition. (See Section 9.5 for meth-

20.12 R Functions

521

ods of adjusting the variance–covariance matrix of ˆ for dependence in the
events per subject.) The printing function corresponding to cph prints the
2
Nagelkerke index RN
described in Section 20.9, and has a latex option for
better output. cph works in conjunction with the generic functions such
as specs, predict, summary, anova, fastbw, which.influence, latex,
residuals, coef, nomogram, and Predict described in Section 20.12, the
same as the logistic regression function lrm does. For the purpose of plotting
predicted survival at a single time, Predict has an additional argument time
for plotting cph fits. It also has an argument loglog which if TRUE causes
instead log-log survival to be plotted on the y-axis. cph has all the arguments
described in Section 20.12 and some that are specific to it.
Similar to Survival.psm, there are Survival.cph, Quantile.cph, and
Mean.cph functions which create other R functions to evaluate survival probabilities and perform other calculations, based on a cph fit with surv=TRUE.
These functions, unlike all the others, allow polygon (linear interpolation)
estimation of survival probabilities, quantiles, and mean survival time as an
option. Quantile.cph is the only automatic way for obtaining survival quantiles with cph. Quantile estimates will be missing when the survival curve
does not extend long enough. Likewise, survival estimates will be missing for
t > maximum follow-up time, when the last event time is censored. Mean.cph
computes the mean survival time if the last failure time in each stratum is
uncensored. Otherwise, Mean.cph may be used to compute restricted mean
lifetime using a user-specified truncation point.327 Quantile and Mean are
especially useful with plot and nomogram. Survival is useful with nomogram.
The R program below demonstrates how several cph-related functions work
well with the nomogram function. Here predicted three-year survival probabilities and median survival time (when defined) are displayed against age and
sex from the previously simulated dataset. The fact that a nonlinear e↵ect
interacts with a stratified factor is taken into account.
surv
Survival ( f . i a )
surv.f
f u n c t i o n ( l p ) s u r v ( 3 , lp , s tr at um= ’ s e x=Female ’ )
surv.m
f u n c t i o n ( l p ) s u r v ( 3 , lp , s tr at um= ’ s e x=Male ’ )
quant
Quantile ( f . i a )
med.f
f u n c t i o n ( l p ) quant ( . 5 , lp , s t rat um= ’ s e x=Female ’ )
med.m
f u n c t i o n ( l p ) quant ( . 5 , lp , st rat um= ’ s e x=Male ’ )
at.surv
c ( . 0 1 , . 0 5 , s e q ( . 1 , . 9 , by=. 1 ) , . 9 5 , . 9 8 , . 9 9 , . 9 9 9 )
at.med
c ( 0 , . 5 , 1 , 1 . 5 , s e q ( 2 , 1 4 , by =2))
n
nomogram ( f . i a , f u n= l i s t ( surv.m , s u r v . f , med.m , m e d . f ) ,
f u n l a b e l=c ( ’ S ( 3 | Male ) ’ , ’ S ( 3 | Female ) ’ ,
’ Median ( Male ) ’ , ’ Median ( Female ) ’ ) ,
f u n . a t= l i s t ( c ( . 8 , . 9 , . 9 5 , . 9 8 , . 9 9 ) ,
c ( .1 , .3 , .5 , .7 , .8 , .9 , .95 , .98 ) ,
c (8 ,10 ,12) , c (1 ,2 ,4 ,8 ,12)))
p l o t ( n , c o l . g r i d=FALSE, lmgp=. 2 )
l a t e x ( f . i a , f i l e = ’ ’ , d i g i t s =3)

Prob{T

t | sex = i} = Si (t)e

X

, where

522

20 Cox Proportional Hazards Regression Model

Xˆ=
1.8
+0.0493age
+5.18⇥10

2.15⇥10
5

(age

6

(age

54.6)3+

30.3)3+

2.15⇥10

+[Female][ 0.0366age + 4.29⇥10
+6.74⇥10

5

(age

54.6)3+

5

2.82⇥10
5

(age

(age

2.32⇥10

7

(age

45.1)3+

69.6)3+

30.3)3+

(age

5

0.00011(age

45.1)3+

69.6)3+ ]

and [c] = 1 if subject is in group c, 0 otherwise; (x)+ = x if x > 0, 0 otherwise.
t SM ale (t) SF emale (t)
0
1.000
1.000
1
0.993
0.902
2
0.984
0.825
3
0.975
0.725
4
0.967
0.648
5
0.956
0.576
6
0.947
0.520
7
0.938
0.481
8
0.928
0.432
9
0.920
0.395
10
0.909
0.358
11
0.904
0.314
12
0.892
0.268
13
0.886
0.223
14
0.877
0.203
r c s p l i n e . p l o t ( l v e f , d . t i m e , e v e n t=cdeath , nk=3)

The corresponding smoothed martingale residual plot for LVEF in Figure 20.7
was created with
cox
cph ( Surv ( d . t i m e , c d e a t h ) ⇠ l v e f , i t e r . m a x =0)
res
r e s i d ( cox )
g ⇠ loess ( res ⇠ lvef )
p l o t ( g , c o v e r a g e=0. 9 5 , c o n f i d e n c e =7 , x l a b=”LVEF” ,
y l a b=” M a r t i n g a l e R e s i d u a l ” )
g
ols ( res ⇠ rcs ( lvef ,5))
p l o t ( g , l v e f=NA, add=T, l t y =2)
l i n e s ( l o w e s s ( l v e f , r e s , i t e r =0) , l t y =3)
l e g e n d ( . 3 , 1 . 1 5 , c ( ” l o e s s F i t and 0 . 9 5 C o n f i d e n c e Bars ” ,
” o l s S p l i n e F i t and 0 . 9 5 C o n f i d e n c e L i m i t s ” ,
” l o w e s s Smoother ” ) , l t y = 1 : 3 , bty=”n” )

Because we desired residuals with respect to the omitted predictor LVEF,
the parameter iter.max=0 had to be given to make cph stop the estimation
process at the starting parameter estimates (default of zero). The e↵ect of this
is to ignore the predictors when computing the residuals; that is, to compute

20.12 R Functions

523

0

Points

10

20

30

40

50

60

70

80

90

100

age (sex=Male)
10

age (sex=Female)

20

10 30

Total Points

0

Linear Predictor

−2

50

10
−1.5

30

20
−1

40

60
30

70

80

40

−0.5

50

50
0

60 70

90
60

90

100
70

0.5

80

90

1

100

1.5

2

S(3 | Male)
0.99

0.98

0.95

0.9

S(3 | Female)
0.95

0.9

0.8

0.7

0.5

0.3

0.1

Median (Male)
12 10

Median (Female)
12

8

4

2

1

Fig. 20.16 Nomogram from a fitted stratified Cox model that allowed for interaction
between age and sex, and nonlinearity in age. The axis for median survival time is
truncated on the left where the median is beyond the last follow-up time.

residuals from a flat line rather than the usual residuals from a fitted straight
line.
The residuals.cph function is a slight modification of Therneau’s residuals.coxph function to obtain martingale, Schoenfeld, score, deviance residuals,
or approximate DFBETA or DFBETAS. Since martingale residuals are always stored by cph (assuming there are covariables present), residuals.cph
merely has to pick them o↵ the fit object and reinsert rows that were deleted
due to missing values. For other residuals, you must have stored the design
matrix and Surv object with the fit by using ..., x=TRUE, y=TRUE. Storing
the design matrix with x=TRUE ensures that the same transformation parameters (e.g., knots) are used in evaluating the model as were used in fitting it.
To use residuals.cph you can use the abbreviation resid. See the help file
for residuals.cph for an example of how martingale residuals may be used
to quickly plot univariable (unadjusted) relationships for several predictors.
Figure 20.10, which used smoothed scaled Schoenfeld partial residuals552
to estimate the form of a predictor’s log hazard ratio over time, was made
with

524

20 Cox Proportional Hazards Regression Model

Srv
Surv ( dm.time , cdeathmi )
cox
cph ( Srv ⇠ pi , x=T, y=T)
c o x . z p h ( cox , ” rank ” )
# Test for PH for each column of X
res
r e s i d ( cox , ” s c a l e d s c h ” )
time
a s . n u m e r i c ( names ( r e s ) )
# Use dimnames(res)[[1]] if more than one predictor
f
l o e s s ( r e s ⇠ time , span=0 . 5 0 )
p l o t ( f , c o v e r a g e=0. 9 5 , c o n f i d e n c e =7 , x l a b=” t ” ,
y l a b=” S c a l e d S c h o e n f e l d R e s i d u a l ” , y l i m=c ( .1 , . 2 5 ) )
l i n e s ( supsmu ( time , r e s ) , l t y =2)
l e g e n d ( 1 . 1 , . 2 1 , c ( ” l o e s s Smoother with span=0 . 5 0 and 0 . 9 5 C . L . ” ,
” Super Smoother ” ) , l t y = 1 : 2 , bty=”n” )

The computation and plotting of scaled Schoenfeld residuals could have been
done automatically in this case by using the single command plot(cox.zph(cox)),
although cox.zph defaults to plotting against the Kaplan–Meier transformation of follow-up time.
The hazard.ratio.plot function in rms repeatedly estimates Cox regression coefficients and confidence limits within time intervals. The log hazard
ratios are plotted against the mean failure/censoring time within the interval.
Figure 20.9 was created with
h a z a r d . r a t i o . p l o t ( pi , S )

20

# S was Surv(dm.time, ...)

If you have multiple degree of freedom factors, you may want to score them
into linear predictors before using hazard.ratio.plot. The predict function
with argument type="terms" will produce a matrix with one column per
factor to do this (Section 20.12).
Therneau’s cox.zph function implements Harrell’s Schoenfeld residual correlation test for PH. This function also stores results that can easily be passed
to a plotting method for cox.zph to automatically plot smoothed residuals
that estimate the e↵ect of each predictor over time.
Therneau has also written an R function survdiff that compares two or
more survival curves using the G ⇢ family of rank tests (Harrington and
Fleming268 ).
The rcorr.cens function in the Hmisc library computes the c index and the
corresponding generalization of Somers’ Dxy rank correlation for a censored
response variable. rcorr.cens also works for uncensored and binary responses
(see ROC area in Section 10.8), although its use of all possible pairings makes
it slow for this purpose. The survival package’s survConcordance has an
extremely fast algorithm for the c index and a fairly accurate estimator of its
standard error.
The calibrate function for cph constructs a bootstrap or cross-validation
optimism-corrected calibration curve for a single time point by resampling
the di↵erences between average Cox predicted survival and Kaplan–Meier estimates (see Section 20.10.1). But more precise is calibrate’s default method
based on adaptive semiparametric regression discussed in the same section.
Figure 20.11 is an example.

20.13 Further Reading

525

The validate function for cph fits validates several statistics describing
2
Cox model fits—slope shrinkage, RN
, D, U, Q, and Dxy . The val.surv function can also be of use in externally validating a Cox model using the methods
presented in Section 18.3.7.

20.13 Further Reading
1

2
3

4

5
6
7
8
9

10
11

12
13
14

15

Good general texts for the Cox PH model include Cox and Oakes,130 Kalbfleisch
and Prentice,324 Lawless,374 Collett,111 Marubini and Valsecchi,437 and Klein
and Moeschberger.343 Therneau and Grambsch597 describe the many ways the
standard Cox model may be extended.
Cupples et al.138 and Marubini and Valsecchi [437, pp. 201–206] present good
description of various methods of computing “adjusted survival curves.”
See Altman and Andersen15 for simpler approximate formulas. Cheng et al.100
derived methods for obtaining pointwise and simultaneous confidence bands for
S(t) for future subjects, and Henderson278 has a comprehensive discussion of
the use of Cox models to estimate survival time for individual subjects.
Aalen2 and Valsecchi et al.618 discuss other residuals useful in graphically checking survival model assumptions. León and Tsai393 derived residuals for estimating covariate transformations that are di↵erent from martingale residuals.
[404] has other methods for generating confidence intervals for martingale residual plots.
Lin et al.404 describe other methods of checking transformations using cumulative martingale residuals.
A parametric analysis of the VA dataset using linear splines and incorporating
X ⇥ t interactions is found in [354].
Winnett and Sasieni665 show how to use scaled Schoenfeld residuals in an iterative fashion to actually model e↵ects that are not in proportional hazards.
See [228, 496] for some methods for obtaining confidence bands for Schoenfeld residual plots. Winnett and Sasieni664 discuss conditions in which the
Grambsch–Therneau scaling of the Schoenfeld residuals does not perform adequately for estimating (t).
[468,512] compared the power of the test for PH based on the correlation between
failure time and Schoenfeld residuals with the power of several other tests.
See Lin et al.404 for another approach to deriving a formal test of PH using
residuals. Other graphical methods for examining the PH assumption are due
to Gray,231 who used hazard smoothing to estimate hazard ratios as a function
of time, and Thaler,595 who developed a nonparametric estimator of the hazard
ratio over time for time-dependent covariables. See Valsecchi et al.618 for other
useful graphical assessments of PH.
A related test of constancy of hazard ratios may be found in [512]. Also, see
Schemper542 for related methods.
See [542] for a variation of the standard Cox likelihood to allow for non-PH.
An excellent review of graphical methods for assessing PH may be found in
Hess.284 . Sahoo and Sengupta532 provide some new graphical methods for assessing PH irrespective of satisfaction of the other model assumptions.
Schemper542 provides a way to determine the e↵ect of falsely assuming PH by
comparing the Cox regression coefficient with a well-described average log hazard ratio. Zucker688 shows how dependent a weighted log-rank test is on the true
hazard ratio function, when the weights are derived from a hypothesized hazard
ratio function. Valsecchi et al.618 proposed a method that is robust to non-PH

526

16

17

18
19
20

20 Cox Proportional Hazards Regression Model
that occurs in the late follow-up period. Their method uses down-weighting of
certain types of “outliers.” See Herndon and Harrell281 for a flexible parametric PH model with time-dependent covariables, which uses the restricted cubic
spline function to specify (t). Putter et al.511 and Muggeo and Tagliavia461
have nice approaches that use time-dependent covariates to model time interactions to allow non-proportional hazards. Perperoglou et al.491, 492 developed
a systematic approach that allows one to continuously vary the amount of non
PH allowed, through the use of a structure matrix that connects predictors
with functions of time. Schuabel et al.538 have a good exposition of internal
time-dependent covariates.
See van Houwelingen and le Cessie [625, Eq. 61] and Verweij and van Houwelingen632 for an interesting index of cross-validated predictive accuracy. Schemper
and Henderson547 relate explained variation to predictive accuracy in Cox models. Hielscher et al.285 compares and illustrates several measures of explained
variation as does Choodari-Oskooei et al.103 . Choodari-Oskooei et al.102 studied explained randomness and predictive accuracy measures.
See similar indexes in Schemper539 and a related idea in [625, Eq. 63]. Mandel, Galai, and Simchen429 presented a time-varying c index. See Korn and
Simon,358 Schemper and Stare,549 and Henderson278 for nice comparisons of
various measures. Pencina and D’Agostino482 provide more details about the c
index and derived new interval estimates. They also discussed the relationship
between c and a version of Kendall’s ⌧ . Pencina et al.484 found advantages of c.
Uno et al.611 described exactly how c depends on the amount of censoring and
proposed a new index, requiring one to choose a time cuto↵, that is invariant to
the amount of censoring. Henderson et al.279 discussed the benefits of using the
probability of a serious prognostication error (e.g., being o↵ by a factor of 2.0
or worse on the time scale) as an accuracy measure. Schemper545 shows that
models with very important predictors can have very low absolute prediction
ability, and he discusses measures of predictive accuracy from a general standpoint. Lawless and Yuan378 present prediction error estimators and confidence
limits, focusing on such measures as error in predicted median or mean survival
time. Schmid and Potapov550 studied the bias of several variations on the c
index under non-proportional hazards and/or nonrandom censoring.
Altman and Royston18 have a good discussion of validation of prognostic models
and present several examples of validation using a simple discrimination index.
Kattan et al.331 describe how to make nomograms for deriving predicted survival probabilities when there are competing risks.
Hielscher et al.285 provides an overview of software for computing accuracy
indexes with censored data.

Chapter 21

Case Study in Cox Regression

21.1 Choosing the Number of Parameters and Fitting
the Model
Consider the randomized trial of estrogen for treatment of prostate cancer85
described in Chapter 8. Let us now develop a model for time until death
(of any cause). There are 354 deaths among the 502 patients. To be able
to efficiently estimate treatment benefit, to test for di↵erential treatment
e↵ect, or to estimate prognosis or absolute treatment benefit for individual
patients, we need a multivariable survival model. In this case study we do
not make use of data reductions obtained in Chapter 8 but show simpler
(partial) approaches to data reduction. We do use the transcan results for
imputation.
First let’s assess the wisdom of fitting a full additive model that does not
assume linearity of e↵ect for any predictor. Categorical predictors are expanded using dummy variables. For pf we could lump the last two categories
as before since the last category has only two patients. Likewise, we could
combine the last two levels of ekg. Continuous predictors are expanded by
fitting four-knot restricted cubic spline functions, which contain two nonlinear terms and thus have a total of three d.f. Table 21.1 defines the candidate
predictors and lists their d.f. The variable stage is not listed as it can be predicted with high accuracy from sz,sg,ap,bm (stage could have been used as
a predictor for imputing missing values on sz, sg). There are a total of 36
candidate d.f. that should not be artificially reduced by “univariable screening” or graphical assessments of association with death. This is about 1/10
as many predictor d.f. as there are deaths, so there is some hope that a fitted
model may validate. Let us also examine this issue by estimating the amount
of shrinkage using Equation 4.1. We first use transcan impute missing data.
r e q u i r e ( rms )
getHdata ( p r o s t a t e )
l e v e l s ( p r o s t a t e $ ekg ) [ l e v e l s ( p r o s t a t e $ ekg ) %i n%

527

528

21 Case Study in Cox Regression

Table 21.1

Predictor

Name d.f. Original Levels

Dose of estrogen
rx
Age in years
age
Weight index: wt(kg) ht(cm)+200 wt
Performance rating
pf
History of cardiovascular disease
Systolic blood pressure/10
Diastolic blood pressure/10
Electrocardiogram code

hx
sbp
dbp
ekg

Serum hemoglobin (g/100ml)
Tumor size (cm2 )
Stage/histologic grade combination
Serum prostatic acid phosphatase
Bone metastasis

hg
sz
sg
ap
bm

3 placebo, 0.2, 1.0, 5.0 mg estrogen
3
3
2 normal, in bed < 50% of time,
in bed > 50%, in bed always
1 present/absent
3
3
5 normal, benign, rhythm disturb.,
block, strain, old myocardial
infarction, new MI
3
3
3
3
1 present/absent

c ( ’ o l d MI ’ , ’ r e c e n t MI ’ ) ]
’MI ’
# combines last 2 levels and uses a new name, MI
prostate $ pf.coded
a s . i n t e g e r ( prostate $ pf )
# save original pf, re-code to 1-4
l e v e l s ( prostate $ pf )
c ( l e v e l s ( prostate $ pf ) [ 1 : 3 ] ,
l e v e l s ( prostate $ pf ) [ 3 ] )
# combine last 2 levels
w

t r a n s c a n (⇠ s z + s g + ap + sbp + dbp + age +
wt + hg + ekg + p f + bm + hx ,
imputed=TRUE, data=p r o s t a t e , p l=FALSE, pr=FALSE)

attach ( prostate )
sz
impute (w,
sg
impute (w,
age
impute (w,
wt
impute (w,
ekg
impute (w,
dd

sz ,
sg ,
age
wt ,
ekg

data=p r o s t a t e )
data=p r o s t a t e )
, data=p r o s t a t e )
data=p r o s t a t e )
, data=p r o s t a t e )

d a t a d i s t ( p r o s t a t e ) ; o p t i o n s ( d a t a d i s t= ’ dd ’ )

u n i t s ( dtime )
’ Month ’
S
Surv ( dtime , s t a t u s != ’ a l i v e ’ )
f

cph ( S ⇠ r x + r c s ( age , 4 ) + r c s ( wt , 4 ) + p f + hx +

21.1 Choosing Number of Parameters/Fitting the Model

529

r c s ( sbp , 4 ) + r c s ( dbp , 4 ) + ekg + r c s ( hg , 4 ) +
r c s ( sg , 4 ) + r c s ( sz , 4 ) + r c s ( l o g ( ap ) , 4 ) + bm)
p r i n t ( f , l a t e x=TRUE, c o e f s=FALSE)

Cox Proportional Hazards Model
cph(formula = S ˜ rx + rcs(age, 4) + rcs(wt, 4) + pf + hx + rcs(sbp,
4) + rcs(dbp, 4) + ekg + rcs(hg, 4) + rcs(sg, 4) + rcs(sz,
4) + rcs(log(ap), 4) + bm)
Model Tests

Discrimination
Indexes
R2
0.238
Dxy
0.333
g
0.787
gr
2.196

Obs
502 LR 2
136.22
Events
354 d.f.
36
Center -2.9933 Pr(> 2 ) 0.0000
Score 2 143.62
Pr(> 2 ) 0.0000
2
The likelihood ratio
statistic is 135.4 with 36 d.f. This test is highly
significant so some modeling is warranted. The AIC value (on the 2 scale) is
135.4 2⇥36 = 63.4. The rough shrinkage estimate is 0.73 (99.4/135.4) so we
estimate that 0.27 of the model fitting will be noise, especially with regard to
calibration accuracy. The approach of Spiegelhalter578 is to fit this full model
and to shrink predicted values. We instead try to do data reduction (blinded
to individual 2 statistics from the above model fit) to see if a reliable model
can be obtained without shrinkage. A good approach at this point might
be to do a variable clustering analysis followed by single degree of freedom
scoring for individual predictors or for clusters of predictors. Instead we do
an informal data reduction. The strategy is described in Table 21.2. For ap,
more exploration is desired to be able to model the shape of e↵ect with such a
highly skewed distribution. Since we expect the tumor variables to be strong
prognostic factors we retain them as separate variables. No assumption is
made for the dose-response shape for estrogen, as there is reason to expect a
non-monotonic e↵ect due to competing risks for cardiovascular death.
heart
hx + ekg %n i n% c ( ’ normal ’ , ’ b e n i g n ’ )
l a b e l ( heart )
’ Heart D i s e a s e Code ’
map
( 2 ⇤dbp + sbp ) / 3
l a b e l (map)
’ Mean A r t e r i a l P r e s s u r e / 10 ’
dd
d a t a d i s t ( dd , h e a r t , map)
f

cph ( S ⇠ r x + r c s ( age , 4 ) + r c s ( wt , 3 ) + p f . c o d e d +
h e a r t + r c s (map , 3 ) + r c s ( hg , 4 ) +
r c s ( sg , 3 ) + r c s ( sz , 3 ) + r c s ( l o g ( ap ) , 6 ) + bm,
x=TRUE, y=TRUE, s u r v=TRUE, t i m e . i n c =5⇤ 1 2 )

p r i n t ( f , l a t e x=TRUE, c o e f s =3)

530

21 Case Study in Cox Regression

Table 21.2

Variables

Reductions
d.f. Saved
Assume variable not important enough
1
for 4 knots; use 3 knots
pf
Assume linearity
1
hx,ekg Make new 0,1,2 variable and assume
5
linearity: 2 = hx and ekg not normal
or benign, 1 = either, 0 = none
sbp,dbp Combine into mean arterial bp and
4
use 3 knots: map = (2 dbp + sbp)/3
sg
Use 3 knots
1
sz
Use 3 knots
1
ap
Look at shape of e↵ect of ap in detail,
2
and take log before expanding as spline
to achieve numerical stability: add 2 knots
wt

Cox Proportional Hazards Model
cph(formula = S ˜ rx + rcs(age, 4) + rcs(wt, 3) + pf.coded +
heart + rcs(map, 3) + rcs(hg, 4) + rcs(sg, 3) + rcs(sz, 3) +
rcs(log(ap), 6) + bm, x = TRUE, y = TRUE, surv = TRUE, time.inc = 5 *
12)
Model Tests
Obs
502 LR 2
115.29
Events 354 d.f.
25
Center -2.949 Pr(> 2 ) 0.0000
Score 2 134.18
Pr(> 2 ) 0.0000

Discrimination
Indexes
R2
0.205
Dxy
0.342
g
0.794
gr
2.212

Coef
S.E. Wald Z Pr(> |Z|)
rx=0.2 mg estrogen -0.0255 0.1536
-0.17
0.8682
rx=1.0 mg estrogen -0.3401 0.1645
-2.07
0.0387
rx=5.0 mg estrogen -0.0733 0.1579
-0.46
0.6426
...
# x, y for predict, validate, calibrate;
# surv, time.inc for calibrate
l a t e x ( anova ( f ) , f i l e = ’ ’ , l a b e l= ’ tab : coxcase

anova1 ’ ) # Table 21.4

The total savings is thus 11 d.f. The likelihood ratio 2 is 115 with 25 d.f.,
with a slightly improved AIC of 65. The rough shrinkage estimate is slightly
better at 0.78, but still worrisome. A further data reduction could be done,
such as using the transcan transformations determined from self-consistency
of predictors, but we stop here and use this model.
From Table 21.4 there are 12 parameters associated with nonlinear e↵ects,
and the overall test of linearity indicates the strong presence of nonlinearity
for at least one of the variables age,wt,map,hg,sz,sg,ap. There is no strong
evidence for a di↵erence in survival time between doses of estrogen.

21.2 Checking Proportional Hazards
Now that we have a tentative model, let us examine the model’s distributional

21.2 Checking Proportional Hazards

531

Table 21.4 Wald Statistics for S
2

rx
age
Nonlinear
wt
Nonlinear
pf.coded
heart
map
Nonlinear
hg
Nonlinear
sg
Nonlinear
sz
Nonlinear
ap
Nonlinear
bm
TOTAL NONLINEAR
TOTAL

d.f.
P
5.27 3 0.1533
24.98 3 < 0.0001
15.53 2 0.0004
10.05 2 0.0066
3.02 1 0.0821
12.14 1 0.0005
21.92 1 < 0.0001
0.16 2 0.9217
0.08 1 0.7792
20.58 3 0.0001
9.70 2 0.0078
2.18 2 0.3368
0.01 1 0.9159
23.44 2 < 0.0001
2.64 1 0.1044
16.68 5 0.0051
16.50 4 0.0024
0.03 1 0.8664
48.27 12 < 0.0001
189.92 25 < 0.0001

to the log hazard (at least if the shape of the e↵ect does not change with
time). In doing this we are temporarily ignoring the fact that the individual
regression coefficients were estimated from the data. For dose of estrogen, for
example, we code the e↵ect as 0 (placebo), 0.024 (0.2 mg), 0.337 (1.0 mg),
and 0.072 (5.0 mg), and age is transformed using its fitted spline function.
In the rms package the predict function easily summarizes multiple terms
and produces a matrix (here, z) containing the total e↵ects for each predictor.
Matrix factors can easily be included in model formulas.
z
p r e d i c t ( f , t y p e= ’ terms ’ )
# required x=T above to store design matrix
f.short
cph ( S ⇠ z , x=TRUE, y=TRUE)
# store raw x, y so can get residuals

The fit f.short based on the matrix of single d.f. predictors z has the
same LR 2 of 115 as the fit f, but with a falsely low 11 d.f. All regression
coefficients are unity.
Now we compute scaled Schoenfeld residuals separately for each predictor
and test the PH assumption using the “correlation with time” test. Also plot
smoothed trends in the residuals. The plot method for cox.zph objects uses
cubic splines to smooth the relationship.

532
phtest
phtest
rx
age
wt
pf.coded
heart
map
hg
sg
sz
ap
bm
GLOBAL

21 Case Study in Cox Regression
c o x . z p h ( f . s h o r t , t r a n s f o r m= ’ i d e n t i t y ’ )

rho
0.09804
-0.03930
0.03099
-0.03813
0.01957
-0.06220
-0.01804
-0.03732
-0.00536
-0.00369
0.04334
NA

chisq
3.63577
0.52882
0.33737
0.52962
0.16518
1.14135
0.11366
0.47522
0.00959
0.00492
0.72845
7.01395

p
0.0566
0.4671
0.5614
0.4668
0.6844
0.2854
0.7360
0.4906
0.9220
0.9441
0.3934
0.7980

p l o t ( p h t e s t , v a r= ’ r x ’ )

10

Beta(t) for rx

5
0

# Figure 21.1

●
●
●●
●
●● ●●●●
●
●
●
●
●
●
●
● ●
● ●
●
●
● ●●
●
●
●●
●
●
●
●
●
●●●
●●● ●●
●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●●
●
●
●
●
●
● ●●●●●●● ●●●●● ●●●●●
●●●
●●
●
●●●●●● ●●● ●● ● ●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●●●●●●
●●●
●●
●● ●●●●
●
● ●
●●●
●●● ●
● ● ●●
●●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●● ●●●● ●●
●●● ●●●
●
●● ●● ●●
●●
●● ●●
●●●
●●●
●
●●
●●
●

−5
−10
−15
−20

●
●●
●
●
● ●●●
●●
●
●
● ●
●
● ●
● ●●
●
●
● ● ●●●
●●
●●● ●● ●
●●●●● ●● ●●
● ●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●

●
●

●

0

20

40

60

Time
Fig. 21.1 Raw and spline-smoothed scaled Schoenfeld residuals for dose of estrogen,
nonlinearly coded from the Cox model fit, with ± 2 standard errors.

Perhaps only the drug e↵ect significantly changes over time (P = 0.05 for
testing the correlation rho between the scaled Schoenfeld residual and time),
but when a global test of PH is done penalizing for 11 d.f., the P value is
0.81. A graphical examination of the trends doesn’t find anything interesting
for the last 10 variables. A residual plot is drawn for rx alone and is shown in
Figure 21.1. We ignore the possible increase in e↵ect of estrogen over time. If
this non-PH is real, a more accurate model might be obtained by stratifying
on rx or by using a time ⇥ rx interaction as a time-dependent covariable.

21.4 Describing Predictor E↵ects

533

Table 21.5 Wald Statistics for S
2

z.dose (Factor+Higher Order Factors)
17.36
All Interactions
11.00
z.other (Factor+Higher Order Factors)
132.68
All Interactions
11.00
z.dose ⇥ z.other (Factor+Higher Order Factors) 11.00
TOTAL
135.61

d.f.
P
11 0.0975
10 0.3575
20 < 0.0001
10 0.3575
10 0.3575
21 < 0.0001

21.3 Testing Interactions
Note that the model has several insignificant predictors. These are not
deleted, as that would not improve predictive accuracy and it would make
accurate confidence intervals hard to obtain. At this point it would be reasonable to test prespecified interactions. Here we test all interactions with
dose. Since the multiple terms for many of the predictors (and for rx) make
for a great number of d.f. for testing interaction (and a loss of power), we do
approximate tests on the data-driven coding of predictors. P -values for these
tests are likely to be somewhat anti-conservative.
z.dose
z [ , ” r x ” ] # same as saying z[,1] - get first column
z.other
z [ , 1]
# all but the first column of z
f.ia
cph ( S ⇠ z . d o s e ⇤ z . o t h e r )
# Figure 21.5:
l a t e x ( anova ( f . i a ) , f i l e = ’ ’ , l a b e l= ’ tab : coxcase anova2 ’ )

The global test of additivity in Table 21.5 has P = 0.39, so we ignore the
interactions (and also forget to penalize for having looked for them below!).

21.4 Describing Predictor E↵ects
Let us plot how each predictor is related to the log hazard of death, including
0.95 confidence bands. Note in Figure 21.2 that due to a peculiarity of the
Cox model the standard error of the predicted X ˆ is zero at the reference
values (medians here, for continuous predictors).
g g p l o t ( P r e d i c t ( f ) , s e p d i s c r e t e= ’ v e r t i c a l ’ , n l e v e l s =4 ,
vnames= ’ names ’ ) # Figure 21.2

534

21 Case Study in Cox Regression

age

ap

hg

1
0

log Relative Hazard

−1
60

70
map

80

0

50

100

9

11

13
sz

sg

15

17

1
0
−1
8

10
wt

12

7.5

10.0

12.5

15.0 0

10

20

30

40

1
0
−1
80

90 100 110 120 130
bm

1

●

0

●

heart
2
1
0

●
●
●

pf.coded
4
3
2
1

rx
placebo
5.0 mg estrogen
1.0 mg estrogen
0.2 mg estrogen

●
●
●
●

−1

0

1

●
●
●
●

−1

0

log Relative Hazard
Fig. 21.2 Shape of each predictor on log hazard of death. Y -axis shows X ˆ, but
the predictors not plotted are set to reference values. Note the highly non-monotonic
relationship with ap, and the increased slope after age 70 which occurs in outcome
models for various diseases.

1

50

21.5 Validating the Model

535

21.5 Validating the Model
We first validate this model for Somers’ Dxy rank correlation between predicted log hazard and observed survival time, and for slope shrinkage. The
bootstrap is used (with 200 resamples) to penalize for possible overfitting, as
discussed in Section 5.3.
s e t . s e e d ( 1 ) # so can reproduce results
v
v a l i d a t e ( f , B=200 , dxy=TRUE)

Divergence or singularity in 195 samples
latex (v ,

f i l e=’ ’ )

Index Original Training Test Optimism Corrected
Sample Sample Sample
Index
Dxy
0.3419 0.3334 0.3007
0.0328
0.3092
R2
0.2053 0.2348 0.1818
0.0530
0.1523
Slope 1.0000 1.0000 0.8347
0.1653
0.8347
D
0.0284 0.0332 0.0248
0.0084
0.0201
U
0.0005 0.0005 0.0015
0.0020
0.0015
Q
0.0289 0.0337 0.0233
0.0104
0.0186
g
0.7938 0.7809 0.6467
0.1342
0.6596

n
5
5
5
5
5
5
5

Here “training” refers to accuracy when evaluated on the bootstrap sample
used to fit the model, and “test” refers to the accuracy when this model is applied without modification to the original sample. The apparent Dxy is 0.34,
but a better estimate of how well the model will discriminate prognoses in
the future is Dxy = 0.31. The bootstrap estimate of slope shrinkage is 0.84,
somewhat close to the simple heuristic estimate. The shrinkage coefficient
could easily be used to shrink predictions to yield better calibration.
Finally, we validate the model (without using the shrinkage coefficient) for
calibration accuracy in predicting the probability of surviving five years. The
bootstrap is used to estimate the optimism in how well predicted five-year
survival from the final Cox model tracks flexible smooth estimates, without any binning of predicted survival probabilities or assuming proportional
hazards.
cal

c a l i b r a t e ( f , B=200 , u=5⇤ 1 2 , maxdim=4)

Using Cox survival estimates at 60 Months

p l o t ( c a l , s u b t i t l e s=FALSE)

# Figure 21.3

The estimated calibration curves are shown in Figure 21.3, similar to what
was done in Figure 19.11. Bootstrap calibration demonstrates some overfitting, consistent with regression to the mean. The absolute error is appreciable
for 5-year survival predicted to be very low or high.

21 Case Study in Cox Regression

Fraction Surviving 60 Month

536

0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Predicted 60 Month Survival
Fig. 21.3 Bootstrap estimate of calibration accuracy for 5-year estimates from the
final Cox model, using adaptive linear spline hazard regression354 . The line nearer the
ideal line corresponds to apparent predictive accuracy. The blue curve corresponds
to bootstrap-corrected estimates.

21.6 Presenting the Model
To present point and interval estimates of predictor e↵ects we draw a hazard
ratio chart (Figure 21.4), and to make a final presentation of the model
we draw a nomogram having multiple “predicted value” axes. Since the ap
relationship is so non-monotonic, use a 20 : 1 hazard ratio for this variable.
p l o t ( summary ( f , ap=c ( 1 , 2 0 ) ) , l o g=TRUE, main= ’ ’ )

# Figure 21.4

The ultimate graphical display for this model will be a nomogram relating
the predictors to X ˆ, estimated three– and five-year survival probabilities
and median survival time. It is easy to add as many “output” axes as desired
to a nomogram.
surv
surv3
surv5
quan
med
ss
nom

Survival ( f )
f u n c t i o n ( x ) s u r v ( 3 ⇤ 1 2 , l p=x )
f u n c t i o n ( x ) s u r v ( 5 ⇤ 1 2 , l p=x )
Quantile ( f )
f u n c t i o n ( x ) quan ( l p=x ) / 12
c ( .05 , .1 , .2 , .3 , .4 , .5 , .6 , .7 , .8 , .9 , .95 )
nomogram ( f , ap=c ( . 1 , . 5 , 1 , 2 , 3 , 4 , 5 , 1 0 , 2 0 , 3 0 , 4 0 ) ,
f u n= l i s t ( surv3 , surv5 , med ) ,
f u n l a b e l=c ( ’ 3 year S u r v i v a l ’ , ’ 5 year S u r v i v a l ’ ,
’ Median S u r v i v a l Time ( y e a r s ) ’ ) ,

21.7 Problems

537

0.50

1.00

2.00

4.00

7.00

age − 76:70
wt − 107:90
pf.coded − 4:1
heart − 2:0
map − 11:9.333333
hg − 14.69922:12.29883
sg − 11:9
sz − 21:5
ap − 20:1
bm − 1:0
rx − 0.2 mg estrogen:placebo
rx − 1.0 mg estrogen:placebo
rx − 5.0 mg estrogen:placebo

Fig. 21.4 Hazard ratios and multi-level confidence bars for e↵ects of predictors in
model, using default ranges except for ap

f u n . a t= l i s t ( s s , s s , c ( . 5 , 1 : 6 ) ) )
p l o t (nom , x f r a c=. 6 5 , lmgp=. 3 5 )
# Figure 21.5

21.7 Problems
Perform Cox regression analyses of survival time using the Mayo Clinic PBC
dataset described in Section 8.9. Provide model descriptions, parameter estimates, and conclusions.
1. Assess the nature of the association of several predictors of your choice.
For polytomous predictors, perform a log-rank-type score test (or k-sample
ANOVA extension if there are more than two levels). For continuous predictors, plot a smooth curve that estimates the relationship between the
predictor and the log hazard or log–log survival. Use both parametric
and nonparametric (using martingale residuals) approaches. Make a test
of H0 : predictor is not associated with outcome versus Ha : predictor
is associated (by a smooth function). The test should have more than 1
d.f. If there is no evidence that the predictor is associated with outcome.
Make a formal test of linearity of each remaining continuous predictor.
Use restricted cubic spline functions with four knots. If you feel that you
can’t narrow down the number of candidate predictors without examining

538

21 Case Study in Cox Regression
0

10

20

30

40

50

60

70

80

90

100

Points
5.0 mg estrogen

rx
1.0 mg estrogen
75

80

85

90

Age in Years
70
120

45

Weight Index = wt(kg)−ht(cm)+200
110

90

80

2

70

60

4

pf.coded
1

3

1

Heart Disease Code
0
22

2

16

Mean Arterial Pressure/10
16

4

18

20

22

Serum Hemoglobin (g/100ml)
14

Combined Index of Stage and Hist.
Grade

12

10

5 7 9 11 13 15
10 20 25 30 35

8

40

45

50

6

55

60

65

4

70

Size of Primary Tumor (cm^2)
5

3

4 5

10

Serum Prostatic Acid Phosphatase
2
1

40

20

0.1

Bone Metastases
0

Total Points
0

20

40

60

80

100 120 140 160 180 200 220 240 260

Linear Predictor
−1.5

−1

−0.5

0

0.5

1

1.5

2

3−year Survival
0.8

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.05

5−year Survival
0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.05

Median Survival Time (years)
6 5

4

3

2

Fig. 21.5 Nomogram for predicting death in prostate cancer trial

1

0.5

2.5

3

